(window.webpackJsonp=window.webpackJsonp||[]).push([[91,98],{"013z":function(e,t,a){"use strict";var n=a("q1tI"),r=a.n(n),s=a("NmYn"),o=a.n(s),i=a("Wbzz"),c=a("Xrax"),l=a("k4MR"),b=a("TSYQ"),p=a.n(b),u=a("QH2O"),m=a("qKvR"),d=function(e){var t,a=e.title,n=e.tabs,r=void 0===n?[]:n;return Object(m.b)("div",{className:p()(u.pageHeader,(t={},t[u.withTabs]=r.length,t))},Object(m.b)("div",{className:"bx--grid"},Object(m.b)("div",{className:"bx--row"},Object(m.b)("div",{className:"bx--col-lg-12"},Object(m.b)("h1",{id:"page-title",className:u.text},a)))))},h=a("BAC9"),O=function(e){var t=e.relativePagePath,a=e.repository,n=Object(i.useStaticQuery)("1364590287").site.siteMetadata.repository,r=a||n,s=r.baseUrl,o=r.subDirectory,c=s+"/edit/"+r.branch+o+"/src/pages"+t;return s?Object(m.b)("div",{className:"bx--row "+h.row},Object(m.b)("div",{className:"bx--col"},Object(m.b)("a",{className:h.link,href:c},"Edit this page on GitHub"))):null},g=a("FCXl"),f=a("dI71"),j=a("I8xM"),k=function(e){function t(){return e.apply(this,arguments)||this}return Object(f.a)(t,e),t.prototype.render=function(){var e=this.props,t=e.tabs,a=e.slug,n=a.split("/").filter(Boolean).slice(-1)[0],r=t.map((function(e){var t,r=o()(e,{lower:!0,strict:!0}),s=r===n,c=new RegExp(n+"/?(#.*)?$"),l=a.replace(c,r);return Object(m.b)("li",{key:e,className:p()((t={},t[j.selectedItem]=s,t),j.listItem)},Object(m.b)(i.Link,{className:j.link,to:""+l},e))}));return Object(m.b)("div",{className:j.tabsContainer},Object(m.b)("div",{className:"bx--grid"},Object(m.b)("div",{className:"bx--row"},Object(m.b)("div",{className:"bx--col-lg-12 bx--col-no-gutter"},Object(m.b)("nav",null,Object(m.b)("ul",{className:j.list},r))))))},t}(r.a.Component),v=a("MjG9");t.a=function(e){var t=e.pageContext,a=e.children,n=e.location,r=e.Title,s=t.frontmatter,b=void 0===s?{}:s,p=t.relativePagePath,u=t.titleType,h=b.tabs,f=b.title,j=b.theme,A=b.description,y=b.keywords,S=Object(i.useStaticQuery)("2456312558").site.pathPrefix,N=S?n.pathname.replace(S,""):n.pathname,T=h?N.split("/").filter(Boolean).slice(-1)[0]||o()(h[0],{lower:!0}):"";return Object(m.b)(l.a,{tabs:h,homepage:!1,theme:j,pageTitle:f,pageDescription:A,pageKeywords:y,titleType:u},Object(m.b)(d,{title:r?Object(m.b)(r,null):f,label:"label",tabs:h}),h&&Object(m.b)(k,{slug:N,tabs:h,currentTab:T}),Object(m.b)(v.a,{padded:!0},a,Object(m.b)(O,{relativePagePath:p})),Object(m.b)(g.a,{pageContext:t,location:n,slug:N,tabs:h,currentTab:T}),Object(m.b)(c.a,null))}},BAC9:function(e,t,a){e.exports={bxTextTruncateEnd:"EditLink-module--bx--text-truncate--end--2pqje",bxTextTruncateFront:"EditLink-module--bx--text-truncate--front--3_lIE",link:"EditLink-module--link--1qzW3",row:"EditLink-module--row--1B9Gk"}},I8xM:function(e,t,a){e.exports={bxTextTruncateEnd:"PageTabs-module--bx--text-truncate--end--267NA",bxTextTruncateFront:"PageTabs-module--bx--text-truncate--front--3xEQF",tabsContainer:"PageTabs-module--tabs-container--8N4k0",list:"PageTabs-module--list--3eFQc",listItem:"PageTabs-module--list-item--nUmtD",link:"PageTabs-module--link--1mDJ1",selectedItem:"PageTabs-module--selected-item--YPVr3"}},"M+yy":function(e,t,a){"use strict";a.r(t),a.d(t,"_frontmatter",(function(){return i})),a.d(t,"default",(function(){return d}));var n=a("wx14"),r=a("zLVn"),s=(a("q1tI"),a("7ljp")),o=a("013z"),i=(a("qKvR"),{}),c=function(e){return function(t){return console.warn("Component '"+e+"' was not imported, exported, or provided by MDXProvider as global scope"),Object(s.b)("div",t)}},l=c("InlineNotification"),b=c("AnchorLinks"),p=c("AnchorLink"),u={_frontmatter:i},m=o.a;function d(e){var t=e.components,a=Object(r.a)(e,["components"]);return Object(s.b)(m,Object(n.a)({},u,a,{components:t,mdxType:"MDXLayout"}),Object(s.b)(l,{kind:"warning",mdxType:"InlineNotification"},Object(s.b)("strong",null,"Work in progress")," Updated 04/13/2020 - End to end testing could be better"),Object(s.b)(b,{mdxType:"AnchorLinks"},Object(s.b)(p,{mdxType:"AnchorLink"},"Overview"),Object(s.b)(p,{mdxType:"AnchorLink"},"Pre-requisites"),Object(s.b)(p,{mdxType:"AnchorLink"},"Deploy to OpenShift"),Object(s.b)(p,{mdxType:"AnchorLink"},"Interactive queries"),Object(s.b)(p,{mdxType:"AnchorLink"},"Integration Tests")),Object(s.b)("h2",null,"Overview"),Object(s.b)("p",null,"In this lab, we’re going to use ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://quarkus.io"}),"Quarkus")," to develop the real time inventory logic using Kafka Streams APIs and microprofile reactive messaging."),Object(s.b)("p",null,"The requirements to address are:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"consume item sold events from the ",Object(s.b)("inlineCode",{parentName:"li"},"items")," topic. Item has SKU as unique key. Item event has store ID reference"),Object(s.b)("li",{parentName:"ul"},"compute for each item its current stock cross stores"),Object(s.b)("li",{parentName:"ul"},"compute the store’s stock for each item"),Object(s.b)("li",{parentName:"ul"},"generate inventory event for store - item - stock"),Object(s.b)("li",{parentName:"ul"},"expose APIs to get stock for a store or for an item")),Object(s.b)("p",null,"Here is a simple diagram to illustrate the components used:"),Object(s.b)("p",null," ",Object(s.b)("span",Object(n.a)({parentName:"p"},{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"736px"}}),"\n      ",Object(s.b)("span",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-background-image",style:{paddingBottom:"42.36111111111111%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABnElEQVQoz12SyW4TQRCG/bQcuHHjwlsgJRcuHEGASKQgc8AHgmIRxRbB9ogo3iCWJx57Fs/SM+Pu/qgekxAo6S91Vdf2V3frtNvja6fNxacOC8+DLINtKoghibFxIIgEMTqKGhg5b32fYDIjms8JZzPUcglFQeuHd0rvssPnL12uplOc9PwOzy8ecdB/zIvBU87895QaKmNRugnhqP+Tg5cfOTk54vXbd3ij0b4gWc6daG2oKmh/O+Ow+4w3V084vn7F4HZIVe6w1mCNQRRK7DTJybOUVFjVSgmrLS0ThUz8mCRX9GYBUaruG/hxxehmQ5TVDH6FGCm207qBMZp/RO5sktByu9kkW5I0p30+JFUVZalkfRF975rv4xtm/poP3UuUUNrnGuqdm9j+hTTBFXTKiXOKxulaqJd1TVVXDyax4tdoidu5afhPXL4raNZrtPDX0v0OMiLNMgXODhYLkiBobCuxVu7Ng/gGabqnXIQh5WaDEpRyLqTBSr7Brbx4IF9iOR4zHQ6Zyyv6k8m9P1utmvjyT56SvFoV/AaWvlsqWlP79gAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}})),"\n  ",Object(s.b)("img",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-image",alt:"1",title:"1",src:"/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/d2d67/item-aggregator-ctx.png",srcSet:["/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/7fc1e/item-aggregator-ctx.png 288w","/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/a5df1/item-aggregator-ctx.png 576w","/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/d2d67/item-aggregator-ctx.png 736w"],sizes:"(max-width: 736px) 100vw, 736px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"})),"\n    ")),Object(s.b)("p",null,"The goal of this lab, is to develop the green component which exposes an API to support Kafka Streams interactive query on top of the aggregate to compute inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic)."),Object(s.b)("p",null,"We will be unit testing the stream logic using ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://kafka.apache.org/documentation/streams/"}),"Apache Kafka Streams")," TopologyTestDriver class. "),Object(s.b)("p",null,"This application is deployed to OpenShift cluster with Event Streams or Strimzi running in the same cluster. We use the Quarkus Kubernetes plugin to build the yaml manifests for deployment. "),Object(s.b)("p",null,"This application needs the ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator"}),"Item Store sell simulator")," to perform the end to end testing and to demonstrate the end to end scenario."),Object(s.b)("h2",null,"Pre-requisites"),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"OpenShift Container Platform")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"v4.6.x")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"IBM Cloud Pak for Integration")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"CP4I2021.4")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"Kafka")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"The lab uses Event Streams v10.x on Cloud Pack for Integration or Kafka Strimzi.")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"Code Source"),": clone the following git repository: ",Object(s.b)("inlineCode",{parentName:"p"},"git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory"),"."),Object(s.b)("h2",null,"Use application as-is"),Object(s.b)("p",null,"If you do not want to develop the application, you can deploy the existing final app to your OpenShift clustr using our ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://hub.docker.com/r/ibmcase/item-aggregator"}),"docker image")," and the following steps:"),Object(s.b)("ol",null,Object(s.b)("li",{parentName:"ol"},Object(s.b)("p",{parentName:"li"},"Get the Kafka streams credentials and Bootstrap URL"),Object(s.b)("p",{parentName:"li"},"As the application is deployed in the same cluster as the Kafka cluster we will use internal route."))),Object(s.b)("h3",null,"Connect to Event Streams"),Object(s.b)("p",null,"We need to complete the configuration to connect to the remote Event Streams running on OpenShift."),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Create the items and inventory topics, following the instructions as described ","[in this note]","(../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:"),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"cloudctl es topic-create --name items --partitions 3 --replication-factor 3\ncloudctl es topic-create --name inventory --partitions 1 --replication-factor 3\ncloudctl es topics\n"))),Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"To connect from your computer to Event Streams running on OpenShift, we need to define a user with ",Object(s.b)("inlineCode",{parentName:"p"},"scram-sha-512")," password, as this is the mechanism for external to the cluster connection. ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://ibm.github.io/event-streams/getting-started/connecting/"}),"See product documentation")," on how to do it, or use our ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"/refarch-eda/use-cases/overview/pre-requisites#get-shram-user"}),"quick summary here"),".")),Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Get Server TLS certificate into the ",Object(s.b)("inlineCode",{parentName:"p"},"certs")," folder. See our ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"/refarch-eda/use-cases/overview/pre-requisites#get-tls-server-public-certificate"}),"quick summary here")),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"oc get secret minimal-prod-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f - \n\n"))),Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Modify the ",Object(s.b)("inlineCode",{parentName:"p"},"application.properties")," file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka streams can create topics to backup state stores, and one for kafka streams consumer and producer tasks:"))),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-properties"}),'kafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\n%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\"${KAFKA_USER}\\" password\\=\\"${KAFKA_PASSWORD}\\";\n%dev.kafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=${USER_CERT_PATH}\n%prod.kafka.ssl.keystore.password=${USER_CERT_PWD}\n%prod.kafka.ssl.keystore.type=PKCS12\n')),Object(s.b)("p",null,"The above settings take into account that when running locally (",Object(s.b)("inlineCode",{parentName:"p"},"%dev")," profile) we use the ",Object(s.b)("inlineCode",{parentName:"p"},"scram-sha")," mechanism to authenticate, and when we deploy on openshift, the ",Object(s.b)("inlineCode",{parentName:"p"},"%prod")," profile is used with TLS mutual authentication  (client certificate in keystore)."),Object(s.b)("p",null,"The same approach applies for Kafka Stream:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),'quarkus.kafka-streams.bootstrap-servers=${KAFKA_BROKERS}\nquarkus.kafka-streams.security.protocol=${SECURE_PROTOCOL}\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\n%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\n%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\"${KAFKA_USER}\\" password\\=\\"${KAFKA_PASSWORD}\\";\nquarkus.kafka-streams.ssl.truststore.location=${KAFKA_CERT_PATH}\nquarkus.kafka-streams.ssl.truststore.password=${KAFKA_CERT_PWD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n# Only if TLS is used for authentication instead of scram\n%prod.quarkus.kafka-streams.ssl.keystore.location=${USER_CERT_PATH}\n%prod.quarkus.kafka-streams.ssl.keystore.password=${USER_CERT_PWD}\n%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12\n')),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Define a file, like ",Object(s.b)("inlineCode",{parentName:"li"},".env"),", to set environment variables, and modify the settings from your Event Streams configuration.")),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),"KAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\nKAFKA_USER=\nKAFKA_PASSWORD=\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=\nSECURE_PROTOCOL=SASL_SSL\n")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Restart the quarkus in dev mode"),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"source .env\n./mvnw quarkus:dev\n")),Object(s.b)("p",{parentName:"li"},"normally you should not get any exception and should get a trace like"),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),"   AdminClientConfig values: \n   bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]\n   client.dns.lookup = default\n   client.id = \n   connections.max.idle.ms = 300000\n   default.api.timeout.ms = 60000\n   metadata.max.age.ms = 300000\n   metric.reporters = []\n   metrics.num.samples = 2\n   metrics.recording.level = INFO\n   metrics.sample.window.ms = 30000\n\n   ....\n   INFO  [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kafka-streams, kubernetes, kubernetes-client, mutiny, resteasy, resteasy-jsonb, resteasy-mutiny, smallrye-context-propagation, smallrye-health, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, swagger-ui, vertx]\n")))),Object(s.b)("h2",null,"Integration tests"),Object(s.b)("p",null," For running the integration test, we propose to copy the e2e folder from the solution repository and follow the ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing"}),"readme instructions section end-to-end-testing "),"."),Object(s.b)("h2",null,"Deploy to OpenShift"),Object(s.b)("p",null,"Be sure to have done ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift"}),"the steps described here")," to get user credentials and server side certificate. "),Object(s.b)("p",null,"The deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.",Object(s.b)("br",{parentName:"p"}),"\n","Here are the interesting properties to set environment variables from secrets "),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-properties"}),"%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL\nquarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\nquarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12\nquarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password\nquarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12\nquarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.USER_CERT_PWD.value=user.password\n")),Object(s.b)("p",null,"And an extract of the expected generated openshift manifests from those configurations:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-yaml"}),"    spec:\n      containers:\n      - env:\n        - name: KAFKA_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: ca.password\n              name: sandbox-rp-cluster-ca-cert\n        - name: USER_CERT_PATH\n          value: /deployments/certs/user/user.p12\n        - name: USER_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: KAFKA_BROKERS\n          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n        - name: KAFKA_CERT_PATH\n          value: /deployments/certs/server/ca.p12\n        - name: KAFKA_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: SECURE_PROTOCOL\n          value: SASL_SSL\n")),Object(s.b)("p",null,"Finally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),"quarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred\n")),Object(s.b)("p",null,"which generates:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),'        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: es-cert\n          readOnly: false\n          subPath: ""\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: ""\n')),Object(s.b)("p",null,"Now any deployment using the following command should work:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"./mvnw clean package -DQuarkus.kubernetes.deploy=true\n")),Object(s.b)("p",null,"The last piece is to go to EventStreams console and look at the inventory topic for messages generated. As an alternate we could use ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"../../overview/pre-requisites#using-kafdrop"}),"Kafdrop"),"."),Object(s.b)("h2",null,"Another item producer"),Object(s.b)("p",null,"We have done a simple app to produce item sale or restock events. The app is not exposed with API defined in Swagger or with JAXRS annotations, but expose one method to be exposed as a service. See the ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/05fcdcb7d09d674d9eb2cda2e28601171ba51166/item-kafka-producer/src/main/java/ibm/gse/eda/api/ItemSimulatorFunction.java#L18-L25"})," ItemSimulatorFunction code"),". "),Object(s.b)("p",null,"The simulator is using reactive messaging, but as we mix imperative with reactive programming, the code is using Emitter, and then Munity Multi to create and send the Kafka records:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-java"}),'@Inject\n@Channel("items")\nEmitter<Item> emitter;\n\n\npublic void sendItems(Integer numberOfRecords) {\n    Multi.createFrom().items(buildItems(numberOfRecords).stream()).subscribe().with(item -> {\n            logger.warning("send " + item.toString());\n            Message<Item> record = KafkaRecord.of(item.storeName,item);\n            emitter.send(record );\n        }, failure -> System.out.println("Failed with " + failure.getMessage()));\n   \n')),Object(s.b)("h3",null,"Running the application in dev mode"),Object(s.b)("p",null,"You can run your application in dev mode that enables live coding using:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),"./mvnw quarkus:dev\n")),Object(s.b)("p",null,"But as we connect to a remote Kafka Cluster you need to define environment variables as:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"KAFKA_BROKERS=....containers.appdomain.cloud:443\nKAFKA_USER=<a>-scram-user\nKAFKA_PASSWORD=<a-password>\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=<server-ca-certificate-password>\nSECURE_PROTOCOL=SASL_SSL\n")),Object(s.b)("h3",null,"Packaging and running the application"),Object(s.b)("p",null,"The application can be packaged using ",Object(s.b)("inlineCode",{parentName:"p"},"./mvnw package"),".\nIt produces the ",Object(s.b)("inlineCode",{parentName:"p"},"item-kafka-producer-1.0.0-SNAPSHOT-runner.jar")," file in the ",Object(s.b)("inlineCode",{parentName:"p"},"/target")," directory.\nBe aware that it’s not an ",Object(s.b)("em",{parentName:"p"},"über-jar")," as the dependencies are copied into the ",Object(s.b)("inlineCode",{parentName:"p"},"target/lib")," directory."),Object(s.b)("p",null,"The application is now runnable using ",Object(s.b)("inlineCode",{parentName:"p"},"java -jar target/item-kafka-producer-1.0.0-SNAPSHOT-runner.jar"),"."),Object(s.b)("h3",null,"Deployment to OpenShift"),Object(s.b)("p",null,"The code includes declaration to build the necessary environment variables, secrets, volumes to get connected to a Kafka cluster using TLS authentication, and deploy in one command:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"mvn package -DskipTests -Dquarkus.kubernetes.deploy=true\n")),Object(s.b)("p",null,"See the application.properties for ",Object(s.b)("inlineCode",{parentName:"p"},"quarkus.openshift.env-vars.*")," settings."),Object(s.b)("p",null,"It is also supposed to be deployed as knative app, but there is an issue on the generation of volume declarations in the knative.yaml file, so we could not make it in one command."),Object(s.b)("p",null,"The code is also available as docker image: ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://hub.docker.com/r/ibmcase/item-kafka-producer"}),"ibmcase/item-kafka-producer"),"."))}d.isMDXComponent=!0},QH2O:function(e,t,a){e.exports={bxTextTruncateEnd:"PageHeader-module--bx--text-truncate--end--mZWeX",bxTextTruncateFront:"PageHeader-module--bx--text-truncate--front--3zvrI",pageHeader:"PageHeader-module--page-header--3hIan",withTabs:"PageHeader-module--with-tabs--3nKxA",text:"PageHeader-module--text--o9LFq"}}}]);
//# sourceMappingURL=component---src-pages-use-cases-kafka-streams-lab-3-index-mdx-d2a7a6f3829d8a210d60.js.map