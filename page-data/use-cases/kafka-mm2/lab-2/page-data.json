{"componentChunkName":"component---src-pages-use-cases-kafka-mm-2-lab-2-index-mdx","path":"/use-cases/kafka-mm2/lab-2/","result":{"pageContext":{"frontmatter":{"title":"Mirror Maker 2 ES on RHOS to local cluster","description":"Using Mirror Maker 2 from Event Streams on OpenShift to local cluster"},"relativePagePath":"/use-cases/kafka-mm2/lab-2/index.mdx","titleType":"append","MdxNode":{"id":"aa04f9fc-a36c-5149-a94c-dd7c4bd185a2","children":[],"parent":"60209e93-c715-5820-a67d-66fa0ff6e5d9","internal":{"content":"---\ntitle: Mirror Maker 2 ES on RHOS to local cluster\ndescription: Using Mirror Maker 2 from Event Streams on OpenShift to local cluster\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Pre-requisites</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nUpdated 01/08/2021\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on OpenShift, and the target cluster is another Kafka cluster (using strimzi) running locally. Mirror Maker 2 runs on local server in same data center as target cluster. (Your laptop is promoted as data center). This lab is similar to the lab 1, but it instead uses the Event Streams within Cloud Pak for Integration as illustrated in the figure below:\n\n ![1](../images/mm2-lab2.png)\n\n 1. Mirror Maker 2 runs on local server.\n 2. A producer in python to send records to `products` topic, will run locally or could be deployed on OpenShift as a **job**\n 3. Local Kafka cluster with replicated topic and Kafka console consumer to see the replicated messages.\n\n## Pre-requisites\n\nYou need to have one instance of Event Streams installed and configured using Cloud Pak for Integration. See the [product documentation for installation](https://ibm.github.io/event-streams/installing/installing/)\n\nWe need to define the `products` topic on the source cluster (Event Streams on CP4I) and get the authentication credentials and TLS certificates.\n\n* Login to the OpenShift cluster using the console and get the API token\n\n ```shell\n oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443\n ```\n\n* If not done from lab 1, clone the github to get access to the Mirror Maker 2 configuration we are using:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n ```\n\n* Verify the Event Streams on OpenShift service external end point URL. This URL will be used to configure Mirror Maker 2. \n\n ```shell\n # Be sure to be in the project where eventstreams runs. The following command may be different.\n oc project eventstreams\n # Use the bootstrap internal URL: light-es is the name of the cluster\n oc get route | grep bootstrap\n\n # The URL is something like\n light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud \n ```\n\n* Rename the `.env-tmpl` file to `.env`. Then edit this `.env` file, with the bootstrap URL and the port 443 at the end: `light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443`.\n* Get a user with write access to topic, group and transactions. You can use the Event Streams Console User Interface to define this user,  with the `Create SCRAM credential` on the internal URL panel. See [this section](/use-cases/overview/pre-requisites#getting-scram-authentication-from-event-streams-on-openshift) for detail.\n\n ```shell\n  oc get kafkausers\n\n # NAME                                    CLUSTER       AUTHENTICATION   AUTHORIZATION\n # light-es-ibm-es-ac-reg                  light-es      scram-sha-512    \n # light-es-ibm-es-georep-source-user      light-es      scram-sha-512    simple\n # light-es-ibm-es-kafka-user              light-es      tls   \n # starter                                 light-es      scram-sha-512           \n ```\n\nModify the `.env` file to set the `ES_OCP_USER` variable with your selected user.\n\n## Start the local Kafka cluster\n\nIn the `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` folder there is a [docker compose file](https://raw.githubusercontent.com/refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local/master/docker-compose.yml) to start a local three brokers cluster with one Zookeeper node.\n\n1. In one Terminal window, start the local cluster using the command:\n\n```shell\ndocker-compose up -d\n```\n\nThe data are persisted on the local disk within the folder named `kafka-data`.\n\nYour local environment is up and running.\n\n## Start Mirror Maker 2\n\n* If not done already, create a `products` topic (with one partition) in the EventStreams cluster using the management console. See [this note](/use-cases/overview/pre-requisites#creating-event-streams-topics) to see how to do this operation.\n* As done in lab_1 the launch script will configure Mirror Maker 2 in standalone mode with the properties specific to the SCRAM authentication with TLS certificates.\n\n    ```properties\n    es-cp4i.ssl.protocol=TLSv1.2\n    es-cp4i.ssl.truststore.password=<>\n    es-cp4i.ssl.truststore.location=/home/es-cert.p12\n    es-cp4i.ssl.truststore.type=PKCS12\n    es-cp4i.security.protocol=SASL_SSL\n    es-cp4i.ssl.endpoint.identification.algorithm=https\n    es-cp4i.sasl.mechanism=SCRAM-SHA-512\n    es-cp4i.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=starter password=<>;\n\n    ```\n \n* Start Mirror Maker2 using the launch script:\n\n    ```shell\n    # In the  es-ic-to-local folder\n    ./launchMM2.sh\n    ```\n\n\n## Start consumer from target cluster\n\nUse Apache Kafka tool like Console consumer to trace the message received on a topic\n\n```shell\ndocker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-cp4i.products --from-beginning\" \n```\n\n\n## Start Producer to source cluster\n\nAs seen in lab 1, we will use the same python script to create products records. This time the script is producing products data to Event Streams on OpenShift. So we need the URL, pem, and user to access this cluster.\n\n\n* If not done before download the pem file using the following CLI commands:\n\n ```shell\n cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n cloudctl es init\n # select the event stream cluster\n cloudctl es certificates --format pem\n ```\n\n save this `.pem` file under `mirror-maker2/es-cpri-to-local` folder.\n\n Now send five records:\n \n ```shell\n \n ./sendProductRecords.sh\n ```\n\n * Going to the Event Streams Console we can see the message in the `products` topic.\n\n![](../images/es-products-topic.png)\n\n\n## Clean up\n\nYou are done with the lab, to stop everything:\n\n```shell\n./cleanLab.sh\n```\n","type":"Mdx","contentDigest":"9692f454862637f6aa36ddbaa7f5a2a0","counter":712,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Mirror Maker 2 ES on RHOS to local cluster","description":"Using Mirror Maker 2 from Event Streams on OpenShift to local cluster"},"exports":{},"rawBody":"---\ntitle: Mirror Maker 2 ES on RHOS to local cluster\ndescription: Using Mirror Maker 2 from Event Streams on OpenShift to local cluster\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Pre-requisites</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nUpdated 01/08/2021\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on OpenShift, and the target cluster is another Kafka cluster (using strimzi) running locally. Mirror Maker 2 runs on local server in same data center as target cluster. (Your laptop is promoted as data center). This lab is similar to the lab 1, but it instead uses the Event Streams within Cloud Pak for Integration as illustrated in the figure below:\n\n ![1](../images/mm2-lab2.png)\n\n 1. Mirror Maker 2 runs on local server.\n 2. A producer in python to send records to `products` topic, will run locally or could be deployed on OpenShift as a **job**\n 3. Local Kafka cluster with replicated topic and Kafka console consumer to see the replicated messages.\n\n## Pre-requisites\n\nYou need to have one instance of Event Streams installed and configured using Cloud Pak for Integration. See the [product documentation for installation](https://ibm.github.io/event-streams/installing/installing/)\n\nWe need to define the `products` topic on the source cluster (Event Streams on CP4I) and get the authentication credentials and TLS certificates.\n\n* Login to the OpenShift cluster using the console and get the API token\n\n ```shell\n oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443\n ```\n\n* If not done from lab 1, clone the github to get access to the Mirror Maker 2 configuration we are using:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n ```\n\n* Verify the Event Streams on OpenShift service external end point URL. This URL will be used to configure Mirror Maker 2. \n\n ```shell\n # Be sure to be in the project where eventstreams runs. The following command may be different.\n oc project eventstreams\n # Use the bootstrap internal URL: light-es is the name of the cluster\n oc get route | grep bootstrap\n\n # The URL is something like\n light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud \n ```\n\n* Rename the `.env-tmpl` file to `.env`. Then edit this `.env` file, with the bootstrap URL and the port 443 at the end: `light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443`.\n* Get a user with write access to topic, group and transactions. You can use the Event Streams Console User Interface to define this user,  with the `Create SCRAM credential` on the internal URL panel. See [this section](/use-cases/overview/pre-requisites#getting-scram-authentication-from-event-streams-on-openshift) for detail.\n\n ```shell\n  oc get kafkausers\n\n # NAME                                    CLUSTER       AUTHENTICATION   AUTHORIZATION\n # light-es-ibm-es-ac-reg                  light-es      scram-sha-512    \n # light-es-ibm-es-georep-source-user      light-es      scram-sha-512    simple\n # light-es-ibm-es-kafka-user              light-es      tls   \n # starter                                 light-es      scram-sha-512           \n ```\n\nModify the `.env` file to set the `ES_OCP_USER` variable with your selected user.\n\n## Start the local Kafka cluster\n\nIn the `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` folder there is a [docker compose file](https://raw.githubusercontent.com/refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local/master/docker-compose.yml) to start a local three brokers cluster with one Zookeeper node.\n\n1. In one Terminal window, start the local cluster using the command:\n\n```shell\ndocker-compose up -d\n```\n\nThe data are persisted on the local disk within the folder named `kafka-data`.\n\nYour local environment is up and running.\n\n## Start Mirror Maker 2\n\n* If not done already, create a `products` topic (with one partition) in the EventStreams cluster using the management console. See [this note](/use-cases/overview/pre-requisites#creating-event-streams-topics) to see how to do this operation.\n* As done in lab_1 the launch script will configure Mirror Maker 2 in standalone mode with the properties specific to the SCRAM authentication with TLS certificates.\n\n    ```properties\n    es-cp4i.ssl.protocol=TLSv1.2\n    es-cp4i.ssl.truststore.password=<>\n    es-cp4i.ssl.truststore.location=/home/es-cert.p12\n    es-cp4i.ssl.truststore.type=PKCS12\n    es-cp4i.security.protocol=SASL_SSL\n    es-cp4i.ssl.endpoint.identification.algorithm=https\n    es-cp4i.sasl.mechanism=SCRAM-SHA-512\n    es-cp4i.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=starter password=<>;\n\n    ```\n \n* Start Mirror Maker2 using the launch script:\n\n    ```shell\n    # In the  es-ic-to-local folder\n    ./launchMM2.sh\n    ```\n\n\n## Start consumer from target cluster\n\nUse Apache Kafka tool like Console consumer to trace the message received on a topic\n\n```shell\ndocker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-cp4i.products --from-beginning\" \n```\n\n\n## Start Producer to source cluster\n\nAs seen in lab 1, we will use the same python script to create products records. This time the script is producing products data to Event Streams on OpenShift. So we need the URL, pem, and user to access this cluster.\n\n\n* If not done before download the pem file using the following CLI commands:\n\n ```shell\n cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n cloudctl es init\n # select the event stream cluster\n cloudctl es certificates --format pem\n ```\n\n save this `.pem` file under `mirror-maker2/es-cpri-to-local` folder.\n\n Now send five records:\n \n ```shell\n \n ./sendProductRecords.sh\n ```\n\n * Going to the Event Streams Console we can see the message in the `products` topic.\n\n![](../images/es-products-topic.png)\n\n\n## Clean up\n\nYou are done with the lab, to stop everything:\n\n```shell\n./cleanLab.sh\n```\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-mm2/lab-2/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}