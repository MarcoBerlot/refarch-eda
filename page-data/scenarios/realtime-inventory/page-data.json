{"componentChunkName":"component---src-pages-scenarios-realtime-inventory-index-mdx","path":"/scenarios/realtime-inventory/","result":{"pageContext":{"frontmatter":{"title":"Near real-time Inventory, powered by IBM Event Streams","description":"An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect."},"relativePagePath":"/scenarios/realtime-inventory/index.mdx","titleType":"append","MdxNode":{"id":"645ba643-c2fb-5b45-9f6a-93fcb3400b19","children":[],"parent":"1397c2b6-79ae-57a1-adc3-d7c425fab022","internal":{"content":"---\ntitle: Near real-time Inventory, powered by IBM Event Streams\ndescription: An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect.\n---\n\nAn IBM Cloud Pak for Integration - Event Streams use case\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 03/09/2022</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Introduction</AnchorLink>\n  <AnchorLink>Use Case Guided Tour</AnchorLink>\n  <AnchorLink>Full Demo Narration</AnchorLink>\n  <AnchorLink>Developer Corner</AnchorLink>\n </AnchorLinks>\n\n----\n# Introduction \n\n## Use Case Overview\n\nToday, a lot of companies which are managing item / product inventory are facing real challenges to get a close to real-time view of item availability\nand global inventory view. The solution can be very complex to implement while integrating Enterprise Resource Planning products and other custom legacy systems. \nAny new solutions are adopting events as a source to exchange data, to put less pressure on existing ERP servers, and to get better visibility \ninto inventory positions while bringing agility to develop new solution with streaming components.\n\nThis scenario implements a simple near real-time inventory management solution based on real life MVPs we developed in 2020 for different customers. \nA production deployment will implement different level of store and warehouse inventory aggregators that will push results as event to an event backbone.\nThose aggregates are used to build different data caching services, adnd to address use cases such as item availability.\n\nAt the high-level, the solution may look like in the following figure:\n\n\n<img src=\"./images/hl-view.png\" alt=\"hl view\" width=\"500\"/>\n\nServers in the Store are sending sale transactions to a central messaging platform, where streaming components are computing the different aggregates \nand are publishing them to other topics. This is a classical data streaming pipeline. Sink connectors, based on Kafka Connect framework,\n may be used to move data to long persistence storage like a Database, integrate results back to Legacy ERP, use indexing like Elastic Search, \n propagate events to dashboards...\n\nIn real life, an as-is solution will include back-end applications to manage the warehouses inventory, connected to a home-built fulfillment application,\ncombined with store applications and servers, e-commerce suite, and a set of SOA services exposing backend systems. This is the larger view of the\nfollowing figure:\n\n ![](./images/as-is-soa.png)\n\nWe may have integration flows to do data mapping, but most of those calls are synchronous.  To get one item availability, a lot of SOAP calls are done, \nincreasing latency, and the risk of failure. There is [an interesting video](https://www.youtube.com/watch?v=FskIb9SariI) from Scott Havens explaining the needs \nfrom transitioning from a synchronous architecture to an event-driven asynchronous architecture when scaling, and low latency are must have. \nThis lab reflects this approach.\n\n## Demonstration components\n\nIn this demonstration we are deploying the following components:\n\n![](./images/mq-es-demo.png)\n\nDiagram source: [rt-inventory diagram](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/docs/diagrams/mq-es-demo.drawio)\n\n1. The [store simulator application](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) is a Quarkus based microservice, used to generate item sales \nto different possible messaging middlewares ( RabbitMQ, IBM MQ or directly to IBM Event Streams). \nIf you want to browse the code, the main readme of this project includes how to package and run this app with docker compose. A code explanation\nsection may give some ideas to developers. The docker image is [quay.io/ibmcase/eda-store-simulator/](https://quay.io/ibmcase/eda-store-simulator) and can be used for demonstration.\n1. The item inventory aggregator is a Kafka Stream, Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-item-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory). \nConsider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on `item.inventory` topic. \nAs a developer you may want to understand Kafka Stream programming with the [following labs](/use-cases/kafka-streams/), and then considering looking at the classes: [ItemProcessingAgent.java](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/main/java/ibm/gse/eda/inventory/domain/ItemProcessingAgent.java).\n1. The store inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-store-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory). The output is in `store.inventory` topic. \n1. The MQ to Kafka, Kafka connector is defined in the [eda-rt-inventory-GitOps](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops) repository under the [kconnect](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/kconnect) folder\n1. The Kafka to Cloud Object Storage Kafka (S3 bucket) connector is also defined in the same [GitOps repository/folder](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/kconnect/kafka-cos-sink-connector.yaml).\n1. The Sink connector to Elastic Search is defined in [this Kafka connector definition](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/kconnect/kafka-elastic-sink-connector.yaml).\n\n[Kafka Connect](/technology/kafka-connect/) is used to integrate external systems into Kafka. For example external systems can inject item sale messages to queue, from which a first MQ source Kafka connector \npublishes the messages to the `items` Kafka topic. Items sold events are processed by a series of streaming microservices down to different topics to keep aggregation results.\nThose topics content could be which will be used by Sink connectors\n to send records to other external systems.\n\n## A GitOps approach for solution deployment\n\nAs any cloud-native and kubernetes based solution, we use continuous integration and continuous deployment practices. From a demonstration point of view, the most interesting part is\nto execute continuous deployment using a [GitOps approach](/use-cases/gitops/). This demonstration has a [GitOps repository](https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git) \n(represented as the yellow rectangle in figure below) which defines ArgoCD apps \nto monitor and deploy the different microservices, streaming processing apps,\nand the different IBM products needed: Event Streams, MQ, API management, event-end-point management. The figure belows presents the adopted strategy:\n\n![](./images/gitops-catalog.png)\n\nThe [gitops catalog repository](https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git), represented with a blue rectangle, defines the different operator subscriptions\nfor the cloud pak for integration components. Centralizing to one repository such operator subscriptions enforces reuse between solutions.\n\nThe solution [gitops](https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git) was created with [KAM CLI](https://github.com/redhat-developer/kam) and is structured as follow:\n\n* **Boostrap** folder: to install different operators and to define the ArgoCD project named `rt-inventory`.\n* **config** folder, is for defining the ArgoCD apps and the app of apps. \n* **kconnect** folder is used to build a custom docker image of Kafka connector with MQ source, Elasticsearch sink and Cloud Object storage sink.\n* **local-demo** folder is for running the solution on your laptop using docker-compose.\n* **environments** folder, is the most important one, it uses [Kustomize](https://kustomize.io/) \nto declare environments (dev, staging) and component deployments (See next section for details). \n\n### A little bit more on environments\n\nFor demonstration purpose, only the `rt-inventory-dev` environment is detailed. One ArgoCD app: [rt-inventory-dev-env](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/config/argocd/rt-inventory-dev-env-app.yaml) is monitoring\n the folder [environments/rt-inventory-dev/env/overlays](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/environments/rt-inventory-dev/env/overlays) which define the namespace and roles,... \nEach application of the solution are also monitored by an ArgoCD and their declarations are done using standard kubernetes manifests. Here is an exampe of app tree structure:\n\n```\n── store-inventory\n│   ├── base\n│   │   └── kustomization.yaml\n│   ├── kustomization.yaml\n│   ├── overlays\n│   │   └── kustomization.yaml\n│   └── services\n│       └── store-inventory\n│           ├── base\n│           │   ├── config\n│           │   │   ├── configmap.yaml\n│           │   │   ├── deployment.yaml\n│           │   │   ├── kustomization.yaml\n│           │   │   ├── rolebinding.yaml\n│           │   │   ├── route.yaml\n│           │   │   ├── service.yaml\n│           │   │   └── serviceaccount.yaml\n│           │   └── kustomization.yaml\n│           ├── kustomization.yaml\n│           └── overlays\n│               └── kustomization.yaml\n```\n\nThe last intesting part is to declare the products used within the Cloud Pak for Integration, and deployed in the context of\nthe respective environments. Everything is in `services` folder. The tree looks like below:\n\n```\n── apicurio\n│   ├── base\n│   │   ├── es-kafka-topics.yaml\n│   │   ├── kustomization.yaml\n│   │   └── registry.yaml\n│   └── overlays\n│       └── kustomization.yaml\n├── event-endpoint\n│   ├── base\n│   │   ├── eventendpointmanager-eepm-eda.yaml\n│   │   └── kustomization.yaml\n│   └── overlays\n│       ├── kustomization.yaml\n│       └── v10.0.4.0\n│           ├── kustomization.yaml\n│           └── patch-version.yaml\n├── ibm-eventstreams\n│   ├── base\n│   │   ├── es-topics.yaml\n│   │   ├── eventstreams-dev.yaml\n│   │   ├── kustomization.yaml\n│   │   ├── scram-user.yaml\n│   │   └── tls-user.yaml\n│   └── overlays\n│       ├── kustomization.yaml\n│       └── v10.5\n│           ├── kustomization.yaml\n│           └── patch-version.yaml\n├── ibm-mq\n│   ├── base\n│   │   ├── kustomization.yaml\n│   │   └── qmgr.yaml\n│   └── overlays\n│       ├── kustomization.yaml\n│       └── v9.2.4\n│           ├── kustomization.yaml\n│           └── patch-channel.yaml\n├── kconnect\n│   ├── README.md\n│   ├── kafka-connect.yaml\n│   ├── kafka-cos-sink-connector.yaml\n│   ├── kustomization.yaml\n│   └── mq-source.json\n└── kustomization.yaml\n```\n\nWith the public docker images, and the public GitOps repository, the solution can be deployed to an OpenShift cluster with or without Cloud Pak for Integration already deployed.\n\n## Choose an option:\n\n* [Run on your laptop](/scenarios/realtime-inventory/#run-on-your-laptop)\n* [Run on OpenShift](/scenarios/realtime-inventory/#install-yourself-on-openshift) step by steps\n* [Run on OpenShift with GitOps](/scenarios/realtime-inventory/#run-with-gitops-on-openshift)\n\n### Pre-requisites for all options\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* [git CLI](https://git-scm.com/downloads).\n* Clone the Inventory lab repository: \n\n  ```sh\n  git clone  https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git\n  ```\n\n\n### Run on your laptop\n\nAs a developer or technical seller you could demonstrate this scenario on your laptop. \nThe docker images used in this solution are in public registry ([Quay.io](https://recovery.quay.io/organization/ibmcase)).\nUnder the [rt-inventory-gitops.git](https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git) local-demo/kstream, we propose \ndifferent docker compose files to have different components running:\n\n* [docker-compose.yaml](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/local-demo/kstreams/docker-compose.yaml) for Event Streams, IBM MQ, Kafka Connector \nthe Store Simulator App, the Item aggregator App, the Store aggregator App, and KafDrop.\n* [docker-compose-all.yaml](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/local-demo/kstreams/docker-compose-all.yaml) same as above plus \nElasticSearch (1 node) and Kibana\n\nOnce you have cloned the gitops repository (see pre-requisites section), go under the `local-demo/kstreams` folder and run\n\n  ```sh\n  docker-compose -f docker-compose-all.yaml up -d\n  ```\n\n  To stop the demonstration do the following command:\n\n  ```sh\n  docker-compose -f docker-compose-all.yaml down\n  ```\n\nThe demonstration script is the [same as below](/scenarios/realtime-inventory/#demonstrate-the-real-time-processing), except that we use Kafdrop to visualize the content of Event Streams topics. \nSee [this section](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops#run-the-solution-locally) in the main readme for expected screen shots.\n\n### Install Yourself on OpenShift\n\nIt is possible to do a step by step deployment of the solution without any gitops deployment. This is more like a lab tutorial, where you can progress more slowly and verify\nthe result at each steps.\n\nGo to the `rt-inventory-gitops/ocp-demo-step-by-step` folder and follow the [README instructions](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/ocp-demo-step-by-step) to make\na step by step approach creating records with the simulator, see them in MQ Queue, start the Kafka Connector MQ source, to move data to Event Streams topic, then\ndo the kafka streams processing. \n\n### Run with gitops on openshift\n\nIn this approach, we propose to use a GitOps repository and deploy the solution using few scripts and `oc` CLI commands.\nIn the figure below, all the components are deployed with OpenShift GitOps. The blue components are IBM product components like Event Streams and MQ operators and some other operators, like Elastic Search.\nThe green rectangles represent the microservices and kafka connectors source or sink deployed as part of the solution.\n\n![](./images/hl-view.png)\n\nSee next guided tour for GitOps approach.\n\n--- \n\n# Use Case Guided Tour\n\n## Bootstrap GitOps\n\nBootstrapping GitOps is mostly to install the MQ, Event Streams, APIC Connect, OpenShift Gitops, and ElasticSearch operators, and do any pre-requisites like entitlement keys.\n\nWe prefer to keep the bootstrap instructions in the source repository, therefore follow [the up to date instructions](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops#bootstrap-gitops) from the gitops main readme.\n\n## Deploy the Solution\n\nOnce operators are deployed. \n\n* Deploying the full solution is by starting ArgoCD app of apps:\n\n  ```sh\n  oc apply -k config/argocd\n  ```\n\n* Access the OpenShift GitOps (ArgoCD) console\n\n  ```sh\n   chrome https://$(oc get route openshift-gitops-server -o jsonpath='{.status.ingress[].host}'  -n openshift-gitops)\n   ```\n\nThe expected set of ArgoCD apps looks like (and all should be green):\n\n  ![](./images/rt-inv-argoapps.png)\n\n  * **rt-inventory-Argo-app** is an app of apps\n  * **rt-inventory-dev-env** is for the rt-inventory-dev namespace\n  * **rt-inventory-dev-services** is for event streams, kafka connect cluster and mq deployments in dev-env namespace\n  * **rt-inventory-store-simulator-app** is for the simulator app used in the demo.\n  * **rt-inventory-item-inventory** for the item aggregator application\n  * **rt-inventory-store-inventory** for the store aggregator application\n  * **rt-inventory-dev-eepm-service** for Event End Point management\n  * **rt-inventory-dev-elastic-svc** for Elastic Search deployment\n  * **rt-inventory-dv-kibana-svc** for Kibana\n\n* Verify pods\n\n```sh\n  oc project rt-inventory-dev\n  oc get pods\n\n  NAME                                         READY   STATUS    RESTARTS   AGE\n  dev-kafka-cruise-control-6d6bf8b774-99rwl    2/2     Running   0          4d\n  dev-kafka-entity-operator-75f7bc8f5c-x4vkt   3/3     Running   0          4d\n  dev-kafka-kafka-0                            1/1     Running   0          4d\n  dev-kafka-kafka-1                            1/1     Running   0          4d\n  dev-kafka-kafka-2                            1/1     Running   0          4d\n  dev-kafka-zookeeper-0                        1/1     Running   0          4d\n  dev-kafka-zookeeper-1                        1/1     Running   0          4d\n  dev-kafka-zookeeper-2                        1/1     Running   0          4d\n  item-inventory-669fd4fffc-4fvhk             1/1     Running   0          30h\n  store-inventory-7df98556ff-f2ndq            1/1     Running   0          29h\n  store-simulator-56f8958498-mvhp9             1/1     Running   0          4d\n  dev-entity-operator-74d7dc5cfb-ksv68                              3/3     Running     0          5d21h\n  dev-ibm-es-ac-reg-77bfbf84b9-qn8ln                                2/2     Running     0          5d21h\n  dev-ibm-es-admapi-6f6bcd465c-h8scj                                1/1     Running     0          5d21h\n  dev-ibm-es-metrics-9c4679cd-n5bb7                                 1/1     Running     0          5d21h\n  dev-ibm-es-recapi-775bf874b9-gqbdn                                1/1     Running     0          5d21h\n  dev-ibm-es-ui-5d488967d4-6v2tm                                    2/2     Running     0          5d21h\n  eda-eepm-mgmt-27885b45-postgres-55b548f64f-nqs6f                  0/1     Init:0/1    0          4d23h\n  eda-eepm-mgmt-27885b45-postgres-backrest-shared-repo-7b58fjdd7s   1/1     Running     0          12d\n  eda-eepm-mgmt-27885b45-postgres-pgbouncer-5575bc4595-c9zng        1/1     Running     0          12d\n  eda-kconnect-cluster-connect-78ccb7cc56-jh2ck                     1/1     Running     0          4d17h\n  elasticsearch-es-default-0                                        1/1     Running     0          36m\n  elasticsearch-es-default-1                                        1/1     Running     0          36m\n  elasticsearch-es-default-2                                        1/1     Running     0          36m\n  kibana-kb-67f4c87c65-9whwz                                        1/1     Running     0          36m\n  store-mq-ibm-mq-0                                                 1/1     Running     1          4d19h\n  ```\n\n## Demonstrate the real-time processing\n\n1. Get the Store simulator route using the following command and start a Web Browser\n\n  ```sh\n  chrome $(oc get routes store-simulator  -o jsonpath=\"{.spec.host}\")\n  ```\n\n  You should reach the Home page of the simulator\n\n  ![](./images/home-page.png)\n\n1. Look at existing stores, using the top right `STORES` menu. This is just for viewing the data. \n\n  ![](./images/stores-view.png)\n\n1. Go to the SIMULATOR menu, and start the controlled scenario which will send predefined records:\n\n  ![](./images/start-simulation.png)\n\n  Here is the ordered list of messages sent:\n\n  | Store | Item | Action |\n  | --- | --- | --- |\n  | Store 1 | Item_1 | +10 |\n  | Store 1 | Item_2 | +5 |\n  | Store 1 | Item_3 | +15 |\n  | Store 2 | Item_1 | +10 |\n  | Store 3 | Item_1 | +10 |\n  | Store 4 | Item_1 | +10 |\n  | Store 5 | Item_1 | +10 |\n  | Store 1 | Item_2 | -5 |\n  | Store 1 | Item_3 | -5 |\n\n\n  once started a table should be displayed to present the records sent to Kafka. (The UI needs some enhancement ;-)\n\n  ![](./images/send-msgs.png) \n\n4- Verify messages are in queue:\n\n   * Access the MQ Console admin: \n\n  ```\n  chrome http://$(oc get routes store-mq-ibm-mq-web  -o jsonpath=\"{.spec.host}\")\n  ```\n\n  * Go to the QM1 Queue manager \n\n  ![](./images/qm1-qm.png)\n\n  * Select the ITEMS queue to verify the messages reach the queue. It may be possible that the Kafka Connector already consumed those messages\n  so the queue may look empty.\n\n  ![](./images/msg-in-queue.png)\n\n  Below is a view of one of those message.\n\n  ![](./images/one-msg-queue.png)\n\n\n5- Let assess if we can see the item stock cross stores: using the `item-aggregator` route, something like ` item-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud ` but completed with '/q/swagger-ui' as we want to access the API\n  \n  To get this route use the following command:\n\n  ```sh\n  chrome http://$(oc get routes item-inventory -o jsonpath=\"{.spec.host}\")\n  ```\n\n   Select the get `/api/v1/items/{itemID}` operation:\n\n  ![](./images/items-stock-req.png)\n\n  Use one of the following item id: [Item_1, Item_2, Item_3, Item_4, Item_5, Item_6, Item_7]. You should get \n  the current stock cross stores\n\n  ![](./images/item-stock-response.png)\n\n6- Let assess a store stock, for that we access the store aggregator URL: `store-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud ` with the `/q/swagger-ui` suffix.\n\n  To get this route use the following command:\n\n  ```sh\n  chrome http://$(oc get routes store-aggregator -o jsonpath=\"{.spec.host}\")\n  ```\n\n  Then using the GET on the `api/v1/stores/inventory/{storeID}`, and enter one of the available store: `[Store_1, Store_2, Store_3, Store_4, Store_5]`\n\n  ![](./images/store-stock-req.png)\n\n  The response should look like:\n\n  ![](./images/store-stock-response.png)\n\n---\n\n# Full Demo Narration\n\nWe will first go over the demonstration using the store simulator then using and end to end test script\n\n\n---\n\n# Developer Corner\n\n## What you will learn\n\n* Use Quarkus, with reactive programming API like Mutiny, and Kafka API to produce messages to Kafka\n* Same Quarkus app can generate messages to RabbitMQ using the AMQP API\n* Same Quarkus app can generate messages to IBM MQ using JMS\n* Use Quarkus and Kafka Streams to compute aggregates to build an inventory view from the stream of sale order events\n* Use the IBM MQ source connector from IBM Event messaging open source contribution\n\n\n## Solution anatomy\n\n1. The [store simulator application](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) is a Quarkus app, which generates item sales to different possible messaging middlewares ( RabbitMQ, MQ or directly to Kafka). \nThe code of this application is in this https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator. If you want to browse the code, the main readme of this project includes how to package and run this app with docker compose, \nand explains how the code works. \nThe docker image is [quay.io/ibmcase/eda-store-simulator/](https://quay.io/ibmcase/eda-store-simulator)\n1. The item inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-item-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory). \nConsider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on `item.inventory` topic. \n1. The store inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-store-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory) the output is in `store.inventory` topic. \n2. The mock up Inventory mainframe application is not implemented and we will use the MQ tools to view the message in the `item.inventory` MQ queue.\n\n \n\n","type":"Mdx","contentDigest":"8cdad9f67e66721c54c0c7795189a214","owner":"gatsby-plugin-mdx","counter":874},"frontmatter":{"title":"Near real-time Inventory, powered by IBM Event Streams","description":"An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect."},"exports":{},"rawBody":"---\ntitle: Near real-time Inventory, powered by IBM Event Streams\ndescription: An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect.\n---\n\nAn IBM Cloud Pak for Integration - Event Streams use case\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 03/09/2022</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Introduction</AnchorLink>\n  <AnchorLink>Use Case Guided Tour</AnchorLink>\n  <AnchorLink>Full Demo Narration</AnchorLink>\n  <AnchorLink>Developer Corner</AnchorLink>\n </AnchorLinks>\n\n----\n# Introduction \n\n## Use Case Overview\n\nToday, a lot of companies which are managing item / product inventory are facing real challenges to get a close to real-time view of item availability\nand global inventory view. The solution can be very complex to implement while integrating Enterprise Resource Planning products and other custom legacy systems. \nAny new solutions are adopting events as a source to exchange data, to put less pressure on existing ERP servers, and to get better visibility \ninto inventory positions while bringing agility to develop new solution with streaming components.\n\nThis scenario implements a simple near real-time inventory management solution based on real life MVPs we developed in 2020 for different customers. \nA production deployment will implement different level of store and warehouse inventory aggregators that will push results as event to an event backbone.\nThose aggregates are used to build different data caching services, adnd to address use cases such as item availability.\n\nAt the high-level, the solution may look like in the following figure:\n\n\n<img src=\"./images/hl-view.png\" alt=\"hl view\" width=\"500\"/>\n\nServers in the Store are sending sale transactions to a central messaging platform, where streaming components are computing the different aggregates \nand are publishing them to other topics. This is a classical data streaming pipeline. Sink connectors, based on Kafka Connect framework,\n may be used to move data to long persistence storage like a Database, integrate results back to Legacy ERP, use indexing like Elastic Search, \n propagate events to dashboards...\n\nIn real life, an as-is solution will include back-end applications to manage the warehouses inventory, connected to a home-built fulfillment application,\ncombined with store applications and servers, e-commerce suite, and a set of SOA services exposing backend systems. This is the larger view of the\nfollowing figure:\n\n ![](./images/as-is-soa.png)\n\nWe may have integration flows to do data mapping, but most of those calls are synchronous.  To get one item availability, a lot of SOAP calls are done, \nincreasing latency, and the risk of failure. There is [an interesting video](https://www.youtube.com/watch?v=FskIb9SariI) from Scott Havens explaining the needs \nfrom transitioning from a synchronous architecture to an event-driven asynchronous architecture when scaling, and low latency are must have. \nThis lab reflects this approach.\n\n## Demonstration components\n\nIn this demonstration we are deploying the following components:\n\n![](./images/mq-es-demo.png)\n\nDiagram source: [rt-inventory diagram](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/docs/diagrams/mq-es-demo.drawio)\n\n1. The [store simulator application](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) is a Quarkus based microservice, used to generate item sales \nto different possible messaging middlewares ( RabbitMQ, IBM MQ or directly to IBM Event Streams). \nIf you want to browse the code, the main readme of this project includes how to package and run this app with docker compose. A code explanation\nsection may give some ideas to developers. The docker image is [quay.io/ibmcase/eda-store-simulator/](https://quay.io/ibmcase/eda-store-simulator) and can be used for demonstration.\n1. The item inventory aggregator is a Kafka Stream, Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-item-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory). \nConsider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on `item.inventory` topic. \nAs a developer you may want to understand Kafka Stream programming with the [following labs](/use-cases/kafka-streams/), and then considering looking at the classes: [ItemProcessingAgent.java](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/main/java/ibm/gse/eda/inventory/domain/ItemProcessingAgent.java).\n1. The store inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-store-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory). The output is in `store.inventory` topic. \n1. The MQ to Kafka, Kafka connector is defined in the [eda-rt-inventory-GitOps](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops) repository under the [kconnect](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/kconnect) folder\n1. The Kafka to Cloud Object Storage Kafka (S3 bucket) connector is also defined in the same [GitOps repository/folder](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/kconnect/kafka-cos-sink-connector.yaml).\n1. The Sink connector to Elastic Search is defined in [this Kafka connector definition](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/kconnect/kafka-elastic-sink-connector.yaml).\n\n[Kafka Connect](/technology/kafka-connect/) is used to integrate external systems into Kafka. For example external systems can inject item sale messages to queue, from which a first MQ source Kafka connector \npublishes the messages to the `items` Kafka topic. Items sold events are processed by a series of streaming microservices down to different topics to keep aggregation results.\nThose topics content could be which will be used by Sink connectors\n to send records to other external systems.\n\n## A GitOps approach for solution deployment\n\nAs any cloud-native and kubernetes based solution, we use continuous integration and continuous deployment practices. From a demonstration point of view, the most interesting part is\nto execute continuous deployment using a [GitOps approach](/use-cases/gitops/). This demonstration has a [GitOps repository](https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git) \n(represented as the yellow rectangle in figure below) which defines ArgoCD apps \nto monitor and deploy the different microservices, streaming processing apps,\nand the different IBM products needed: Event Streams, MQ, API management, event-end-point management. The figure belows presents the adopted strategy:\n\n![](./images/gitops-catalog.png)\n\nThe [gitops catalog repository](https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git), represented with a blue rectangle, defines the different operator subscriptions\nfor the cloud pak for integration components. Centralizing to one repository such operator subscriptions enforces reuse between solutions.\n\nThe solution [gitops](https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git) was created with [KAM CLI](https://github.com/redhat-developer/kam) and is structured as follow:\n\n* **Boostrap** folder: to install different operators and to define the ArgoCD project named `rt-inventory`.\n* **config** folder, is for defining the ArgoCD apps and the app of apps. \n* **kconnect** folder is used to build a custom docker image of Kafka connector with MQ source, Elasticsearch sink and Cloud Object storage sink.\n* **local-demo** folder is for running the solution on your laptop using docker-compose.\n* **environments** folder, is the most important one, it uses [Kustomize](https://kustomize.io/) \nto declare environments (dev, staging) and component deployments (See next section for details). \n\n### A little bit more on environments\n\nFor demonstration purpose, only the `rt-inventory-dev` environment is detailed. One ArgoCD app: [rt-inventory-dev-env](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/config/argocd/rt-inventory-dev-env-app.yaml) is monitoring\n the folder [environments/rt-inventory-dev/env/overlays](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/environments/rt-inventory-dev/env/overlays) which define the namespace and roles,... \nEach application of the solution are also monitored by an ArgoCD and their declarations are done using standard kubernetes manifests. Here is an exampe of app tree structure:\n\n```\n── store-inventory\n│   ├── base\n│   │   └── kustomization.yaml\n│   ├── kustomization.yaml\n│   ├── overlays\n│   │   └── kustomization.yaml\n│   └── services\n│       └── store-inventory\n│           ├── base\n│           │   ├── config\n│           │   │   ├── configmap.yaml\n│           │   │   ├── deployment.yaml\n│           │   │   ├── kustomization.yaml\n│           │   │   ├── rolebinding.yaml\n│           │   │   ├── route.yaml\n│           │   │   ├── service.yaml\n│           │   │   └── serviceaccount.yaml\n│           │   └── kustomization.yaml\n│           ├── kustomization.yaml\n│           └── overlays\n│               └── kustomization.yaml\n```\n\nThe last intesting part is to declare the products used within the Cloud Pak for Integration, and deployed in the context of\nthe respective environments. Everything is in `services` folder. The tree looks like below:\n\n```\n── apicurio\n│   ├── base\n│   │   ├── es-kafka-topics.yaml\n│   │   ├── kustomization.yaml\n│   │   └── registry.yaml\n│   └── overlays\n│       └── kustomization.yaml\n├── event-endpoint\n│   ├── base\n│   │   ├── eventendpointmanager-eepm-eda.yaml\n│   │   └── kustomization.yaml\n│   └── overlays\n│       ├── kustomization.yaml\n│       └── v10.0.4.0\n│           ├── kustomization.yaml\n│           └── patch-version.yaml\n├── ibm-eventstreams\n│   ├── base\n│   │   ├── es-topics.yaml\n│   │   ├── eventstreams-dev.yaml\n│   │   ├── kustomization.yaml\n│   │   ├── scram-user.yaml\n│   │   └── tls-user.yaml\n│   └── overlays\n│       ├── kustomization.yaml\n│       └── v10.5\n│           ├── kustomization.yaml\n│           └── patch-version.yaml\n├── ibm-mq\n│   ├── base\n│   │   ├── kustomization.yaml\n│   │   └── qmgr.yaml\n│   └── overlays\n│       ├── kustomization.yaml\n│       └── v9.2.4\n│           ├── kustomization.yaml\n│           └── patch-channel.yaml\n├── kconnect\n│   ├── README.md\n│   ├── kafka-connect.yaml\n│   ├── kafka-cos-sink-connector.yaml\n│   ├── kustomization.yaml\n│   └── mq-source.json\n└── kustomization.yaml\n```\n\nWith the public docker images, and the public GitOps repository, the solution can be deployed to an OpenShift cluster with or without Cloud Pak for Integration already deployed.\n\n## Choose an option:\n\n* [Run on your laptop](/scenarios/realtime-inventory/#run-on-your-laptop)\n* [Run on OpenShift](/scenarios/realtime-inventory/#install-yourself-on-openshift) step by steps\n* [Run on OpenShift with GitOps](/scenarios/realtime-inventory/#run-with-gitops-on-openshift)\n\n### Pre-requisites for all options\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* [git CLI](https://git-scm.com/downloads).\n* Clone the Inventory lab repository: \n\n  ```sh\n  git clone  https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git\n  ```\n\n\n### Run on your laptop\n\nAs a developer or technical seller you could demonstrate this scenario on your laptop. \nThe docker images used in this solution are in public registry ([Quay.io](https://recovery.quay.io/organization/ibmcase)).\nUnder the [rt-inventory-gitops.git](https://github.com/ibm-cloud-architecture/rt-inventory-gitops.git) local-demo/kstream, we propose \ndifferent docker compose files to have different components running:\n\n* [docker-compose.yaml](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/local-demo/kstreams/docker-compose.yaml) for Event Streams, IBM MQ, Kafka Connector \nthe Store Simulator App, the Item aggregator App, the Store aggregator App, and KafDrop.\n* [docker-compose-all.yaml](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/local-demo/kstreams/docker-compose-all.yaml) same as above plus \nElasticSearch (1 node) and Kibana\n\nOnce you have cloned the gitops repository (see pre-requisites section), go under the `local-demo/kstreams` folder and run\n\n  ```sh\n  docker-compose -f docker-compose-all.yaml up -d\n  ```\n\n  To stop the demonstration do the following command:\n\n  ```sh\n  docker-compose -f docker-compose-all.yaml down\n  ```\n\nThe demonstration script is the [same as below](/scenarios/realtime-inventory/#demonstrate-the-real-time-processing), except that we use Kafdrop to visualize the content of Event Streams topics. \nSee [this section](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops#run-the-solution-locally) in the main readme for expected screen shots.\n\n### Install Yourself on OpenShift\n\nIt is possible to do a step by step deployment of the solution without any gitops deployment. This is more like a lab tutorial, where you can progress more slowly and verify\nthe result at each steps.\n\nGo to the `rt-inventory-gitops/ocp-demo-step-by-step` folder and follow the [README instructions](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/ocp-demo-step-by-step) to make\na step by step approach creating records with the simulator, see them in MQ Queue, start the Kafka Connector MQ source, to move data to Event Streams topic, then\ndo the kafka streams processing. \n\n### Run with gitops on openshift\n\nIn this approach, we propose to use a GitOps repository and deploy the solution using few scripts and `oc` CLI commands.\nIn the figure below, all the components are deployed with OpenShift GitOps. The blue components are IBM product components like Event Streams and MQ operators and some other operators, like Elastic Search.\nThe green rectangles represent the microservices and kafka connectors source or sink deployed as part of the solution.\n\n![](./images/hl-view.png)\n\nSee next guided tour for GitOps approach.\n\n--- \n\n# Use Case Guided Tour\n\n## Bootstrap GitOps\n\nBootstrapping GitOps is mostly to install the MQ, Event Streams, APIC Connect, OpenShift Gitops, and ElasticSearch operators, and do any pre-requisites like entitlement keys.\n\nWe prefer to keep the bootstrap instructions in the source repository, therefore follow [the up to date instructions](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops#bootstrap-gitops) from the gitops main readme.\n\n## Deploy the Solution\n\nOnce operators are deployed. \n\n* Deploying the full solution is by starting ArgoCD app of apps:\n\n  ```sh\n  oc apply -k config/argocd\n  ```\n\n* Access the OpenShift GitOps (ArgoCD) console\n\n  ```sh\n   chrome https://$(oc get route openshift-gitops-server -o jsonpath='{.status.ingress[].host}'  -n openshift-gitops)\n   ```\n\nThe expected set of ArgoCD apps looks like (and all should be green):\n\n  ![](./images/rt-inv-argoapps.png)\n\n  * **rt-inventory-Argo-app** is an app of apps\n  * **rt-inventory-dev-env** is for the rt-inventory-dev namespace\n  * **rt-inventory-dev-services** is for event streams, kafka connect cluster and mq deployments in dev-env namespace\n  * **rt-inventory-store-simulator-app** is for the simulator app used in the demo.\n  * **rt-inventory-item-inventory** for the item aggregator application\n  * **rt-inventory-store-inventory** for the store aggregator application\n  * **rt-inventory-dev-eepm-service** for Event End Point management\n  * **rt-inventory-dev-elastic-svc** for Elastic Search deployment\n  * **rt-inventory-dv-kibana-svc** for Kibana\n\n* Verify pods\n\n```sh\n  oc project rt-inventory-dev\n  oc get pods\n\n  NAME                                         READY   STATUS    RESTARTS   AGE\n  dev-kafka-cruise-control-6d6bf8b774-99rwl    2/2     Running   0          4d\n  dev-kafka-entity-operator-75f7bc8f5c-x4vkt   3/3     Running   0          4d\n  dev-kafka-kafka-0                            1/1     Running   0          4d\n  dev-kafka-kafka-1                            1/1     Running   0          4d\n  dev-kafka-kafka-2                            1/1     Running   0          4d\n  dev-kafka-zookeeper-0                        1/1     Running   0          4d\n  dev-kafka-zookeeper-1                        1/1     Running   0          4d\n  dev-kafka-zookeeper-2                        1/1     Running   0          4d\n  item-inventory-669fd4fffc-4fvhk             1/1     Running   0          30h\n  store-inventory-7df98556ff-f2ndq            1/1     Running   0          29h\n  store-simulator-56f8958498-mvhp9             1/1     Running   0          4d\n  dev-entity-operator-74d7dc5cfb-ksv68                              3/3     Running     0          5d21h\n  dev-ibm-es-ac-reg-77bfbf84b9-qn8ln                                2/2     Running     0          5d21h\n  dev-ibm-es-admapi-6f6bcd465c-h8scj                                1/1     Running     0          5d21h\n  dev-ibm-es-metrics-9c4679cd-n5bb7                                 1/1     Running     0          5d21h\n  dev-ibm-es-recapi-775bf874b9-gqbdn                                1/1     Running     0          5d21h\n  dev-ibm-es-ui-5d488967d4-6v2tm                                    2/2     Running     0          5d21h\n  eda-eepm-mgmt-27885b45-postgres-55b548f64f-nqs6f                  0/1     Init:0/1    0          4d23h\n  eda-eepm-mgmt-27885b45-postgres-backrest-shared-repo-7b58fjdd7s   1/1     Running     0          12d\n  eda-eepm-mgmt-27885b45-postgres-pgbouncer-5575bc4595-c9zng        1/1     Running     0          12d\n  eda-kconnect-cluster-connect-78ccb7cc56-jh2ck                     1/1     Running     0          4d17h\n  elasticsearch-es-default-0                                        1/1     Running     0          36m\n  elasticsearch-es-default-1                                        1/1     Running     0          36m\n  elasticsearch-es-default-2                                        1/1     Running     0          36m\n  kibana-kb-67f4c87c65-9whwz                                        1/1     Running     0          36m\n  store-mq-ibm-mq-0                                                 1/1     Running     1          4d19h\n  ```\n\n## Demonstrate the real-time processing\n\n1. Get the Store simulator route using the following command and start a Web Browser\n\n  ```sh\n  chrome $(oc get routes store-simulator  -o jsonpath=\"{.spec.host}\")\n  ```\n\n  You should reach the Home page of the simulator\n\n  ![](./images/home-page.png)\n\n1. Look at existing stores, using the top right `STORES` menu. This is just for viewing the data. \n\n  ![](./images/stores-view.png)\n\n1. Go to the SIMULATOR menu, and start the controlled scenario which will send predefined records:\n\n  ![](./images/start-simulation.png)\n\n  Here is the ordered list of messages sent:\n\n  | Store | Item | Action |\n  | --- | --- | --- |\n  | Store 1 | Item_1 | +10 |\n  | Store 1 | Item_2 | +5 |\n  | Store 1 | Item_3 | +15 |\n  | Store 2 | Item_1 | +10 |\n  | Store 3 | Item_1 | +10 |\n  | Store 4 | Item_1 | +10 |\n  | Store 5 | Item_1 | +10 |\n  | Store 1 | Item_2 | -5 |\n  | Store 1 | Item_3 | -5 |\n\n\n  once started a table should be displayed to present the records sent to Kafka. (The UI needs some enhancement ;-)\n\n  ![](./images/send-msgs.png) \n\n4- Verify messages are in queue:\n\n   * Access the MQ Console admin: \n\n  ```\n  chrome http://$(oc get routes store-mq-ibm-mq-web  -o jsonpath=\"{.spec.host}\")\n  ```\n\n  * Go to the QM1 Queue manager \n\n  ![](./images/qm1-qm.png)\n\n  * Select the ITEMS queue to verify the messages reach the queue. It may be possible that the Kafka Connector already consumed those messages\n  so the queue may look empty.\n\n  ![](./images/msg-in-queue.png)\n\n  Below is a view of one of those message.\n\n  ![](./images/one-msg-queue.png)\n\n\n5- Let assess if we can see the item stock cross stores: using the `item-aggregator` route, something like ` item-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud ` but completed with '/q/swagger-ui' as we want to access the API\n  \n  To get this route use the following command:\n\n  ```sh\n  chrome http://$(oc get routes item-inventory -o jsonpath=\"{.spec.host}\")\n  ```\n\n   Select the get `/api/v1/items/{itemID}` operation:\n\n  ![](./images/items-stock-req.png)\n\n  Use one of the following item id: [Item_1, Item_2, Item_3, Item_4, Item_5, Item_6, Item_7]. You should get \n  the current stock cross stores\n\n  ![](./images/item-stock-response.png)\n\n6- Let assess a store stock, for that we access the store aggregator URL: `store-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud ` with the `/q/swagger-ui` suffix.\n\n  To get this route use the following command:\n\n  ```sh\n  chrome http://$(oc get routes store-aggregator -o jsonpath=\"{.spec.host}\")\n  ```\n\n  Then using the GET on the `api/v1/stores/inventory/{storeID}`, and enter one of the available store: `[Store_1, Store_2, Store_3, Store_4, Store_5]`\n\n  ![](./images/store-stock-req.png)\n\n  The response should look like:\n\n  ![](./images/store-stock-response.png)\n\n---\n\n# Full Demo Narration\n\nWe will first go over the demonstration using the store simulator then using and end to end test script\n\n\n---\n\n# Developer Corner\n\n## What you will learn\n\n* Use Quarkus, with reactive programming API like Mutiny, and Kafka API to produce messages to Kafka\n* Same Quarkus app can generate messages to RabbitMQ using the AMQP API\n* Same Quarkus app can generate messages to IBM MQ using JMS\n* Use Quarkus and Kafka Streams to compute aggregates to build an inventory view from the stream of sale order events\n* Use the IBM MQ source connector from IBM Event messaging open source contribution\n\n\n## Solution anatomy\n\n1. The [store simulator application](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) is a Quarkus app, which generates item sales to different possible messaging middlewares ( RabbitMQ, MQ or directly to Kafka). \nThe code of this application is in this https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator. If you want to browse the code, the main readme of this project includes how to package and run this app with docker compose, \nand explains how the code works. \nThe docker image is [quay.io/ibmcase/eda-store-simulator/](https://quay.io/ibmcase/eda-store-simulator)\n1. The item inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-item-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory). \nConsider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on `item.inventory` topic. \n1. The store inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-store-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory) the output is in `store.inventory` topic. \n2. The mock up Inventory mainframe application is not implemented and we will use the MQ tools to view the message in the `item.inventory` MQ queue.\n\n \n\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/scenarios/realtime-inventory/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}