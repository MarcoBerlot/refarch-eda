---
title: Kafka Mirror Maker 2
description: Kafka Mirror Maker 2
---

This section introduces **Mirror Maker 2.0**, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - [KIP 382](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0).

It is used for disaster recovery, with a active - passive model, so it will be easier to get target cluster and consumer / producer up and runnning. It can be used to replicate over multiple clusters.
It is possible to deploy in an active - active mode where both clusters get continuous data and replicated data.

In replication, data in topic, topic states and metadata are replicated.

IBM Event Streams release 10.0 is supporting Mirror Maker 2 as part of the [geo-replication feature](https://ibm.github.io/event-streams/georeplication/about/).

## General concepts

[Mirror maker 2.0](https://strimzi.io/docs/master/#con-configuring-mirror-maker-deployment-configuration-kafka-mirror-maker) is the new solution to replicate data in topics from one Kafka cluster to another. It uses the [Kafka Connect](../kafka-connect/) framework to simplify configuration and horizontal scaling. 

The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.

![Kafka Connect](../images/mm-k-connect.png)

MirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. It runs in standalone mode, which can be used for development and test purpose, or in distributed mode for production deployment. With distribution mode, MirrorMaker 2.0 creates the following topics on the target cluster:

* *...-configs.source.internal*: This topic is used to store the connector and task configuration.
* *...-offsets.source.internal*: This topic is used to store offsets for Kafka Connect.
* *...-status.source.internal*: This topic is used to store status updates of connectors and tasks.
* *source.heartbeats*
* *source.checkpoints.internal*

A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol.  The IBM Event Streams instance runs on the Cloud.

```properties
clusters=source, target
source.bootstrap.servers=${KAFKA_SOURCE_BROKERS}
target.bootstrap.servers=${KAFKA_TARGET_BROKERS}
target.security.protocol=SASL_SSL
target.ssl.protocol=TLSv1.2
target.ssl.endpoint.identification.algorithm=https
target.sasl.mechanism=PLAIN
target.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="token" password=${KAFKA_TARGET_APIKEY};
# enable and configure individual replication flows
source->target.enabled=true
source->target.topics=products
tasks.max=10
```

* Topics to be replicated are configured via a _whitelist_ that can includes regular expression. So if you use naming convention for your topic, you could do fine grained selection of the replicated topic. It is possible to specify topic to do not replicate via the _blacklist_ property.
* White listed topics are set with the `source->target.topics` attribute of the replication flow and uses [Java regular expression](https://www.vogella.com/tutorials/JavaRegularExpressions/article.html) syntax.
* Blacklisted topics: by default the following pattern is applied:

```properties
blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
```

We can also define the _blacklist_ with the properties: `topics.blacklist`. Comma-separated lists and Java Regular Expressions are supported.

Internally, `MirrorSourceConnector` and `MirrorCheckpointConnector` will create multiple Kafka tasks (up to the value of `tasks.max` property), and `MirrorHeartbeatConnector` creates an additional task. `MirrorSourceConnector` will have one task per topic-partition combination to replicate, while `MirrorCheckpointConnector` will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the `assign()` API, so there is no consumer group used while fetching data from source topics. There is no call to `commit()` either; rebalancing occurs only when there is a new topic created that matches the _whitelist_ pattern.

Mirror Maker 2 can run on VM, or within container on kubernetes cluster. 

### Why replicating?

The classical needs for replication between clusters can be bullet listed as:

* Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The following article goes over those principals.
* Active active cluster mirroring for inter services communication: consumers and producers are on both side and consumer or produce to their local cluster.
* Moving data to read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view.
* GDPR compliance to isolate data in country and geography
* Hybrid cloud operations to share data between on-premise cluster and managed service clusters.


## Deployment examples

We encourage you to go over our [Mirror maker 2 labs](../../scenarios/kafka-mm2/) which addresses different replication scenarios. The `Connect` column defines where the Mirror Maker 2 runs.


| Scenario | Source                 | Target                 | Connect |
|-------------|------------------------|------------------------|:-------:|
| 1  | Event Streams on Cloud  | Local Kafka | Local on localhost   |


## Replication considerations

### Topic metadata replication

It is possible to disable the topic metadata replication. We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare.
By synchronizing configuration properties, the need for rebalancing is reduced.

When doing manual configuration, even if the initial topic configuration was duplicated, any dynamic changes to the topic properties are not going to be automatically propagated and the administrator needs to change the target topic. If the throughput on the source topic has increased and the number of partition was increased to support the load, then the target cluster will not have the same downstream capability which may lead to overloading (disk space or memory capacity).

Also if the consumer of a partition is expecting to process the event in order within the partition, then changing the number of partitions between source and target will make the ordering not valid any more. 

If the replication factor are set differently between the two clusters then the availability guarantees of the replicated data may be impacted and bad settings with broker failure will lead to data lost. 

Finally, it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates a backlog in the pipeline and increases the end to end latency observed by the downstream application.

### Naming convention

Mirror maker 2 sets the prefix for the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid infinite loop when doing bi-directional mirroring. At the consumer side the `subscribe()` function supports regular expression for topic name. So a code like:

```java
kafkaConsumer.subscribe("^.*accounts")
```

will listen to all the topics in the cluster having cluster name prefixed topics and the local `accounts` topic. This could be useful when we want to aggregate data from different data centers / clusters.

### Offset management

Mirror maker 2 track offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its record, it gets the offset in the partition the record was saved.

In the diagram below we have a source topic/partition A with the last write offset done by a producer to be  5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as 12 in the target partition. offset numbers do not match between partitions.
So if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment. This information is saved in the `checkpoint` topic.

![](../images/mm2-offset-mgt.png)

Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example the normal behavior is to increase the offset by one 2,3,4,5,6,7, which is mapped to 12,13,14,15,16,... on target cluster. If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the `offset-synch` topic.

The `checkpoint` and `offset_synch` topics enable replication to be fully restored from the correct offset position on failover. On the following diagram, once the cluster source is down, the consumers on the target cluster are restarted, and they will start from the last committed offset of the source, which was offset 3 that is in fact offset 12 on target replicated topic.

![](../images/mm2-offset-mgt-2.png)

### Record duplication

Exactly-once delivery is difficult to achieve in distributed system. In the case of Kafka, producer, brokers, and consumers are working together to ensure only one message is processed end to end. With coding practice and configuration settings, within a unique cluster, Kafka can guarantee exactly once processing. No duplicated records between producer and broker, and committed reads, on consumer side, are not reprocessed in case of consumer restarts.

But for cross cluster replications, the semantic is based on at least once approach. Duplicates can happen when the mirror maker source task stops before committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates. The following diagram illustrate this case, record offset 26 on target topic is a duplicate of record 25.

![](../images/mm2-dup.png)

As mirror maker 2 is a generic topic consumer, it will not participate to a read-committed process, if the topic includes duplicate messages it will propagate to the target.

In the future MM2 will be able to support exactly once by using the `checkpoint` topic on the target cluster to keep the state of the committed offset from the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part of the same transaction.

### Consumer coding

We recommend to review the [producer implementation best practices](../kafka-producers-consumers/#kafka-producers) and the [consumer considerations](../kafka-producers-consumers/#kafka-consumers).

For platform sizing, the main metric to assess, is the number of partitions in the cluster to replicate. The number of partitions and number of brokers are somehow connected as getting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster and each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size.


## sort out

To deploy MirrorMaker2 the tool, we can use the Strimzi Kafka latest docker image deployed on Openshift cluster (We address Strimzi deployment in [this note](../../technology/kafka-mirrormaker/)).

To define the clusters and topic configuration we use yaml files. One simple example to replicate from IBM Cloud Event streams to Kafka on premise is in the folder [deployments/strimzi/es-mirror-maker.properties](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/strimzi/es-mirror-maker.properties)

Using the same kafka image we can start a mirror maker container with:

```properties
clusters = source, target
source.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443
source.security.protocol=SSL
source.ssl.truststore.password=password
source.ssl.truststore.location=/home/truststore.jks
target.bootstrap.servers = kafka1:9092
# enable and configure individual replication flows
source->target.enabled = true
source->target.topics = test
```

```bash
./connect-mirror-maker.sh /home/strimzi.properties
```
When Mirror maker starts it will create some topics on source cluster to manage the offsets and topic metadata:

```
mm2-configs.target.internal                                   1            3
mm2-offset-syncs.target.internal                              1            3
mm2-offsets.target.internal                                   25           3
mm2-status.target.internal                                    5            3
```

And on the target cluster:

```
__consumer_offsets
heartbeats
mm2-configs.source.internal
mm2-offsets.source.internal
mm2-status.source.internal
source.checkpoints.internal
source.heartbeats
source.test
```

The `source.test` topic is the replicated `test` topic from the source cluster.

![](../images/mm-k-connect.png)
