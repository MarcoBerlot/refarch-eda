{"componentChunkName":"component---src-pages-use-cases-connect-rabbitmq-index-mdx","path":"/use-cases/connect-rabbitmq/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to RabbitMQ Source Connector","description":"Apache Kafka to RabbitMQ Source Connector usecase"},"relativePagePath":"/use-cases/connect-rabbitmq/index.mdx","titleType":"append","MdxNode":{"id":"c10c2cb0-8485-57da-9499-c8cf408f8e51","children":[],"parent":"d257ca0d-14bb-550b-8afa-1f34b2a6487d","internal":{"content":"---\ntitle: Kafka Connect to RabbitMQ Source Connector\ndescription: Apache Kafka to RabbitMQ Source Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 10/23/2020</strong>\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Just run it!</AnchorLink>\n<AnchorLink>Deploy on OpenShift</AnchorLink>\n</AnchorLinks>\n\nThis hands-on lab demonstrates how to use IBM RabbitMQ source connector to inject message to Event Streams On Premise. \nWe are using the [IBM messaging github: source Kafka connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source) open sourced component. The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe following diagram illustrates the component of this demo / lab:\n\n![0](./images/comp-view.png)\n\nThe configurations used in this use case are in the `refarch-eda-tools` repository that you should clone:\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n```\n\n## Just run it!\n\nFor demonstration purpose, we have defined a docker compose file and a set of images ready to be used. So it will be eaiery to demonstrate the scenario of sending messages to Rabbit MQ `items` queue, have a Kafka connector configured and a simple Kafka consumer which consumes JSon doc from `items` topic. \n\nTo send message to RabbitMQ, we have implemented a simple store simulator, to send item sale messages for a set of stores. The code is under [eda-store-simulator](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) repository and we have also uploaded the image [into dockerhub ibmcase/eda-store-simulator](https://hub.docker.com/repository/docker/ibmcase/eda-store-simulator).\n\nTo quickly start a local demonstration environment, the [github repository for this lab](https://github.com/ibm-cloud-architecture/refarch-eda-tools) includes a [docker compose file](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/rabbitmq-source-lab/docker-compose.yaml) under the `labs/rabbitmq-source-lab` folder.\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\ncd refarch-eda-tools/labs/rabbitmq-source-lab\ndocker-compose up &\ndocker ps \n\nIMAGE                              COMMAND                   PORT                  \nibmcase/kconnect:v1.0.0            \"./bin/connect-distr…\"    0.0.0.0:8083->8083/  \nstrimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"    0.0.0.0:9092->9092/\nibmcase/eda-store-simulator        \"/deployments/run-ja…\"    0.0.0.0:8080->8080/\nrabbitmq:3-management              \"docker-entrypoint.s…\"    0.0.0.0:5672->5672/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp   \nstrimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/zookeepe…\"    0.0.0.0:2181->2181/                                                                                      \n```\n\n* Once the different containers are started, you can deploy the source connector as describe [in this section](#) but which come to a simple POST\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\n```\n\n* And then looking at the deployed connectors using [http://localhost:8083/connectors](http://localhost:8083/connectors) or the RabbitMQ connector with [http://localhost:8083/connectors/RabbitMQSourceConnector](http://localhost:8083/connectors/RabbitMQSourceConnector).\n\n* Send some records using the simulator using the [http://localhost:8080/#/simulator](http://localhost:8080/#/simulator) URL and the user interface: \n\n ![1](./images/store-simul-rmq.png)\n\n\n* Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:1567/](http://localhost:1567/) using the rabbit-user/rabbit-pass login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nGo to the Queue tab and select the `items` queue:\n\n![9](./images/rabbitmq-item-queue-2.png)\n\n* Now verify the Kafka connect-* topics are created successfully:\n\n ```shell\n docker run -ti --network kafkanet strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list\"\n # you should get at least\n __consumer_offsets\n connect-configs\n connect-offsets\n connect-status\n items\n ```\n\n* Verify the message arrived to the `items` in kafka topic:\n\n ```shell\n docker run -ti --network kafkanet strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic items --from-beginning\"\n\n # You may have a trace like which maps the messages sent \n \"{\\\"id\\\":0,\\\"price\\\":11.0,\\\"quantity\\\":6,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_3\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395325\\\",\\\"type\\\":\\\"SALE\\\"}\"\n\"{\\\"id\\\":1,\\\"price\\\":68.5,\\\"quantity\\\":0,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_1\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395447\\\",\\\"type\\\":\\\"SALE\\\"}\"\n ```\n\nYou validated the solution work end to end. Now we can try to deploy those components to OpenShift.\n\n## Deploy on OpenShift\n\nIn this section, we want to run and deploy the connector and Rabbit MQ on OpenShift using Event Streams on Premise.\n\n### Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req from the [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites), like for example the deployment of the store simulator._\n\n## Deploy Rabbit MQ\n\n## Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n","type":"Mdx","contentDigest":"8ece8325e83c2fc13605a48a0dbaf14e","counter":641,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Kafka Connect to RabbitMQ Source Connector","description":"Apache Kafka to RabbitMQ Source Connector usecase"},"exports":{},"rawBody":"---\ntitle: Kafka Connect to RabbitMQ Source Connector\ndescription: Apache Kafka to RabbitMQ Source Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 10/23/2020</strong>\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Just run it!</AnchorLink>\n<AnchorLink>Deploy on OpenShift</AnchorLink>\n</AnchorLinks>\n\nThis hands-on lab demonstrates how to use IBM RabbitMQ source connector to inject message to Event Streams On Premise. \nWe are using the [IBM messaging github: source Kafka connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source) open sourced component. The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe following diagram illustrates the component of this demo / lab:\n\n![0](./images/comp-view.png)\n\nThe configurations used in this use case are in the `refarch-eda-tools` repository that you should clone:\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n```\n\n## Just run it!\n\nFor demonstration purpose, we have defined a docker compose file and a set of images ready to be used. So it will be eaiery to demonstrate the scenario of sending messages to Rabbit MQ `items` queue, have a Kafka connector configured and a simple Kafka consumer which consumes JSon doc from `items` topic. \n\nTo send message to RabbitMQ, we have implemented a simple store simulator, to send item sale messages for a set of stores. The code is under [eda-store-simulator](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) repository and we have also uploaded the image [into dockerhub ibmcase/eda-store-simulator](https://hub.docker.com/repository/docker/ibmcase/eda-store-simulator).\n\nTo quickly start a local demonstration environment, the [github repository for this lab](https://github.com/ibm-cloud-architecture/refarch-eda-tools) includes a [docker compose file](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/rabbitmq-source-lab/docker-compose.yaml) under the `labs/rabbitmq-source-lab` folder.\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\ncd refarch-eda-tools/labs/rabbitmq-source-lab\ndocker-compose up &\ndocker ps \n\nIMAGE                              COMMAND                   PORT                  \nibmcase/kconnect:v1.0.0            \"./bin/connect-distr…\"    0.0.0.0:8083->8083/  \nstrimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"    0.0.0.0:9092->9092/\nibmcase/eda-store-simulator        \"/deployments/run-ja…\"    0.0.0.0:8080->8080/\nrabbitmq:3-management              \"docker-entrypoint.s…\"    0.0.0.0:5672->5672/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp   \nstrimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/zookeepe…\"    0.0.0.0:2181->2181/                                                                                      \n```\n\n* Once the different containers are started, you can deploy the source connector as describe [in this section](#) but which come to a simple POST\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\n```\n\n* And then looking at the deployed connectors using [http://localhost:8083/connectors](http://localhost:8083/connectors) or the RabbitMQ connector with [http://localhost:8083/connectors/RabbitMQSourceConnector](http://localhost:8083/connectors/RabbitMQSourceConnector).\n\n* Send some records using the simulator using the [http://localhost:8080/#/simulator](http://localhost:8080/#/simulator) URL and the user interface: \n\n ![1](./images/store-simul-rmq.png)\n\n\n* Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:1567/](http://localhost:1567/) using the rabbit-user/rabbit-pass login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nGo to the Queue tab and select the `items` queue:\n\n![9](./images/rabbitmq-item-queue-2.png)\n\n* Now verify the Kafka connect-* topics are created successfully:\n\n ```shell\n docker run -ti --network kafkanet strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list\"\n # you should get at least\n __consumer_offsets\n connect-configs\n connect-offsets\n connect-status\n items\n ```\n\n* Verify the message arrived to the `items` in kafka topic:\n\n ```shell\n docker run -ti --network kafkanet strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic items --from-beginning\"\n\n # You may have a trace like which maps the messages sent \n \"{\\\"id\\\":0,\\\"price\\\":11.0,\\\"quantity\\\":6,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_3\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395325\\\",\\\"type\\\":\\\"SALE\\\"}\"\n\"{\\\"id\\\":1,\\\"price\\\":68.5,\\\"quantity\\\":0,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_1\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395447\\\",\\\"type\\\":\\\"SALE\\\"}\"\n ```\n\nYou validated the solution work end to end. Now we can try to deploy those components to OpenShift.\n\n## Deploy on OpenShift\n\nIn this section, we want to run and deploy the connector and Rabbit MQ on OpenShift using Event Streams on Premise.\n\n### Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req from the [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites), like for example the deployment of the store simulator._\n\n## Deploy Rabbit MQ\n\n## Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-rabbitmq/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}