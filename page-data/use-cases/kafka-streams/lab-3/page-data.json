{"componentChunkName":"component---src-pages-use-cases-kafka-streams-lab-3-index-mdx","path":"/use-cases/kafka-streams/lab-3/","result":{"pageContext":{"frontmatter":{"title":"Kafka Streams Test Lab 3","description":"Using Kafka Streams to compute real time inventory stock"},"relativePagePath":"/use-cases/kafka-streams/lab-3/index.mdx","titleType":"append","MdxNode":{"id":"77708b60-f77b-5b8e-8070-251c4cbab58a","children":[],"parent":"2c7111bc-107e-5563-a59a-ee1cee267695","internal":{"content":"---\ntitle: Kafka Streams Test Lab 3\ndescription: Using Kafka Streams to compute real time inventory stock\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Develop the application</AnchorLink>\n    <AnchorLink>Integration Tests</AnchorLink>\n    <AnchorLink>Deploy to OpenShift</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n\n- In this lab scenario we're going to use [Quarkus](https://quarkus.io) to develop the core application with Kafka streams api and microprofile reactive messaging.\n- We will be testing using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TestDriver to mimic a Topology, a Stream and Table.\n\nThe requirements to address are:\n\n- consume item sold from items topic, item has unique key. Item event has store information\n- compute for each item its current stock cross store\n- compute the store stock for each item\n- generate inventory event for store - item - stock\n- expose APIs to get stock for a store or for an item\n\n## Scenario Prerequisites\n\n**Java**\n- For the purposes of this lab we suggest Java 11+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n**OpenShift Container Platform**\n- v4.4.x\n\n**IBM Cloud Pak for Integration**\n- CP4I2020.2\n\n**IBM Event Streams**\n- The section on use with Event Streams on CP4I assumes Event Streams v10. If using a previous version such as ESv2019.4.2, there are some differences to how you would configure `application.properties` to establish a connection.\n\n**Code Source**: clone the following git repository: `git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory`.\n\nThe final source code is in this project: [https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory).\n\n## Use application as-is\n\nIf you do not want to develop the application, you can deploy it on OpenShift using our docker image. See the [repository readme](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory) to do so.\n\n## Develop the application\n\n### Setting up the Quarkus Application\n\n- We will bootstrap the Quarkus application with the following Maven command (See [Quarkus maven tooling guide](https://quarkus.io/guides/maven-tooling#project-creation) for more information):\n\n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.7.2.Final:create \\\n    -DprojectGroupId=ibm.garage \\\n    -DprojectArtifactId=quarkus-kstreams-lab3 \\\n    -Dextensions=\"resteasy-jsonb,resteasy-mutiny,smallrye-health,quarkus-smallrye-openapi,openshift\"\n```\n\nYou can replace the `projectGroupId, projectArtifactId` fields as you like.\n- recall that is if you want to add an extension do something like: `./mvnw Quarkus:add-extension -Dextensions=\"kafka\"`\n\n### Start the dev mode \n\n```shell\n./mvnw quarkus:dev\n```\nGoing to the URL [http://localhost:8080/](http://localhost:8080/) will generate an exception, as we need to add configuration for Kafka-Streams.\n\nLet add the minimum into the `application.properties`\n\n```properties\nquarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n\nquarkus.log.console.level=INFO\nquarkus.log.console.enable=true\nquarkus.http.port=8080\nquarkus.swagger-ui.always-include=true\nquarkus.openshift.expose=true\n```\n\nNow the application should display a basic web page. As we defined to use OpenAPI the following address should give us the API defined: [http://localhost:8080/swagger-ui/#/default](http://localhost:8080/swagger-ui/#/default).\n\nand health works too: [http://localhost:8080/health](http://localhost:8080/health)\n\nLet add a simple resource under the following package `ibm.garage.lab3.api`\n\n```java\nmport javax.enterprise.context.ApplicationScoped;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\n\nimport io.smallrye.mutiny.Uni;\nimport io.vertx.core.json.JsonObject;\n\n@ApplicationScoped\n@Path(\"/inventory\")\npublic class InventoryResource {\n    \n    @GET\n    @Path(\"/store/{storeID}\")\n    @Produces(MediaType.APPLICATION_JSON)\n    public  Uni<JsonObject> getStock(@PathParam(\"storeID\") String storeID) {\n            JsonObject stock = new JsonObject(\"{\\\"name\\\": \\\"hello you\\\", \\\"id\\\": \\\"\" + storeID + \"\\\"}\");\n            return Uni.createFrom().item( stock);\n    }\n}\n```\n\nOutside of the traditional JAXRS annotation, we are using Uni class from [Mutiny](https://smallrye.io/smallrye-mutiny/) to get the API being asynchronous non-blocking.\n\nA refresh [http://localhost:8080/swagger-ui/](http://localhost:8080/swagger-ui/) will get the new API which should work.\n\n### Deploy to OpenShift using s2i\n\nBefore going too far in the development, let deploy this simple app to OpenShift. We assume you are logged to the cluster via `oc login...`\n\nThe following command should package the application and create OpenShift manifests, build a docker images and push it to OpenShift Private registry.\n\n```shell\n./mvnw package -Dquarkus.kubernetes.deploy=true -Dquarkus.container-image.build=true\n```\n\nIt can take some seconds to build and deploy: `oc get pods -w` lets you see the build pods and the running app once the build is done. As we expose the application an OpenShift route was created. The url is at the end of the build output:\n`The deployed application can be accessed at: http://quarkus-kstreams-lab3...`\n\nFor example this was the URL to access the swagger:\n\n[http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/](http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/)\n\n\n### Define the domain entities\n\nUnder the `src/main/java/../domain` folder add the two classes representing the business entities we are using:\n\n```Java\npublic class Item {\n    public static String RESTOCK = \"RESTOCK\";\n    public static String SALE = \"SALE\";\n    public String storeName;\n    public String sku;\n    public int quantity;\n    public String type;\n    public Double price;\n    public String timestamp;\n\n    public Item(){}\n}\n```\n\nThis item will also being used for event structure on `items` topic. The type attribute is to specify if this is a sale event or a restock event.\n\nThe inventory per store includes a map of item.sku and quantity.\n\n```Java\npublic class Inventory {\n    public String storeName;\n    public HashMap<String,Long> stock = new HashMap<String,Long>();\n    public Inventory(){}\n}\n```\n\nAs part of the logic we want to add methods in the Inventory class to update the quantity given an item. So the two following methods are added\n\n```Java\npublic Inventory updateStockQuantity(String k, Item newValue) {\n        this.storeName = k;\n        if (newValue.type.equals(\"SALE\")) \n            newValue.quantity=-newValue.quantity;\n        return this.updateStock(newValue.sku,newValue.quantity);\n    }\n\n    public Inventory updateStock(String sku, long newV) {\n        if (stock.get(sku) == null) {\n            stock.put(sku, Long.valueOf(newV));\n        } else {\n            Long currentValue = stock.get(sku);\n            stock.put(sku, Long.valueOf(newV) + currentValue );\n        }\n        return this;\n    }\n```\n\nModify the InventoryResource to return the inventory instead of JsonObject.\n\n```java\npublic  Uni<Inventory> getStock(@PathParam(\"storeID\") String storeID) {\n        Inventory stock = new Inventory();\n        stock.storeName = storeID;\n        Item newItem = new Item();\n        newItem.quantity = 10;\n        newItem.sku=\"item-01\";\n        newItem.type = Item.RESTOCK;\n        stock.updateStockQuantity(storeID, newItem);\n            return Uni.createFrom().item( stock);\n    }\n```\n\nYou should get a json like:\n\n```json\n{\"stock\": {\n    \"item-01\": 10\n  },\n  \"storeName\": \"Store-A\"\n}\n```\n\nWe are good with the REST end point. Lets add Kafka-streams\n\n### Add Kafka\n\n```shell\n./mvnw Quarkus:add-extension -Dextensions=\"kafka,kafka-streams,smallrye-reactive-messaging-kafka\"\n```\n\nSince we will be using the Kafka Streams testing functionality we will need to edit the `pom.xml` to add\nthe dependency to our project. Open `pom.xml` and add the following.\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n```\n\nModify the properties to add kafka, kafka-streams and reactive messaging parameters like\n\n```properties\nquarkus.kafka-streams.auto.offset.reset=latest\nquarkus.kafka-streams.health.enabled=true\nquarkus.kafka-streams.consumer.session.timeout.ms=7000\nquarkus.kafka-streams.consumer.heartbeat.interval.ms=200\nquarkus.kafka-streams.application-id=item-aggregator\nquarkus.kafka-streams.topics=items,inventory\n\nmp.messaging.incoming.item-channel.connector=smallrye-kafka\nmp.messaging.incoming.item-channel.topic=items\nmp.messaging.incoming.item-channel.group.id=item-aggregator\n```\n\n### Define an item deserializer\n\nThe item needs to be deserialized to a Item bean, so we add a new class:\n\n```java\nimport io.quarkus.kafka.client.serialization.JsonbDeserializer;\n\npublic class ItemDeserializer extends JsonbDeserializer<Item> {\n    public ItemDeserializer(){\n        // pass the class to the parent.\n        super(Item.class);\n    }\n}\n```\n\nand a declaration in the properties file (change the class name if needed):\n\n```properties\nmp.messaging.incoming.item-channel.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nmp.messaging.incoming.item-channel.value.deserializer=ibm.garage.lab3.infrastructure.ItemDeserializer\n```\n\n### Define the topology\n\nWhile in dev mode, we can add the `StoreInventoryAgent` class under the infrastructure folder. This class will define the topology to consume messages from the `items` topic. The Serdes are class to support the serialization and deserialization of the beans we define as part of the event model.\n\nWe will start just by having a print out topology to get the plumbing done. So it will consume items topic:\n```Java\n@ApplicationScoped\npublic class StoreInventoryAgent {\n    \n    public String itemSoldTopicName = \"items\";\n\n    private JsonbSerde<Item> itemSerde = new JsonbSerde<>(Item.class);\n   \n\n    public Topology buildTopology(){\n        StreamsBuilder builder = new StreamsBuilder();\n        \n        builder.stream(itemSoldTopicName, \n            Consumed.with(Serdes.String(), itemSerde))\n            .peek( (k,v) -> System.out.println(k));\n\n        return builder.build();\n    }\n}\n```\n\n### Connect to Event Streams\n\nWe need to complete the configuration to connect to the remote Event Streams running on OpenShift.\n\n* Create the items and inventory topics, following the instructions as described [in this note](../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:\n\n ```shell\n cloudctl es topic-create --name items --partitions 3 --replication-factor 3\n cloudctl es topic-create --name inventory --partitions 1 --replication-factor 3\n cloudctl es topics\n ```\n\n* Get a user and scram-sha-512 password to remotely connect [see product documentation](https://ibm.github.io/event-streams/getting-started/connecting/) on how to do it, or use our [quick summary here](http://localhost:8000/use-cases/overview/pre-requisites#get-shram-user).\n\n* Get Server certificate. See our [quick summary here](http://localhost:8000/use-cases/overview/pre-requisites#get-tls-server-public-certificate)\n\n* Modify the properties file to define the kafka connection properties:\n\n ```properties\nkafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\nkafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\nkafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n ```\n\n* Define a file like `.env` to set environment variables, and modify the settings accordingly\n\n    ```\n    KAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\n    KAFKA_USER=\n    KAFKA_PASSWORD=\n    KAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\n    KAFKA_CERT_PWD=\n    SECURE_PROTOCOL=SASL_SSL\n    ```\n\n* Restart the quarkus in dev mode\n\n ```shell\n source .env\n ./mvnw quarkus:dev\n ```\n\n normally you should not get any exception and a trace like\n\n ```\n    AdminClientConfig values: \n    bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]\n    client.dns.lookup = default\n    client.id = \n    connections.max.idle.ms = 300000\n    default.api.timeout.ms = 60000\n    metadata.max.age.ms = 300000\n    metric.reporters = []\n    metrics.num.samples = 2\n    metrics.recording.level = INFO\n    metrics.sample.window.ms = 30000\n ```\n\n### Complete the topology\n\nThe requirements can be bullet listed as:\n\n* out-topic: inventory: contains the invenory stock events.\n* Ktable <storeID, <itemID, count> with store. To keep store inventory\n* Interactive query to get data from store and expose the result as reactive REST resource.\n\nTo update the buildTopology function by getting the store and build a Ktable\n\n```Java\n KTable<String,Inventory> inventory = builder.stream(itemSoldTopicName, \n                        Consumed.with(Serdes.String(), itemSerde))\n            // use store name as key\n            .map((k,v) ->  new KeyValue<>(v.storeName, v))\n            .groupByKey(Grouped.with(Serdes.String(),itemSerde))\n       \n```\n\nThen the operation to take this <storeName, item> record and transform it to Inventory instance, and update existing inventory entry is the `aggregate` function:\n\n```Java\n.aggregate(\n      () ->  new Inventory(), // initializer\n      (k , newItem, currentInventory) \n            -> currentInventory.updateStockQuantity(k,newItem), \n      Materialized.<String,Inventory,KeyValueStore<Bytes,byte[]>>as(StoreInventoryAgent.STOCKS_STORE_NAME)\n            .withKeySerde(Serdes.String())\n            .withValueSerde(inventorySerde));\n```\n\nFirst row is to initialize new key, record with an empty Inventory object. \nThe second row is executed when a key is found (first key too), and update the currentInventory with the new quantity from the item. The outcome of this is a Ktable<storeName, Inventory> \nThe content is materialized in a store.\n\nThe update operation on the inventory is a simple hashmap update: \n\n```Java\n// in Inventory Class\npublic Inventory updateStockQuantity(String k, Item newValue) {\n        this.storeName = k;\n        if (newValue.type.equals(\"SALE\")) \n            newValue.quantity=-newValue.quantity;\n        return this.updateStock(newValue.sku,newValue.quantity);\n    }\n\n    public Inventory updateStock(String sku, long newV) {\n        if (stock.get(sku) == null) {\n            stock.put(sku, Long.valueOf(newV));\n        } else {\n            Long currentValue = stock.get(sku);\n            stock.put(sku, Long.valueOf(newV) + currentValue );\n        }\n        return this;\n    }\n```\n\nFinally the KTable is streamed out to the inventory topic:\n\n```Java\ninventory.toStream()\n            .to(inventoryStockTopicName,\n                Produced.with(Serdes.String(),inventorySerde));\n      \n```\n\nThe KTable is also materialized as a store that can be accessed via an API like `/inventory/store/{storeid}/{itemid}` using interactive query.\n\nAs items topic can be partitioned, a REST call may not reach the good end points, as the local store may not have the expected queried key. So the code is using interactive query to get access to the local state stores or return a URL of a remote store where the records for the given key are.\n\n## Integration tests\n\n For running the integration test, we propose to copy the e2e folder from the solution repository and follow the [readme instructions section end-to-end-testing ](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing).\n\n## Deploy to OpenShift\n\nBe sure to have done [the steps described here](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to get user credentials and Server side certificate. \nWe need to define a config map manifest (src/main/kubernetes/configmap.yml) for the user and brokers internal URL. The userID used is one of the scram-sha-512 ones.\n\n ```yaml\n apiVersion: v1\n kind: ConfigMap\n metadata:\n    name: item-aggregator-cm\n data:\n    KAFKA_USER: app-demo\n    SECURE_PROTOCOL: SASL_SSL\n    KAFKA_BROKERS: minimal-prod-kafka-bootstrap.eventstreams.svc:9093\n ```\n\nConfigure it with `oc apply -f src/main/kubernetes/configmap.yml`.","type":"Mdx","contentDigest":"dffbd5a90212f0af57d419af89778a9c","counter":629,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Streams Test Lab 3\ndescription: Using Kafka Streams to compute real time inventory stock\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Develop the application</AnchorLink>\n    <AnchorLink>Integration Tests</AnchorLink>\n    <AnchorLink>Deploy to OpenShift</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n\n- In this lab scenario we're going to use [Quarkus](https://quarkus.io) to develop the core application with Kafka streams api and microprofile reactive messaging.\n- We will be testing using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TestDriver to mimic a Topology, a Stream and Table.\n\nThe requirements to address are:\n\n- consume item sold from items topic, item has unique key. Item event has store information\n- compute for each item its current stock cross store\n- compute the store stock for each item\n- generate inventory event for store - item - stock\n- expose APIs to get stock for a store or for an item\n\n## Scenario Prerequisites\n\n**Java**\n- For the purposes of this lab we suggest Java 11+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n**OpenShift Container Platform**\n- v4.4.x\n\n**IBM Cloud Pak for Integration**\n- CP4I2020.2\n\n**IBM Event Streams**\n- The section on use with Event Streams on CP4I assumes Event Streams v10. If using a previous version such as ESv2019.4.2, there are some differences to how you would configure `application.properties` to establish a connection.\n\n**Code Source**: clone the following git repository: `git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory`.\n\nThe final source code is in this project: [https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory).\n\n## Use application as-is\n\nIf you do not want to develop the application, you can deploy it on OpenShift using our docker image. See the [repository readme](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory) to do so.\n\n## Develop the application\n\n### Setting up the Quarkus Application\n\n- We will bootstrap the Quarkus application with the following Maven command (See [Quarkus maven tooling guide](https://quarkus.io/guides/maven-tooling#project-creation) for more information):\n\n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.7.2.Final:create \\\n    -DprojectGroupId=ibm.garage \\\n    -DprojectArtifactId=quarkus-kstreams-lab3 \\\n    -Dextensions=\"resteasy-jsonb,resteasy-mutiny,smallrye-health,quarkus-smallrye-openapi,openshift\"\n```\n\nYou can replace the `projectGroupId, projectArtifactId` fields as you like.\n- recall that is if you want to add an extension do something like: `./mvnw Quarkus:add-extension -Dextensions=\"kafka\"`\n\n### Start the dev mode \n\n```shell\n./mvnw quarkus:dev\n```\nGoing to the URL [http://localhost:8080/](http://localhost:8080/) will generate an exception, as we need to add configuration for Kafka-Streams.\n\nLet add the minimum into the `application.properties`\n\n```properties\nquarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n\nquarkus.log.console.level=INFO\nquarkus.log.console.enable=true\nquarkus.http.port=8080\nquarkus.swagger-ui.always-include=true\nquarkus.openshift.expose=true\n```\n\nNow the application should display a basic web page. As we defined to use OpenAPI the following address should give us the API defined: [http://localhost:8080/swagger-ui/#/default](http://localhost:8080/swagger-ui/#/default).\n\nand health works too: [http://localhost:8080/health](http://localhost:8080/health)\n\nLet add a simple resource under the following package `ibm.garage.lab3.api`\n\n```java\nmport javax.enterprise.context.ApplicationScoped;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\n\nimport io.smallrye.mutiny.Uni;\nimport io.vertx.core.json.JsonObject;\n\n@ApplicationScoped\n@Path(\"/inventory\")\npublic class InventoryResource {\n    \n    @GET\n    @Path(\"/store/{storeID}\")\n    @Produces(MediaType.APPLICATION_JSON)\n    public  Uni<JsonObject> getStock(@PathParam(\"storeID\") String storeID) {\n            JsonObject stock = new JsonObject(\"{\\\"name\\\": \\\"hello you\\\", \\\"id\\\": \\\"\" + storeID + \"\\\"}\");\n            return Uni.createFrom().item( stock);\n    }\n}\n```\n\nOutside of the traditional JAXRS annotation, we are using Uni class from [Mutiny](https://smallrye.io/smallrye-mutiny/) to get the API being asynchronous non-blocking.\n\nA refresh [http://localhost:8080/swagger-ui/](http://localhost:8080/swagger-ui/) will get the new API which should work.\n\n### Deploy to OpenShift using s2i\n\nBefore going too far in the development, let deploy this simple app to OpenShift. We assume you are logged to the cluster via `oc login...`\n\nThe following command should package the application and create OpenShift manifests, build a docker images and push it to OpenShift Private registry.\n\n```shell\n./mvnw package -Dquarkus.kubernetes.deploy=true -Dquarkus.container-image.build=true\n```\n\nIt can take some seconds to build and deploy: `oc get pods -w` lets you see the build pods and the running app once the build is done. As we expose the application an OpenShift route was created. The url is at the end of the build output:\n`The deployed application can be accessed at: http://quarkus-kstreams-lab3...`\n\nFor example this was the URL to access the swagger:\n\n[http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/](http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/)\n\n\n### Define the domain entities\n\nUnder the `src/main/java/../domain` folder add the two classes representing the business entities we are using:\n\n```Java\npublic class Item {\n    public static String RESTOCK = \"RESTOCK\";\n    public static String SALE = \"SALE\";\n    public String storeName;\n    public String sku;\n    public int quantity;\n    public String type;\n    public Double price;\n    public String timestamp;\n\n    public Item(){}\n}\n```\n\nThis item will also being used for event structure on `items` topic. The type attribute is to specify if this is a sale event or a restock event.\n\nThe inventory per store includes a map of item.sku and quantity.\n\n```Java\npublic class Inventory {\n    public String storeName;\n    public HashMap<String,Long> stock = new HashMap<String,Long>();\n    public Inventory(){}\n}\n```\n\nAs part of the logic we want to add methods in the Inventory class to update the quantity given an item. So the two following methods are added\n\n```Java\npublic Inventory updateStockQuantity(String k, Item newValue) {\n        this.storeName = k;\n        if (newValue.type.equals(\"SALE\")) \n            newValue.quantity=-newValue.quantity;\n        return this.updateStock(newValue.sku,newValue.quantity);\n    }\n\n    public Inventory updateStock(String sku, long newV) {\n        if (stock.get(sku) == null) {\n            stock.put(sku, Long.valueOf(newV));\n        } else {\n            Long currentValue = stock.get(sku);\n            stock.put(sku, Long.valueOf(newV) + currentValue );\n        }\n        return this;\n    }\n```\n\nModify the InventoryResource to return the inventory instead of JsonObject.\n\n```java\npublic  Uni<Inventory> getStock(@PathParam(\"storeID\") String storeID) {\n        Inventory stock = new Inventory();\n        stock.storeName = storeID;\n        Item newItem = new Item();\n        newItem.quantity = 10;\n        newItem.sku=\"item-01\";\n        newItem.type = Item.RESTOCK;\n        stock.updateStockQuantity(storeID, newItem);\n            return Uni.createFrom().item( stock);\n    }\n```\n\nYou should get a json like:\n\n```json\n{\"stock\": {\n    \"item-01\": 10\n  },\n  \"storeName\": \"Store-A\"\n}\n```\n\nWe are good with the REST end point. Lets add Kafka-streams\n\n### Add Kafka\n\n```shell\n./mvnw Quarkus:add-extension -Dextensions=\"kafka,kafka-streams,smallrye-reactive-messaging-kafka\"\n```\n\nSince we will be using the Kafka Streams testing functionality we will need to edit the `pom.xml` to add\nthe dependency to our project. Open `pom.xml` and add the following.\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n```\n\nModify the properties to add kafka, kafka-streams and reactive messaging parameters like\n\n```properties\nquarkus.kafka-streams.auto.offset.reset=latest\nquarkus.kafka-streams.health.enabled=true\nquarkus.kafka-streams.consumer.session.timeout.ms=7000\nquarkus.kafka-streams.consumer.heartbeat.interval.ms=200\nquarkus.kafka-streams.application-id=item-aggregator\nquarkus.kafka-streams.topics=items,inventory\n\nmp.messaging.incoming.item-channel.connector=smallrye-kafka\nmp.messaging.incoming.item-channel.topic=items\nmp.messaging.incoming.item-channel.group.id=item-aggregator\n```\n\n### Define an item deserializer\n\nThe item needs to be deserialized to a Item bean, so we add a new class:\n\n```java\nimport io.quarkus.kafka.client.serialization.JsonbDeserializer;\n\npublic class ItemDeserializer extends JsonbDeserializer<Item> {\n    public ItemDeserializer(){\n        // pass the class to the parent.\n        super(Item.class);\n    }\n}\n```\n\nand a declaration in the properties file (change the class name if needed):\n\n```properties\nmp.messaging.incoming.item-channel.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nmp.messaging.incoming.item-channel.value.deserializer=ibm.garage.lab3.infrastructure.ItemDeserializer\n```\n\n### Define the topology\n\nWhile in dev mode, we can add the `StoreInventoryAgent` class under the infrastructure folder. This class will define the topology to consume messages from the `items` topic. The Serdes are class to support the serialization and deserialization of the beans we define as part of the event model.\n\nWe will start just by having a print out topology to get the plumbing done. So it will consume items topic:\n```Java\n@ApplicationScoped\npublic class StoreInventoryAgent {\n    \n    public String itemSoldTopicName = \"items\";\n\n    private JsonbSerde<Item> itemSerde = new JsonbSerde<>(Item.class);\n   \n\n    public Topology buildTopology(){\n        StreamsBuilder builder = new StreamsBuilder();\n        \n        builder.stream(itemSoldTopicName, \n            Consumed.with(Serdes.String(), itemSerde))\n            .peek( (k,v) -> System.out.println(k));\n\n        return builder.build();\n    }\n}\n```\n\n### Connect to Event Streams\n\nWe need to complete the configuration to connect to the remote Event Streams running on OpenShift.\n\n* Create the items and inventory topics, following the instructions as described [in this note](../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:\n\n ```shell\n cloudctl es topic-create --name items --partitions 3 --replication-factor 3\n cloudctl es topic-create --name inventory --partitions 1 --replication-factor 3\n cloudctl es topics\n ```\n\n* Get a user and scram-sha-512 password to remotely connect [see product documentation](https://ibm.github.io/event-streams/getting-started/connecting/) on how to do it, or use our [quick summary here](http://localhost:8000/use-cases/overview/pre-requisites#get-shram-user).\n\n* Get Server certificate. See our [quick summary here](http://localhost:8000/use-cases/overview/pre-requisites#get-tls-server-public-certificate)\n\n* Modify the properties file to define the kafka connection properties:\n\n ```properties\nkafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\nkafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\nkafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n ```\n\n* Define a file like `.env` to set environment variables, and modify the settings accordingly\n\n    ```\n    KAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\n    KAFKA_USER=\n    KAFKA_PASSWORD=\n    KAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\n    KAFKA_CERT_PWD=\n    SECURE_PROTOCOL=SASL_SSL\n    ```\n\n* Restart the quarkus in dev mode\n\n ```shell\n source .env\n ./mvnw quarkus:dev\n ```\n\n normally you should not get any exception and a trace like\n\n ```\n    AdminClientConfig values: \n    bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]\n    client.dns.lookup = default\n    client.id = \n    connections.max.idle.ms = 300000\n    default.api.timeout.ms = 60000\n    metadata.max.age.ms = 300000\n    metric.reporters = []\n    metrics.num.samples = 2\n    metrics.recording.level = INFO\n    metrics.sample.window.ms = 30000\n ```\n\n### Complete the topology\n\nThe requirements can be bullet listed as:\n\n* out-topic: inventory: contains the invenory stock events.\n* Ktable <storeID, <itemID, count> with store. To keep store inventory\n* Interactive query to get data from store and expose the result as reactive REST resource.\n\nTo update the buildTopology function by getting the store and build a Ktable\n\n```Java\n KTable<String,Inventory> inventory = builder.stream(itemSoldTopicName, \n                        Consumed.with(Serdes.String(), itemSerde))\n            // use store name as key\n            .map((k,v) ->  new KeyValue<>(v.storeName, v))\n            .groupByKey(Grouped.with(Serdes.String(),itemSerde))\n       \n```\n\nThen the operation to take this <storeName, item> record and transform it to Inventory instance, and update existing inventory entry is the `aggregate` function:\n\n```Java\n.aggregate(\n      () ->  new Inventory(), // initializer\n      (k , newItem, currentInventory) \n            -> currentInventory.updateStockQuantity(k,newItem), \n      Materialized.<String,Inventory,KeyValueStore<Bytes,byte[]>>as(StoreInventoryAgent.STOCKS_STORE_NAME)\n            .withKeySerde(Serdes.String())\n            .withValueSerde(inventorySerde));\n```\n\nFirst row is to initialize new key, record with an empty Inventory object. \nThe second row is executed when a key is found (first key too), and update the currentInventory with the new quantity from the item. The outcome of this is a Ktable<storeName, Inventory> \nThe content is materialized in a store.\n\nThe update operation on the inventory is a simple hashmap update: \n\n```Java\n// in Inventory Class\npublic Inventory updateStockQuantity(String k, Item newValue) {\n        this.storeName = k;\n        if (newValue.type.equals(\"SALE\")) \n            newValue.quantity=-newValue.quantity;\n        return this.updateStock(newValue.sku,newValue.quantity);\n    }\n\n    public Inventory updateStock(String sku, long newV) {\n        if (stock.get(sku) == null) {\n            stock.put(sku, Long.valueOf(newV));\n        } else {\n            Long currentValue = stock.get(sku);\n            stock.put(sku, Long.valueOf(newV) + currentValue );\n        }\n        return this;\n    }\n```\n\nFinally the KTable is streamed out to the inventory topic:\n\n```Java\ninventory.toStream()\n            .to(inventoryStockTopicName,\n                Produced.with(Serdes.String(),inventorySerde));\n      \n```\n\nThe KTable is also materialized as a store that can be accessed via an API like `/inventory/store/{storeid}/{itemid}` using interactive query.\n\nAs items topic can be partitioned, a REST call may not reach the good end points, as the local store may not have the expected queried key. So the code is using interactive query to get access to the local state stores or return a URL of a remote store where the records for the given key are.\n\n## Integration tests\n\n For running the integration test, we propose to copy the e2e folder from the solution repository and follow the [readme instructions section end-to-end-testing ](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing).\n\n## Deploy to OpenShift\n\nBe sure to have done [the steps described here](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to get user credentials and Server side certificate. \nWe need to define a config map manifest (src/main/kubernetes/configmap.yml) for the user and brokers internal URL. The userID used is one of the scram-sha-512 ones.\n\n ```yaml\n apiVersion: v1\n kind: ConfigMap\n metadata:\n    name: item-aggregator-cm\n data:\n    KAFKA_USER: app-demo\n    SECURE_PROTOCOL: SASL_SSL\n    KAFKA_BROKERS: minimal-prod-kafka-bootstrap.eventstreams.svc:9093\n ```\n\nConfigure it with `oc apply -f src/main/kubernetes/configmap.yml`.","frontmatter":{"title":"Kafka Streams Test Lab 3","description":"Using Kafka Streams to compute real time inventory stock"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-streams/lab-3/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}