(window.webpackJsonp=window.webpackJsonp||[]).push([[50,85],{"013z":function(e,t,a){"use strict";var n=a("q1tI"),o=a.n(n),c=a("NmYn"),i=a.n(c),r=a("Wbzz"),s=a("Xrax"),b=a("k4MR"),l=a("TSYQ"),p=a.n(l),m=a("QH2O"),d=a("qKvR"),u=function(e){var t,a=e.title,n=e.tabs,o=void 0===n?[]:n;return Object(d.b)("div",{className:p()(m.pageHeader,(t={},t[m.withTabs]=o.length,t))},Object(d.b)("div",{className:"bx--grid"},Object(d.b)("div",{className:"bx--row"},Object(d.b)("div",{className:"bx--col-lg-12"},Object(d.b)("h1",{id:"page-title",className:m.text},a)))))},h=a("BAC9"),O=function(e){var t=e.relativePagePath,a=e.repository,n=Object(r.useStaticQuery)("1364590287").site.siteMetadata.repository,o=a||n,c=o.baseUrl,i=o.subDirectory,s=c+"/edit/"+o.branch+i+"/src/pages"+t;return c?Object(d.b)("div",{className:"bx--row "+h.row},Object(d.b)("div",{className:"bx--col"},Object(d.b)("a",{className:h.link,href:s},"Edit this page on GitHub"))):null},j=a("FCXl"),g=a("dI71"),f=a("I8xM"),k=function(e){function t(){return e.apply(this,arguments)||this}return Object(g.a)(t,e),t.prototype.render=function(){var e=this.props,t=e.tabs,a=e.slug,n=a.split("/").filter(Boolean).slice(-1)[0],o=t.map((function(e){var t,o=i()(e,{lower:!0,strict:!0}),c=o===n,s=new RegExp(n+"/?(#.*)?$"),b=a.replace(s,o);return Object(d.b)("li",{key:e,className:p()((t={},t[f.selectedItem]=c,t),f.listItem)},Object(d.b)(r.Link,{className:f.link,to:""+b},e))}));return Object(d.b)("div",{className:f.tabsContainer},Object(d.b)("div",{className:"bx--grid"},Object(d.b)("div",{className:"bx--row"},Object(d.b)("div",{className:"bx--col-lg-12 bx--col-no-gutter"},Object(d.b)("nav",null,Object(d.b)("ul",{className:f.list},o))))))},t}(o.a.Component),v=a("MjG9");t.a=function(e){var t=e.pageContext,a=e.children,n=e.location,o=e.Title,c=t.frontmatter,l=void 0===c?{}:c,p=t.relativePagePath,m=t.titleType,h=l.tabs,g=l.title,f=l.theme,N=l.description,y=l.keywords,A=Object(r.useStaticQuery)("2456312558").site.pathPrefix,w=A?n.pathname.replace(A,""):n.pathname,x=h?w.split("/").filter(Boolean).slice(-1)[0]||i()(h[0],{lower:!0}):"";return Object(d.b)(b.a,{tabs:h,homepage:!1,theme:f,pageTitle:g,pageDescription:N,pageKeywords:y,titleType:m},Object(d.b)(u,{title:o?Object(d.b)(o,null):g,label:"label",tabs:h}),h&&Object(d.b)(k,{slug:w,tabs:h,currentTab:x}),Object(d.b)(v.a,{padded:!0},a,Object(d.b)(O,{relativePagePath:p})),Object(d.b)(j.a,{pageContext:t,location:n,slug:w,tabs:h,currentTab:x}),Object(d.b)(s.a,null))}},BAC9:function(e,t,a){e.exports={bxTextTruncateEnd:"EditLink-module--bx--text-truncate--end--2pqje",bxTextTruncateFront:"EditLink-module--bx--text-truncate--front--3_lIE",link:"EditLink-module--link--1qzW3",row:"EditLink-module--row--1B9Gk"}},I8xM:function(e,t,a){e.exports={bxTextTruncateEnd:"PageTabs-module--bx--text-truncate--end--267NA",bxTextTruncateFront:"PageTabs-module--bx--text-truncate--front--3xEQF",tabsContainer:"PageTabs-module--tabs-container--8N4k0",list:"PageTabs-module--list--3eFQc",listItem:"PageTabs-module--list-item--nUmtD",link:"PageTabs-module--link--1mDJ1",selectedItem:"PageTabs-module--selected-item--YPVr3"}},QH2O:function(e,t,a){e.exports={bxTextTruncateEnd:"PageHeader-module--bx--text-truncate--end--mZWeX",bxTextTruncateFront:"PageHeader-module--bx--text-truncate--front--3zvrI",pageHeader:"PageHeader-module--page-header--3hIan",withTabs:"PageHeader-module--with-tabs--3nKxA",text:"PageHeader-module--text--o9LFq"}},ycnG:function(e,t,a){"use strict";a.r(t),a.d(t,"_frontmatter",(function(){return r})),a.d(t,"default",(function(){return u}));var n=a("wx14"),o=a("zLVn"),c=(a("q1tI"),a("7ljp")),i=a("013z"),r=(a("qKvR"),{}),s=function(e){return function(t){return console.warn("Component '"+e+"' was not imported, exported, or provided by MDXProvider as global scope"),Object(c.b)("div",t)}},b=s("InlineNotification"),l=s("AnchorLinks"),p=s("AnchorLink"),m={_frontmatter:r},d=i.a;function u(e){var t=e.components,a=Object(o.a)(e,["components"]);return Object(c.b)(d,Object(n.a)({},m,a,{components:t,mdxType:"MDXLayout"}),Object(c.b)(b,{kind:"warning",mdxType:"InlineNotification"},Object(c.b)("strong",null,"Update 10/23/2020")," - Work in progress"),Object(c.b)(l,{mdxType:"AnchorLinks"},Object(c.b)(p,{mdxType:"AnchorLink"},"Overview"),Object(c.b)(p,{mdxType:"AnchorLink"},"Solution anatomy"),Object(c.b)(p,{mdxType:"AnchorLink"},"General pre-requisites"),Object(c.b)(p,{mdxType:"AnchorLink"},"Lab 1: RabbitMQ Source Connector to Kafka items topic")),Object(c.b)("h2",null,"Overview"),Object(c.b)("p",null,"This lab addresses multiple scenarios that aim to build an end to end data pipeline, as depicted by the following figure, using Event Streams / Kafka on Premise and Kafka Connect cluster. At the high level Kafka Connect is used to integrate external systems into the Kafka ecosystem. For example external systems can inject item sale messages to queue manager, from which a first Kafka source connector publishes the messages to a Kafka topic, which then will be processed by a series of event driven microservices down to a final topic, which will be used by Sink connectors."),Object(c.b)("p",null," ",Object(c.b)("span",Object(n.a)({parentName:"p"},{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1152px"}}),"\n      ",Object(c.b)("span",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-background-image",style:{paddingBottom:"41.66666666666667%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABjUlEQVQoz1WR207CQBCG+4w+hvHCB/DGxBhfwMOtifFwafDCaLTeYDQBAUEubKIYKrRaqJRtadnt/s4sB+0kk25383/zz4yVJBpxDGSZRq41NCWQU9J/ni9TKYViaIiYtKRPU2A61ebWEgIYDOlREEDSA4HDkcRoLJcFGMhRc0Lsll5x//JdQDNUGFMEZGeSQKNI46UTwekKbB23sbb3hP4wMU4kV6K4rvpY3aug9OCCO7uqevj8ThFFgOdr06VlHFA3Is6xvl/HednFJFXoDybQ+p8NMlluBdg+a+Gu4aPnT7Fx2ELdGZlnNsWNWHqu4q83yDCOlXE1oRZC8YN2cIv61wV6oQtFIkXVlcqp6MzRbBo0x0xC0pytBexv1rmxw0D3K8BD7wR29wDdwRsyuuPlMJAXwUCOx3aAlU0bO6eNmcMFUEpFVXLYzz6ObjpovkXmnsUXjz4uKx7BFtvX5swxjDLYjT7aH2ER+NEfIhynuG9+4qbWw7sXmXZ+RIJS2cFttTN3qJYaUhc2/guBql9DC/31CgAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}})),"\n  ",Object(c.b)("img",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-image",alt:"1",title:"1",src:"/refarch-eda/static/80e56b9ba5956ed5e6259f03cca0d611/3cbba/kconnect-overview.png",srcSet:["/refarch-eda/static/80e56b9ba5956ed5e6259f03cca0d611/7fc1e/kconnect-overview.png 288w","/refarch-eda/static/80e56b9ba5956ed5e6259f03cca0d611/a5df1/kconnect-overview.png 576w","/refarch-eda/static/80e56b9ba5956ed5e6259f03cca0d611/3cbba/kconnect-overview.png 1152w","/refarch-eda/static/80e56b9ba5956ed5e6259f03cca0d611/f5f59/kconnect-overview.png 1522w"],sizes:"(max-width: 1152px) 100vw, 1152px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"})),"\n    ")),Object(c.b)("p",null,"To support this lab we are reusing a classical business use case around ",Object(c.b)("inlineCode",{parentName:"p"},"real time inventory processing")," where stores are sending their sale transactions to a central messaging platform, based on queues, and with the adoption of loosely coupled microservices, real time analytics and complex event processing, Kafka is added to this legacy environment. Adopting Kafka Connect helps to integrate with existing applications without any changes."),Object(c.b)("p",null,"All the components of this scenario are ready to run on OpenShift, but we are also providing different docker compose files to run all of those components on your local workstation. The important body of knowledge of this scenario is related to the programming model we used, and the Kafka Connect configuration and code."),Object(c.b)("h2",null,"What you will learn"),Object(c.b)("ul",null,Object(c.b)("li",{parentName:"ul"},"Use Quarkus, with reactive programming API like Mutiny, with Kafka API to produce message to Kafka"),Object(c.b)("li",{parentName:"ul"},"Same Quarkus app can generate messages to RabbitMQ using the AMQP API"),Object(c.b)("li",{parentName:"ul"},"Same Quarkus app can generate mesasges to IBM MQ using JMS"),Object(c.b)("li",{parentName:"ul"},"Use Kafka Connect to get source and sink cluster to get date from the queue and to queue or DB"),Object(c.b)("li",{parentName:"ul"},"Use Quarkus and Kafka streams to compute aggregates to build an inventory view from the stream of events"),Object(c.b)("li",{parentName:"ul"},"Use the RabbitMQ source connector from IBM Event messaging open source contribution"),Object(c.b)("li",{parentName:"ul"},"Use the IBM MQ sink connector from IBM Event messaging open source contribution"),Object(c.b)("li",{parentName:"ul"},"Use the JDBC sink connector from IBM Event messaging open source contribution")),Object(c.b)("h2",null,"Solution anatomy"),Object(c.b)("p",null,"This end to end scenario is divided into smaller labs that can be combined to support the real time inventory data pipeline as illustrated in the figure below:"),Object(c.b)("p",null," ",Object(c.b)("span",Object(n.a)({parentName:"p"},{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1152px"}}),"\n      ",Object(c.b)("span",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-background-image",style:{paddingBottom:"44.09722222222222%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABzklEQVQoz02S22sTQRjF81/7IooPIj4pfSwitiBY+mAVGowtlFKTEOs1LTGSJmluJFuby2az99n5+WUmaTrLwly+c75zzkwuCjW+D0miAW0+7n47tNZkWSYzxWZoAsEGgSaKIIwEKZDcdKZwHPA8TRJb4rmXSnEmey5RGBrqJA45LlZ5d3pN4dtAGmwaxrFgE4RYkzv57ZCmmtlM03NCuqOAN4U2D3drbO2VCHzXgLI0Yf9TlcdvL3hdaEmzjK9/J1wNFszn0Oqk4lQIo0Sh0qVkeLbXYOdzm5tpRK3jMvYS7o+jUpOXB3X2zzq4c8WjnRoHxaGxXv5zi5JEclpnpnghbc5KNXoj39qQBkGY0JpWuHBOyX+vy1r2xVsqSD9QVC+bdLpDdJZieLQiZ8MWhUGAO/lnDq1HGIvSk84r8tfPqd+ck/irA0k1FFWeF0vjFOc25sH2L57sXi4VanOLm8vL+Nmc8LHc50djYraWkZRrY/KVIe4iNmqUyljfy2Da5+nhFi+Ot63CNWFvNGbmJxQvBrwvdam2p9a+5PzhyxWHlS5+pIxCdQ933hgxnPdlNlkRrsQtiy1A3b01JXmZWLQlWquzzmxNqpbv1D7d/zqUp/BjHjIrAAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}})),"\n  ",Object(c.b)("img",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-image",alt:"2",title:"2",src:"/refarch-eda/static/64851f0b9449995c6d7bfd4dfffe17b6/3cbba/kconnect-scenario-components.png",srcSet:["/refarch-eda/static/64851f0b9449995c6d7bfd4dfffe17b6/7fc1e/kconnect-scenario-components.png 288w","/refarch-eda/static/64851f0b9449995c6d7bfd4dfffe17b6/a5df1/kconnect-scenario-components.png 576w","/refarch-eda/static/64851f0b9449995c6d7bfd4dfffe17b6/3cbba/kconnect-scenario-components.png 1152w","/refarch-eda/static/64851f0b9449995c6d7bfd4dfffe17b6/36102/kconnect-scenario-components.png 1518w"],sizes:"(max-width: 1152px) 100vw, 1152px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"})),"\n    ")),Object(c.b)("ol",null,Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"The store simulator application is a Quarkus based app, which generate item sales to different possible messaging middlewares ( RabbitMQ, MQ or event direct to Kafka). The code of this application is in this ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator"})," refarch-eda-store-simulator repository"),". If you want to browse the code, the main read me of this project includes how to package and run this app, and the code approach used. The docker image is ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://hub.docker.com/r/ibmcase/eda-store-simulator"}),"ibmcase/eda-store-simulator/")),Object(c.b)("ul",{parentName:"li"},Object(c.b)("li",{parentName:"ul"},"RabbitMQ runs in docker image started locally via docker compose. The messages are in the ",Object(c.b)("inlineCode",{parentName:"li"},"items")," queue."),Object(c.b)("li",{parentName:"ul"},"The first lab consists of setting up the Rabbit MQ source Kafka connector to get items from the queue to the ",Object(c.b)("inlineCode",{parentName:"li"},"items")," topic in Kafka.  focusing on the injection to Kafka, is documented in the ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"../../use-cases/connect-rabbitmq/"}),"use case: Kafka connect - Rabbit MQ"),"."),Object(c.b)("li",{parentName:"ul"},"[A]"," The Sink connector description is in the ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-tools"}),"real time inventory lab")," folder. The Kafka connector source is in ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://github.com/ibm-messaging/kafka-connect-rabbitmq-source"}),"this project.")))),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"The inventory MS is a Kafka Stream application, done with Kafka Stream API. The source code is in ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory"}),"this project"),". Consider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose API on top of Kafka Streams interactive queries and publish inventory events on ",Object(c.b)("inlineCode",{parentName:"p"},"inventory")," topic. The code is also used in the ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"../../use-cases/kafka-streams/lab-3/"}),"Kafka Streams lab 3"))),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"The mock up Inventory mainframe application is not implemented and we will use the MQ tools to view the message in the ",Object(c.b)("inlineCode",{parentName:"p"},"inventory")," MQ queue."),Object(c.b)("ul",{parentName:"li"},Object(c.b)("li",{parentName:"ul"},"The MQ Sink connector ","[B]"," configuration is defined in the ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-tools"}),"real time inventory lab")," folder, ",Object(c.b)("inlineCode",{parentName:"li"},"mq-sink.json")," file."),Object(c.b)("li",{parentName:"ul"},"MQ broker runs in docker container started with docker-compose"),Object(c.b)("li",{parentName:"ul"},"The lab scenario is ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"../../use-cases/connect-mq/"}),"describe in the Connect MQ use case ")))),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"The Inventory Application, using DB2 as datasource is a quarkus app using hibernate with panache, defined in the ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app"}),"nventory-app")," folder"),Object(c.b)("ul",{parentName:"li"},Object(c.b)("li",{parentName:"ul"},"The JDBC Sink connector ","[C]"," configuration is defined in the the ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-tools"}),"real time inventory lab")," folder, ",Object(c.b)("inlineCode",{parentName:"li"},"jdbc-sink.json")," file ."),Object(c.b)("li",{parentName:"ul"},"The ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"../../use-cases/connect-jdbc/"}),"Connect JDBC use case")," lab goes over how the Kafka Connect JDBC sink works."))),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"The ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"#scenario-4:-run-the-solution-components-end-to-end-on-kubernetes"}),"last scenario")," addresses the end to end solution, which is basically an end to end demonstration of a simple data pipeline for a real time view of an inventory solution."))),Object(c.b)("h2",null,"Demonstration Script"),Object(c.b)("p",null,"For an end to end demonstration the following steps can be demonstrated once the environment is up and running."),Object(c.b)("ol",null,Object(c.b)("li",{parentName:"ol"},"Start with the Store Simulator -> Stores view to present the stores Acme Inc is managing. Those stores ID will be used as key for sending events to Kafka.")),Object(c.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1152px"}},"\n      ",Object(c.b)("span",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-background-image",style:{paddingBottom:"39.93055555555555%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABBklEQVQoz3VQy07DMBD0R0HFBQ699MZP8C0FiYd66JnPghSknriAhJo4cZP4OXidrrFKWWk03tnd2U3EYr3F/HGD+dOEy/sKF3cVZrcVzpevOFu+JD6Fq4cNrtfvuHneYrF6S7OiVhrNfoJMbPBVK3x87vAth5TvYg+hLphBc21vJo+YC5wIbw2Ct/EVDswFN3Fwv++jECHEoRJRHMcR1rmEfd/D+5CgdVwUqK4xap36SS/n84UsUAzDAGNMbPZQSuUaaRQ6mhF4rmRRCvxOF1qbDLuug4uXUo00CjJm8z+GZZJ/VTQi8DAvo8vJlDR9+OR/L6SrpJRo2zZz0zQ5Z40X8NJjwx+kMFuGIA3abwAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}})),"\n  ",Object(c.b)("img",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-image",alt:"0",title:"0",src:"/refarch-eda/static/e89a897c43dcbaceb5e7ee1bb1077ec5/3cbba/stores-view.png",srcSet:["/refarch-eda/static/e89a897c43dcbaceb5e7ee1bb1077ec5/7fc1e/stores-view.png 288w","/refarch-eda/static/e89a897c43dcbaceb5e7ee1bb1077ec5/a5df1/stores-view.png 576w","/refarch-eda/static/e89a897c43dcbaceb5e7ee1bb1077ec5/3cbba/stores-view.png 1152w","/refarch-eda/static/e89a897c43dcbaceb5e7ee1bb1077ec5/ecbf4/stores-view.png 1272w"],sizes:"(max-width: 1152px) 100vw, 1152px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"})),"\n    "),Object(c.b)("ol",null,Object(c.b)("li",{parentName:"ol"},"In the Simulator view start to send 10 messages to ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:""}),"Rabbit MQ"))),Object(c.b)("h2",null,"General pre-requisites"),Object(c.b)(b,{kind:"info",mdxType:"InlineNotification"},"You need to decide what your 'on-premise' environment is for this scenario. You can run with docker compose locally on your laptopn, or deploy the component to an OpenShift cluster. For quick demonstration purpose, each lab / use case have docker compose files to test them in isolation, or a bigger end to end processing with Rabbit MQ, Kafka Connect, Store Simulator, Item aggregator and DB2. If you do not want to build all the components, we have each of them available in Docker Hub."),Object(c.b)("p",null,"We need the following tools to run the different labs. We try to use docker images as much as possible to do not impact your local laptop."),Object(c.b)("ul",null,Object(c.b)("li",{parentName:"ul"},Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://docs.docker.com/"}),"Docker")," and docker compose to run the solution locally."),Object(c.b)("li",{parentName:"ul"},"You need to get ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://docs.docker.com/compose/"}),"docker compose"),"."),Object(c.b)("li",{parentName:"ul"},Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://git-scm.com/downloads"}),"git CLI"),"."),Object(c.b)("li",{parentName:"ul"},Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://maven.apache.org/install.html"}),"Maven"),".")),Object(c.b)("p",null,"Clone the labs repository: ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-tools"}),"refarch-eda-tools")),Object(c.b)("h3",null,"OpenShift specific setup"),Object(c.b)("ol",null,Object(c.b)("li",{parentName:"ol"},"Connect to your OpenShift cluster using ‘oc’ cli"),Object(c.b)("li",{parentName:"ol"},"Create a new project ",Object(c.b)("inlineCode",{parentName:"li"},"oc new-project eda-inventory")),Object(c.b)("li",{parentName:"ol"},"If not done yet, deploy Event Streams using the ",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"/refarch-eda/use-cases/overview/pre-requisites#install-event-streams-using-operators"}),"instructions here"))),Object(c.b)("h2",null,"Lab 1: RabbitMQ Source Connector to Kafka items topic"),Object(c.b)("p",null,"Following the ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"/refarch-eda/use-cases/connect-rabbitmq/"}),"RabbitMQ Source Connector lab")," use case to deploy the connector runtime, connecting your source RabbitMQ environment to the target Kafka environment and ",Object(c.b)("inlineCode",{parentName:"p"},"items")," topic."),Object(c.b)("h2",null,"Lab 2: Deploy the MQ Sink Connector on OpenShift"),Object(c.b)("p",null,"Optionally, follow the ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"/refarch-eda/use-cases/connect-mq/"}),"MQ Sink Connector")," use case to deploy the connector runtime to OpenShift, connecting your source Kafka environment and topic to the target MQ queue manager."),Object(c.b)("h2",null,"Step 4: Deploy the JDBC Sink Connector locally"),Object(c.b)("p",null,"Follow the ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"/refarch-eda/use-cases/connect-jdbc/"}),"JDBC Sink Connector")," use case to deploy the connector runtime, connecting your source Kafka environment and topic to the target DB2 database."),Object(c.b)("h2",null,"Step 5: Run the solution components end to end on Kubernetes"),Object(c.b)("p",null,"This solution covers all the components of the data pipeline. It still uses DB2 and Event Streams on Cloud but deploy all the other component in OpenShift as part of IBM Kubernetes Service."),Object(c.b)("h3",null,"Pre-requisites"),Object(c.b)("p",null,"Create the following services in IBM Cloud:"),Object(c.b)("ul",null,Object(c.b)("li",{parentName:"ul"},Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started"}),"DB2 instance"),"."),Object(c.b)("li",{parentName:"ul"},Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://cloud.ibm.com/docs/containers?topic=containers-cs_cluster_tutorial#cs_cluster_tutorial"}),"IBM Kubernetes Service"),".")),Object(c.b)("h3",null,"Deployment"),Object(c.b)("p",null,"To be finished !"),Object(c.b)("ol",null,Object(c.b)("li",{parentName:"ol"},"Verify the Store Sale Simulator runs"),Object(c.b)("li",{parentName:"ol"},"Verify the connectors ",Object(c.b)("inlineCode",{parentName:"li"},"http://localhost:8083/connectors"))),Object(c.b)("h2",null,"TODOs"),Object(c.b)("ul",null,Object(c.b)("li",{parentName:"ul"},"Update deployment instructions to prioritize OCP, utilzing Tekton and/or Build/DeploymentConfigs as necessary",Object(c.b)("ul",Object(n.a)({parentName:"li"},{className:"contains-task-list"}),Object(c.b)("li",Object(n.a)({parentName:"ul"},{className:"task-list-item"}),Object(c.b)("input",Object(n.a)({parentName:"li"},{type:"checkbox",checked:!1,disabled:!0}))," ","Provide PRIMARY deployment instructions to OCP, including necessary backend services"),Object(c.b)("li",Object(n.a)({parentName:"ul"},{className:"task-list-item"}),Object(c.b)("input",Object(n.a)({parentName:"li"},{type:"checkbox",checked:!1,disabled:!0}))," ","Provide SECONDARY deployment instructions via Docker Compose, including necessary backend services"))),Object(c.b)("li",{parentName:"ul"},"Provide support for IBM Event Streams on Cloud and IBM Event Streams V10 via CP4I.",Object(c.b)("ul",Object(n.a)({parentName:"li"},{className:"contains-task-list"}),Object(c.b)("li",Object(n.a)({parentName:"ul"},{className:"task-list-item"}),Object(c.b)("input",Object(n.a)({parentName:"li"},{type:"checkbox",checked:!1,disabled:!0}))," ","IBM Event Streams on Cloud"),Object(c.b)("li",Object(n.a)({parentName:"ul"},{className:"task-list-item"}),Object(c.b)("input",Object(n.a)({parentName:"li"},{type:"checkbox",checked:!1,disabled:!0}))," ","IBM Event Streams V10")))),Object(c.b)("p",null,"Older content to classify"),Object(c.b)("ol",null,Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"Login to the cloud via CLI: ",Object(c.b)("inlineCode",{parentName:"p"},"ibmcloud login"))),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"Initialize the Event Streams CLI and select the target Event Streams cluster: ",Object(c.b)("inlineCode",{parentName:"p"},"ibmcloud es init"))),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"Define connect topics: When running in distributed mode, the connectors need three topics as presented in the ",Object(c.b)("inlineCode",{parentName:"p"},"create topics")," table ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://ibm.github.io/event-streams/connecting/setting-up-connectors/"}),"here"),"."),Object(c.b)("ul",{parentName:"li"},Object(c.b)("li",{parentName:"ul"},Object(c.b)("strong",{parentName:"li"},"connect-configs"),": This topic will store the connector and task configurations."),Object(c.b)("li",{parentName:"ul"},Object(c.b)("strong",{parentName:"li"},"connect-offsets"),": This topic is used to store offsets for Kafka Connect."),Object(c.b)("li",{parentName:"ul"},Object(c.b)("strong",{parentName:"li"},"connect-status"),": This topic will store status updates of connectors and tasks.")),Object(c.b)("p",{parentName:"li"},"Using IBM Event Streams CLI, the topics are created via the commands like:"),Object(c.b)("pre",{parentName:"li"},Object(c.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"# log to the kubernetes cluster:\nibmcloud login -a https://icp-console.apps.green.ocp.csplab.local\n# initialize the event streams CLI plugin\nibmcloud es init\n# Create the Kafka topics for Kafka connect\nibmcloud es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compact\nibmcloud es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compact\nibmcloud es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact\n# Create the topic for the scenarios\nibmcloud es topic-create inventory\nibmcloud es topic-create items\nibmcloud es topics\n"))),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"Create API KEY with a manager-level access.")),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"Clone the lab repository: ",Object(c.b)("inlineCode",{parentName:"p"},"git clone https://github.com/jbcodeforce/eda-kconnect-lab && cd eda-kconnect-lab"),".")),Object(c.b)("li",{parentName:"ol"},Object(c.b)("p",{parentName:"li"},"Prepare the script to set the environment variables used by all the components of the solution, like the Kafka broker URLs and APIKEy."),Object(c.b)("ul",{parentName:"li"},Object(c.b)("li",{parentName:"ul"},"First rename the ",Object(c.b)("inlineCode",{parentName:"li"},"scripts/setenv-TMP.sh")," to ",Object(c.b)("inlineCode",{parentName:"li"},"scripts/setenv.sh")),Object(c.b)("li",{parentName:"ul"},"Then modify the KAFKA_BROKERS and KAFKA_APIKEY with the respecting values as defined in the Event Streams credentials.")),Object(c.b)("pre",{parentName:"li"},Object(c.b)("code",Object(n.a)({parentName:"pre"},{className:"language-json"}),'{\n  "api_key": "bA ... Qp",\n  "apikey": "bA ... Qp",\n  "iam_apikey_description": "Auto-generated for key 4d ... c6",\n  "iam_apikey_name": "es-mgr-creds",\n  "iam_role_crn": "crn:v1:bluemix:public:iam::::serviceRole:Manager",\n  "iam_serviceid_crn": "crn:v1:bluemix:public:iam-identity::a/db ... f2::serviceid:ServiceId-7a ... 6d",\n  "instance_id": "29 ... 15",\n  "kafka_admin_url": "https://70 ... 1g.svc01.us-east.eventstreams.cloud.ibm.com",\n  "kafka_brokers_sasl": [\n    "broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093",\n    "broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093",\n    "broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093",\n    "broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093",\n    "broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093",\n    "broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093"\n  ],\n  "kafka_http_url": "https://70 ... 1g.svc01.us-east.eventstreams.cloud.ibm.com",\n  "password": "bA ... Qp",\n  "user": "token"\n}\n')))))}u.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-scenarios-realtime-inventory-index-mdx-d144e93d16d76cb4b4f3.js.map