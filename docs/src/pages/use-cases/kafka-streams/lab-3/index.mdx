---
title: Kafka Streams Test Lab 3
description: Using Kafka Streams to compute real time inventory stock
---

<InlineNotification kind="warning">
<strong>Work in progress</strong> Updated 04/13/2020 - End to end testing could be better
</InlineNotification>

<AnchorLinks>
    <AnchorLink>Overview</AnchorLink>
    <AnchorLink>Pre-requisites</AnchorLink>
    <AnchorLink>Deploy to OpenShift</AnchorLink>
    <AnchorLink>Interactive queries</AnchorLink>
    <AnchorLink>Integration Tests</AnchorLink>
</AnchorLinks>


## Overview

In this lab, we're going to use [Quarkus](https://quarkus.io) to develop the real time inventory logic using Kafka Streams APIs and microprofile reactive messaging.

The requirements to address are:

- consume item sold events from the `items` topic. Item has SKU as unique key. Item event has store ID reference
- compute for each item its current stock cross stores
- compute the store's stock for each item
- generate inventory event for store - item - stock
- expose APIs to get stock for a store or for an item

Here is a simple diagram to illustrate the components used:

 ![1](./images/item-aggregator-ctx.png)

The goal of this lab, is to develop the green component which exposes an API to support Kafka Streams interactive query on top of the aggregate to compute inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic).

We will be unit testing the stream logic using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TopologyTestDriver class. 

This application is deployed to OpenShift cluster with Event Streams or Strimzi running in the same cluster. We use the Quarkus Kubernetes plugin to build the yaml manifests for deployment. 

This application needs the [Item Store sell simulator](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) to perform the end to end testing and to demonstrate the end to end scenario.

## Pre-requisites

**OpenShift Container Platform**
- v4.6.x

**IBM Cloud Pak for Integration**
- CP4I2021.4

**Kafka**
- The lab uses Event Streams v10.x on Cloud Pack for Integration or Kafka Strimzi.

**Code Source**: clone the following git repository: `git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory`.


## Use application as-is

If you do not want to develop the application, you can deploy the existing final app to your OpenShift clustr using our [docker image](https://hub.docker.com/r/ibmcase/item-aggregator) and the following steps:

1. Get the Kafka streams credentials and Bootstrap URL

 As the application is deployed in the same cluster as the Kafka cluster we will use internal route.



### Connect to Event Streams

We need to complete the configuration to connect to the remote Event Streams running on OpenShift.

* Create the items and inventory topics, following the instructions as described [in this note](../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:

 ```shell
 cloudctl es topic-create --name items --partitions 3 --replication-factor 3
 cloudctl es topic-create --name inventory --partitions 1 --replication-factor 3
 cloudctl es topics
 ```

* To connect from your computer to Event Streams running on OpenShift, we need to define a user with `scram-sha-512` password, as this is the mechanism for external to the cluster connection. [See product documentation](https://ibm.github.io/event-streams/getting-started/connecting/) on how to do it, or use our [quick summary here](/use-cases/overview/pre-requisites#get-shram-user).

* Get Server TLS certificate into the `certs` folder. See our [quick summary here](/use-cases/overview/pre-requisites#get-tls-server-public-certificate)

 ```shell
 oc get secret minimal-prod-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f - 

 ```

* Modify the `application.properties` file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka streams can create topics to backup state stores, and one for kafka streams consumer and producer tasks:

```properties
kafka.bootstrap.servers=${KAFKA_BROKERS}
kafka.security.protocol=${SECURE_PROTOCOL}
kafka.ssl.protocol=TLSv1.2
%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\=\"${KAFKA_USER}\" password\=\"${KAFKA_PASSWORD}\";
%dev.kafka.sasl.mechanism=SCRAM-SHA-512
kafka.ssl.truststore.location=${KAFKA_CERT_PATH}
kafka.ssl.truststore.password=${KAFKA_CERT_PWD}
kafka.ssl.truststore.type=PKCS12
%prod.kafka.ssl.keystore.location=${USER_CERT_PATH}
%prod.kafka.ssl.keystore.password=${USER_CERT_PWD}
%prod.kafka.ssl.keystore.type=PKCS12
```

The above settings take into account that when running locally (`%dev` profile) we use the `scram-sha` mechanism to authenticate, and when we deploy on openshift, the `%prod` profile is used with TLS mutual authentication  (client certificate in keystore).

The same approach applies for Kafka Stream:

```
quarkus.kafka-streams.bootstrap-servers=${KAFKA_BROKERS}
quarkus.kafka-streams.security.protocol=${SECURE_PROTOCOL}
quarkus.kafka-streams.ssl.protocol=TLSv1.2
%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512
%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\=\"${KAFKA_USER}\" password\=\"${KAFKA_PASSWORD}\";
quarkus.kafka-streams.ssl.truststore.location=${KAFKA_CERT_PATH}
quarkus.kafka-streams.ssl.truststore.password=${KAFKA_CERT_PWD}
quarkus.kafka-streams.ssl.truststore.type=PKCS12
# Only if TLS is used for authentication instead of scram
%prod.quarkus.kafka-streams.ssl.keystore.location=${USER_CERT_PATH}
%prod.quarkus.kafka-streams.ssl.keystore.password=${USER_CERT_PWD}
%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12
```

* Define a file, like `.env`, to set environment variables, and modify the settings from your Event Streams configuration.

```
KAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443
KAFKA_USER=
KAFKA_PASSWORD=
KAFKA_CERT_PATH=${PWD}/certs/es-cert.p12
KAFKA_CERT_PWD=
SECURE_PROTOCOL=SASL_SSL
```

* Restart the quarkus in dev mode

 ```shell
 source .env
 ./mvnw quarkus:dev
 ```

 normally you should not get any exception and should get a trace like

 ```
    AdminClientConfig values: 
    bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]
    client.dns.lookup = default
    client.id = 
    connections.max.idle.ms = 300000
    default.api.timeout.ms = 60000
    metadata.max.age.ms = 300000
    metric.reporters = []
    metrics.num.samples = 2
    metrics.recording.level = INFO
    metrics.sample.window.ms = 30000

    ....
    INFO  [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kafka-streams, kubernetes, kubernetes-client, mutiny, resteasy, resteasy-jsonb, resteasy-mutiny, smallrye-context-propagation, smallrye-health, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, swagger-ui, vertx]
 ```



## Integration tests

 For running the integration test, we propose to copy the e2e folder from the solution repository and follow the [readme instructions section end-to-end-testing ](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing).

## Deploy to OpenShift

Be sure to have done [the steps described here](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to get user credentials and server side certificate. 

The deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.  
Here are the interesting properties to set environment variables from secrets 

```properties
%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred
quarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL
quarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL
quarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093
quarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12
quarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred
quarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password
quarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert
quarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password
quarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12
quarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred
quarkus.openshift.env-vars.USER_CERT_PWD.value=user.password
```

And an extract of the expected generated openshift manifests from those configurations:

```yaml
    spec:
      containers:
      - env:
        - name: KAFKA_CERT_PWD
          valueFrom:
            secretKeyRef:
              key: ca.password
              name: sandbox-rp-cluster-ca-cert
        - name: USER_CERT_PATH
          value: /deployments/certs/user/user.p12
        - name: USER_CERT_PWD
          valueFrom:
            secretKeyRef:
              key: user.password
              name: sandbox-rp-tls-cred
        - name: KAFKA_BROKERS
          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093
        - name: KAFKA_CERT_PATH
          value: /deployments/certs/server/ca.p12
        - name: KAFKA_PASSWORD
          valueFrom:
            secretKeyRef:
              key: user.password
              name: sandbox-rp-tls-cred
        - name: SECURE_PROTOCOL
          value: SASL_SSL
```

Finally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:

```
quarkus.openshift.mounts.es-cert.path=/deployments/certs/server
quarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert
quarkus.openshift.mounts.user-cert.path=/deployments/certs/user
quarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred
```

which generates:

```
        volumeMounts:
        - mountPath: /deployments/certs/server
          name: es-cert
          readOnly: false
          subPath: ""
        - mountPath: /deployments/certs/user
          name: user-cert
          readOnly: false
          subPath: ""
```

Now any deployment using the following command should work:

```shell
./mvnw clean package -DQuarkus.kubernetes.deploy=true
```

The last piece is to go to EventStreams console and look at the inventory topic for messages generated. As an alternate we could use [Kafdrop](../../overview/pre-requisites#using-kafdrop).

## Another item producer

We have done a simple app to produce item sale or restock events. The app is not exposed with API defined in Swagger or with JAXRS annotations, but expose one method to be exposed as a service. See the [ ItemSimulatorFunction code](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/05fcdcb7d09d674d9eb2cda2e28601171ba51166/item-kafka-producer/src/main/java/ibm/gse/eda/api/ItemSimulatorFunction.java#L18-L25). 

The simulator is using reactive messaging, but as we mix imperative with reactive programming, the code is using Emitter, and then Munity Multi to create and send the Kafka records:

```java
@Inject
@Channel("items")
Emitter<Item> emitter;


public void sendItems(Integer numberOfRecords) {
    Multi.createFrom().items(buildItems(numberOfRecords).stream()).subscribe().with(item -> {
            logger.warning("send " + item.toString());
            Message<Item> record = KafkaRecord.of(item.storeName,item);
            emitter.send(record );
        }, failure -> System.out.println("Failed with " + failure.getMessage()));
   
```

### Running the application in dev mode

You can run your application in dev mode that enables live coding using:

```
./mvnw quarkus:dev
```

But as we connect to a remote Kafka Cluster you need to define environment variables as:

```shell
KAFKA_BROKERS=....containers.appdomain.cloud:443
KAFKA_USER=<a>-scram-user
KAFKA_PASSWORD=<a-password>
KAFKA_CERT_PATH=${PWD}/certs/es-cert.p12
KAFKA_CERT_PWD=<server-ca-certificate-password>
SECURE_PROTOCOL=SASL_SSL
```

### Packaging and running the application

The application can be packaged using `./mvnw package`.
It produces the `item-kafka-producer-1.0.0-SNAPSHOT-runner.jar` file in the `/target` directory.
Be aware that it’s not an _über-jar_ as the dependencies are copied into the `target/lib` directory.

The application is now runnable using `java -jar target/item-kafka-producer-1.0.0-SNAPSHOT-runner.jar`.

### Deployment to OpenShift

The code includes declaration to build the necessary environment variables, secrets, volumes to get connected to a Kafka cluster using TLS authentication, and deploy in one command:

```shell
mvn package -DskipTests -Dquarkus.kubernetes.deploy=true
```

See the application.properties for `quarkus.openshift.env-vars.*` settings.

It is also supposed to be deployed as knative app, but there is an issue on the generation of volume declarations in the knative.yaml file, so we could not make it in one command.

The code is also available as docker image: [ibmcase/item-kafka-producer](https://hub.docker.com/r/ibmcase/item-kafka-producer).