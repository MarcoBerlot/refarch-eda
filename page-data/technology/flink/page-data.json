{"componentChunkName":"component---src-pages-technology-flink-index-mdx","path":"/technology/flink/","result":{"pageContext":{"frontmatter":{"title":"Apache Flink Technology Summary","description":"Apache Flink Technology Summary"},"relativePagePath":"/technology/flink/index.mdx","titleType":"append","MdxNode":{"id":"01ed8d59-b501-56f1-8565-2fdc0a20cee1","children":[],"parent":"1ab296f9-e862-5731-bd75-f2fc0e891c4f","internal":{"content":"---\ntitle: Apache Flink Technology Summary\ndescription: Apache Flink Technology Summary\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 9/30/2021- Work in progress</strong>\n</InlineNotification>\n\n## Why Flink?\n\nIn classical IT architecture, we can see two types of data processing: transactional and analytics. \nWith 'monolytics' application, the database system serves multiple applications which sometimes access the same database \ninstances and tables. This approach cause problems to support evolution and scaling. \nMicroservice architecture addresses part of those problems by isolating data storage per service. \n\nTo get insight from the data, the traditional approach is to develop data warehouse and ETL jobs to copy and transform data \nfrom the transactional systems to the warehouse. ETL process extracts data from a transactional database, transforms data \ninto a common representation that might include validation, value normalization, encoding, deduplication, and schema \ntransformation, and finally loads the new record into the target analytical database. They are batches and run periodically.\n\nFrom the data warehouse, the analysts build queries, metrics, and dashboards / reports to address a specific business question. \nMassive storage is needed, which uses different protocol such as: NFS, S3, HDFS...\n\nToday, there is a new way to think about data by seeing they are created as continuous streams of events, which can be processed\n in real time, and serve as the foundation for stateful stream processing application: the analytics move to the real data stream.\n\nWe can define three classes of applications implemented with stateful stream processing:\n\n1. **Event-driven applications**: to adopt the reactive manifesto for scaling, resilience, responsive application, leveraging messaging as communication system.\n1. **Data pipeline applications**: replace ETL with low latency stream processing.\n1. **Data analytics applications**: immediatly act on the data and query live updated reports. \n\nFor more real industry use cases content see the [Flink Forward web site.](https://www.flink-forward.org/)\n\n## The What \n\n[Apache Flink](https://flink.apache.org) (2016) is a framework and **distributed processing** engine for stateful computations over unbounded and bounded data streams. Flink supports batch (data set )and graph (data stream) processing. It is very good at:\n\n* Very low latency processing event time semantics to get consistent and accurate results even in case of out of order events\n* Exactly once state consistency \n* Millisecond latencies while processing millions of events per second\n* Expressive and easy-to-use APIs: map, reduce, join, window, split, and connect.\n* fault tolerance, and high availability: supports worker and master failover, eliminating any single point of failure\n* A lot of connectors to integrate with Kafka, Cassandra, Elastic Search, JDBC, S3...\n* Support container and deployment on Kubernetes\n* Support updating the application code and migrate jobs to different Flink clusters without losing the state of the application\n* Also support batch processing\n\nThe figure below illustrates those different models combined with [Zepellin](https://zeppelin.apache.org/) as a multi purpose notebook to develop data analytic projects on top of Spark, Python or Flink.\n\n\n ![Flink components](./images/arch.png)\n\n\n## Stream processing concepts\n\nIn [Flink](https://ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/#stream-processing), applications are composed of streaming dataflows that may be transformed by user-defined operators. These dataflows form directed graphs that start with one or more sources, and end in one or more sinks. The data flows between operations. \nThe figure below, from product documentation, summarizes the simple APIs used to develop a data stream processing flow:\n\n ![1](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/program_dataflow.svg)\n \n *src: apache Flink product doc*\n\nStream processing includes a set of functions to transform data, to produce a new output stream. Intermediate steps compute rolling aggregations like min, max, mean, or collect and buffer records in time window to compute metrics on finite set of events. \nTo properly define window operator semantics, we need to determine both how events are assigned to buckets and how often the window produces a result. Flink's streaming model is based on windowing and checkpointing, it uses controlled cyclic dependency graph\n as its execution engine.\n\nThe following figure is showing integration of stream processing runtime with an append log system, like Kafka, with internal local state persistence and continuous checkpoint to remote storage as HA support:\n\n![](./images/flink-rt-processing.png)\n\nAs part of the checkpointing process, Flink saves the 'offset read commit' information of the append log, so in case of a failure, Flink recovers a stateful streaming application by restoring its state from a previous checkpoint and resetting the read position on the append log.\n\nThe evolution of microservice is to become more event-driven, which are stateful streaming applications that ingest event streams and process the events with application-specific business logic. This logic can be done in flow defined in Flink and executed in the clustered runtime.\n\n![](./images/evt-app.png)\n\nA lot of predefined connectors exist to connect to specific source and sink. Transform operators can be chained. Dataflow can consume from Kafka, Kinesis, Queue, and any data sources. A typical high level view of Flink app is presented in figure below:\n\n ![2](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/flink-application-sources-sinks.png)\n\n *src: apache Flink product doc*\n\n\nPrograms in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks.\n\n ![3](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/parallel_dataflow.svg)\n\n *src: apache Flink site*\n\nA Flink application, can be stateful, run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines.\nState is always accessed local, which helps Flink applications achieve high throughput and low-latency. You can choose to keep state on the JVM heap, or if it is too large, saves it in efficiently organized on-disk data structures.\n\n ![4](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/local-state.png)\n\nThis is the Job Manager component which parallelizes the job and distributes slices of [the Data Stream](https://ci.apache.org/projects/flink/flink-docs-stable/dev/datastream_api.html) flow, you defined, to the Task Managers for execution. Each parallel slice of your job will be executed in a **task slot**.\n\n ![5](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/distributed-runtime.svg)\n\nOnce Flink is started (for example with the docker image), Flink Dashboard [http://localhost:8081/#/overview](http://localhost:8081/#/overview) presents the execution reporting of those components:\n\n ![6](./images/flink-dashboard.png)\n\nThe execution is from one of the training examples, the number of task slot was set to 4, and one job is running.\n\nSpark is not a true real time processing while Flink is. Flink and Spark support batch processing too. \n\n\n## Statefulness\n\nWhen using aggregates or windows operators, states need to be kept. For fault tolerant Flink uses checkpoints and savepoints. \nCheckpoints represent a snapshot of where the input data stream is with each operator's state. A streaming dataflow can be resumed from a checkpoint while maintaining consistency (exactly-once processing semantics) by restoring the state of the operators and by replaying the records from the point of the checkpoint.\n\nIn case of failure of a parallel execution, Flink stops the stream flow, then restarts operators from the last checkpoints. When doing the reallocation of data partition for processing, states are reallocated too. \nStates are saved on distributed file systems. When coupled with Kafka as data source, the committed read offset will be part of the checkpoint data.\n\nFlink uses the concept of `Checkpoint Barriers`, which represents a separation of records, so records received since the last snapshot are part of the future snapshot. Barrier can be seen as a mark, a tag in the data stream that close a snapshot. \n\n ![Checkpoints](./images/checkpoints.png)\n\nIn Kafka, it will be the last committed read offset. The barrier flows with the stream so can be distributed. Once a sink operator (the end of a streaming DAG) has received the `barrier n` from all of its input streams, it acknowledges that `snapshot n` to the checkpoint coordinator. \nAfter all sinks have acknowledged a snapshot, it is considered completed. Once `snapshot n` has been completed, the job will never ask the source for records before such snapshot.\n\nState snapshots are save in a state backend (in memory, HDFS, RockDB). \n\nKeyedStream is a key-value store. Key match the key in the stream, state update does not need transaction.\n\nFor DataSet (Batch processing) there is no checkpoint, so in case of failure the stream is replayed.\n\n\n### Windowing\n\n[Windows](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/windows.html) are buckets within a Stream and can be defined with times, or count of elements.\n\n* **Tumbling** window assign events into nonoverlapping buckets of fixed size. When the window border is passed, all the events are sent to an evaluation function for processing. Count-based tumbling windows define how many events are collected before triggering evaluation. Time based timbling window define time interval of n seconds. Amount of the data vary in a window. `.keyBy(...).window(TumblingProcessingTimeWindows.of(Time.seconds(2)))`\n\n![](./images/tumbling.png)\n\n* **Sliding** window: same but windows can overlap. An event might belong to multiple buckets. So there is a `window sliding time` parameter: `.keyBy(...).window(SlidingProcessingTimeWindows.of(Time.seconds(2), Time.seconds(1)))`\n\n![](./images/sliding.png)\n\n* **Session** window: Starts when the data stream processes records and stop when there is inactivity, so the timer set this threshold: `.keyBy(...).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))`. The operator creates one window for each data element received.\n\n![](./images/session.png)\n\n* **Global** window: one window per key and never close. The processing is done with Trigger:\n\n    ```java\n    .keyBy(0)\n\t.window(GlobalWindows.create())\n\t.trigger(CountTrigger.of(5))\n    ```\n\nKeyStream can help to run in parallel, each window will have the same key.\n\nTime is central to the stream processing, and the time is a parameter of the flow / environment and can take different meanings:\n\n* `ProcessingTime` = system time of the machine executing the task: best performance and low latency\n* `EventTime` = the time at the source level, embedded in the record. Deliver consistent and deterministic results regardless of order \n* `IngestionTime` = time when getting into Flink. \n\nSee example [TumblingWindowOnSale.java](https://github.com/jbcodeforce/flink-studies/blob/master/my-flink/src/main/java/jbcodeforce/windows/TumblingWindowOnSale.java) and to test it, do the following:\n\n```shell\n# Start the SaleDataServer that starts a server on socket 9181 and will read the avg.txt file and send each line to the socket\njava -cp target/my-flink-1.0.0-SNAPSHOT.jar jbcodeforce.sale.SaleDataServer\n# inside the job manager container start with \n`flink run -d -c jbcodeforce.windows.TumblingWindowOnSale /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar`.\n# The job creates the data/profitPerMonthWindowed.txt file with accumulated sale and number of record in a 2 seconds tumbling time window\n(June,Bat,Category5,154,6)\n(August,PC,Category5,74,2)\n(July,Television,Category1,50,1)\n(June,Tablet,Category2,142,5)\n(July,Steamer,Category5,123,6)\n...\n```\n\n### Trigger\n\n[Trigger](https://ci.apache.org/projects/flink/flink-docs-release-1.13/dev/stream/operators/windows.html#triggers) determines when a window is ready to be processed. All windows have default trigger. For example tumbling window has a 2s trigger. Global window has explicit trigger. We can implement our own triggers by implementing the Trigger interface with different methods to implement: onElement(..), onEventTime(...), onProcessingTime(...)\n\nDefault triggers:\n\n* EventTimeTrigger: fires based upon progress of event time\n* ProcessingTimeTrigger: fires based upon progress of processing time\n* CountTrigger: fires when # of element in a window > parameter\n* PurgingTrigger\n\n### Eviction\n\nEvictor is used to remove elements from a window after the trigger fires and before or after the window function is applied. The logic to remove is app specific.\n\nThe predefined evictors: CountEvictor, DeltaEvictor and TimeEvictor.\n\n### Watermark\n\n[Watermark](https://ci.apache.org/projects/flink/flink-docs-release-1.13/dev/event_timestamps_watermarks.html) is the mechanism to keep how the event time has progressed: with windowing operator, event time stamp is used, but windows are defined on elapse time, for example, 10 minutes, so watermark helps to track te point of time where no more delayed events will arrive. \nThe Flink API expects a WatermarkStrategy that contains both a TimestampAssigner and WatermarkGenerator. A TimestampAssigner is a simple function that extracts a field from an event. A number of common strategies are available out of the box as static methods on WatermarkStrategy, so reference to the documentation and examples.\n\nWatermark is crucial for out of order events, and when dealing with multi sources. Kafka topic partitions can be a challenge without watermark. With IoT device and network latency, it is possible to get an event with an earlier timestamp, while the operator has already processed such event timestamp from other sources.\n\nIt is possible to configure to accept late events, with the `allowed lateness` time by which element can be late before being dropped. Flink keeps a state of Window until the allowed lateness time expires.\n\n\n## Resources\n\n* [Product documentation](https://flink.apache.org/flink-architecture.html). \n* [Official training](https://ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/)\n* Base docker image is: [https://hub.docker.com/_/flink](https://hub.docker.com/_/flink)\n* [Flink docker setup](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html) and the docker-compose files in this repo.\n* [FAQ](https://wints.github.io/flink-web/faq.html)\n* [Cloudera flink stateful tutorial](https://github.com/cloudera/flink-tutorials/tree/master/flink-stateful-tutorial): very good example for inventory transaction and queries on item considered as stream\n* [Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana](https://www.elastic.co/blog/building-real-time-dashboard-applications-with-apache-flink-elasticsearch-and-kibana)\n* Udemy Apache Flink a real time hands-on. (But a 2 stars enablement for me)","type":"Mdx","contentDigest":"e46b2a472ad9e6802fbcdbb67845702e","owner":"gatsby-plugin-mdx","counter":821},"frontmatter":{"title":"Apache Flink Technology Summary","description":"Apache Flink Technology Summary"},"exports":{},"rawBody":"---\ntitle: Apache Flink Technology Summary\ndescription: Apache Flink Technology Summary\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 9/30/2021- Work in progress</strong>\n</InlineNotification>\n\n## Why Flink?\n\nIn classical IT architecture, we can see two types of data processing: transactional and analytics. \nWith 'monolytics' application, the database system serves multiple applications which sometimes access the same database \ninstances and tables. This approach cause problems to support evolution and scaling. \nMicroservice architecture addresses part of those problems by isolating data storage per service. \n\nTo get insight from the data, the traditional approach is to develop data warehouse and ETL jobs to copy and transform data \nfrom the transactional systems to the warehouse. ETL process extracts data from a transactional database, transforms data \ninto a common representation that might include validation, value normalization, encoding, deduplication, and schema \ntransformation, and finally loads the new record into the target analytical database. They are batches and run periodically.\n\nFrom the data warehouse, the analysts build queries, metrics, and dashboards / reports to address a specific business question. \nMassive storage is needed, which uses different protocol such as: NFS, S3, HDFS...\n\nToday, there is a new way to think about data by seeing they are created as continuous streams of events, which can be processed\n in real time, and serve as the foundation for stateful stream processing application: the analytics move to the real data stream.\n\nWe can define three classes of applications implemented with stateful stream processing:\n\n1. **Event-driven applications**: to adopt the reactive manifesto for scaling, resilience, responsive application, leveraging messaging as communication system.\n1. **Data pipeline applications**: replace ETL with low latency stream processing.\n1. **Data analytics applications**: immediatly act on the data and query live updated reports. \n\nFor more real industry use cases content see the [Flink Forward web site.](https://www.flink-forward.org/)\n\n## The What \n\n[Apache Flink](https://flink.apache.org) (2016) is a framework and **distributed processing** engine for stateful computations over unbounded and bounded data streams. Flink supports batch (data set )and graph (data stream) processing. It is very good at:\n\n* Very low latency processing event time semantics to get consistent and accurate results even in case of out of order events\n* Exactly once state consistency \n* Millisecond latencies while processing millions of events per second\n* Expressive and easy-to-use APIs: map, reduce, join, window, split, and connect.\n* fault tolerance, and high availability: supports worker and master failover, eliminating any single point of failure\n* A lot of connectors to integrate with Kafka, Cassandra, Elastic Search, JDBC, S3...\n* Support container and deployment on Kubernetes\n* Support updating the application code and migrate jobs to different Flink clusters without losing the state of the application\n* Also support batch processing\n\nThe figure below illustrates those different models combined with [Zepellin](https://zeppelin.apache.org/) as a multi purpose notebook to develop data analytic projects on top of Spark, Python or Flink.\n\n\n ![Flink components](./images/arch.png)\n\n\n## Stream processing concepts\n\nIn [Flink](https://ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/#stream-processing), applications are composed of streaming dataflows that may be transformed by user-defined operators. These dataflows form directed graphs that start with one or more sources, and end in one or more sinks. The data flows between operations. \nThe figure below, from product documentation, summarizes the simple APIs used to develop a data stream processing flow:\n\n ![1](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/program_dataflow.svg)\n \n *src: apache Flink product doc*\n\nStream processing includes a set of functions to transform data, to produce a new output stream. Intermediate steps compute rolling aggregations like min, max, mean, or collect and buffer records in time window to compute metrics on finite set of events. \nTo properly define window operator semantics, we need to determine both how events are assigned to buckets and how often the window produces a result. Flink's streaming model is based on windowing and checkpointing, it uses controlled cyclic dependency graph\n as its execution engine.\n\nThe following figure is showing integration of stream processing runtime with an append log system, like Kafka, with internal local state persistence and continuous checkpoint to remote storage as HA support:\n\n![](./images/flink-rt-processing.png)\n\nAs part of the checkpointing process, Flink saves the 'offset read commit' information of the append log, so in case of a failure, Flink recovers a stateful streaming application by restoring its state from a previous checkpoint and resetting the read position on the append log.\n\nThe evolution of microservice is to become more event-driven, which are stateful streaming applications that ingest event streams and process the events with application-specific business logic. This logic can be done in flow defined in Flink and executed in the clustered runtime.\n\n![](./images/evt-app.png)\n\nA lot of predefined connectors exist to connect to specific source and sink. Transform operators can be chained. Dataflow can consume from Kafka, Kinesis, Queue, and any data sources. A typical high level view of Flink app is presented in figure below:\n\n ![2](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/flink-application-sources-sinks.png)\n\n *src: apache Flink product doc*\n\n\nPrograms in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks.\n\n ![3](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/parallel_dataflow.svg)\n\n *src: apache Flink site*\n\nA Flink application, can be stateful, run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines.\nState is always accessed local, which helps Flink applications achieve high throughput and low-latency. You can choose to keep state on the JVM heap, or if it is too large, saves it in efficiently organized on-disk data structures.\n\n ![4](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/local-state.png)\n\nThis is the Job Manager component which parallelizes the job and distributes slices of [the Data Stream](https://ci.apache.org/projects/flink/flink-docs-stable/dev/datastream_api.html) flow, you defined, to the Task Managers for execution. Each parallel slice of your job will be executed in a **task slot**.\n\n ![5](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/distributed-runtime.svg)\n\nOnce Flink is started (for example with the docker image), Flink Dashboard [http://localhost:8081/#/overview](http://localhost:8081/#/overview) presents the execution reporting of those components:\n\n ![6](./images/flink-dashboard.png)\n\nThe execution is from one of the training examples, the number of task slot was set to 4, and one job is running.\n\nSpark is not a true real time processing while Flink is. Flink and Spark support batch processing too. \n\n\n## Statefulness\n\nWhen using aggregates or windows operators, states need to be kept. For fault tolerant Flink uses checkpoints and savepoints. \nCheckpoints represent a snapshot of where the input data stream is with each operator's state. A streaming dataflow can be resumed from a checkpoint while maintaining consistency (exactly-once processing semantics) by restoring the state of the operators and by replaying the records from the point of the checkpoint.\n\nIn case of failure of a parallel execution, Flink stops the stream flow, then restarts operators from the last checkpoints. When doing the reallocation of data partition for processing, states are reallocated too. \nStates are saved on distributed file systems. When coupled with Kafka as data source, the committed read offset will be part of the checkpoint data.\n\nFlink uses the concept of `Checkpoint Barriers`, which represents a separation of records, so records received since the last snapshot are part of the future snapshot. Barrier can be seen as a mark, a tag in the data stream that close a snapshot. \n\n ![Checkpoints](./images/checkpoints.png)\n\nIn Kafka, it will be the last committed read offset. The barrier flows with the stream so can be distributed. Once a sink operator (the end of a streaming DAG) has received the `barrier n` from all of its input streams, it acknowledges that `snapshot n` to the checkpoint coordinator. \nAfter all sinks have acknowledged a snapshot, it is considered completed. Once `snapshot n` has been completed, the job will never ask the source for records before such snapshot.\n\nState snapshots are save in a state backend (in memory, HDFS, RockDB). \n\nKeyedStream is a key-value store. Key match the key in the stream, state update does not need transaction.\n\nFor DataSet (Batch processing) there is no checkpoint, so in case of failure the stream is replayed.\n\n\n### Windowing\n\n[Windows](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/windows.html) are buckets within a Stream and can be defined with times, or count of elements.\n\n* **Tumbling** window assign events into nonoverlapping buckets of fixed size. When the window border is passed, all the events are sent to an evaluation function for processing. Count-based tumbling windows define how many events are collected before triggering evaluation. Time based timbling window define time interval of n seconds. Amount of the data vary in a window. `.keyBy(...).window(TumblingProcessingTimeWindows.of(Time.seconds(2)))`\n\n![](./images/tumbling.png)\n\n* **Sliding** window: same but windows can overlap. An event might belong to multiple buckets. So there is a `window sliding time` parameter: `.keyBy(...).window(SlidingProcessingTimeWindows.of(Time.seconds(2), Time.seconds(1)))`\n\n![](./images/sliding.png)\n\n* **Session** window: Starts when the data stream processes records and stop when there is inactivity, so the timer set this threshold: `.keyBy(...).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))`. The operator creates one window for each data element received.\n\n![](./images/session.png)\n\n* **Global** window: one window per key and never close. The processing is done with Trigger:\n\n    ```java\n    .keyBy(0)\n\t.window(GlobalWindows.create())\n\t.trigger(CountTrigger.of(5))\n    ```\n\nKeyStream can help to run in parallel, each window will have the same key.\n\nTime is central to the stream processing, and the time is a parameter of the flow / environment and can take different meanings:\n\n* `ProcessingTime` = system time of the machine executing the task: best performance and low latency\n* `EventTime` = the time at the source level, embedded in the record. Deliver consistent and deterministic results regardless of order \n* `IngestionTime` = time when getting into Flink. \n\nSee example [TumblingWindowOnSale.java](https://github.com/jbcodeforce/flink-studies/blob/master/my-flink/src/main/java/jbcodeforce/windows/TumblingWindowOnSale.java) and to test it, do the following:\n\n```shell\n# Start the SaleDataServer that starts a server on socket 9181 and will read the avg.txt file and send each line to the socket\njava -cp target/my-flink-1.0.0-SNAPSHOT.jar jbcodeforce.sale.SaleDataServer\n# inside the job manager container start with \n`flink run -d -c jbcodeforce.windows.TumblingWindowOnSale /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar`.\n# The job creates the data/profitPerMonthWindowed.txt file with accumulated sale and number of record in a 2 seconds tumbling time window\n(June,Bat,Category5,154,6)\n(August,PC,Category5,74,2)\n(July,Television,Category1,50,1)\n(June,Tablet,Category2,142,5)\n(July,Steamer,Category5,123,6)\n...\n```\n\n### Trigger\n\n[Trigger](https://ci.apache.org/projects/flink/flink-docs-release-1.13/dev/stream/operators/windows.html#triggers) determines when a window is ready to be processed. All windows have default trigger. For example tumbling window has a 2s trigger. Global window has explicit trigger. We can implement our own triggers by implementing the Trigger interface with different methods to implement: onElement(..), onEventTime(...), onProcessingTime(...)\n\nDefault triggers:\n\n* EventTimeTrigger: fires based upon progress of event time\n* ProcessingTimeTrigger: fires based upon progress of processing time\n* CountTrigger: fires when # of element in a window > parameter\n* PurgingTrigger\n\n### Eviction\n\nEvictor is used to remove elements from a window after the trigger fires and before or after the window function is applied. The logic to remove is app specific.\n\nThe predefined evictors: CountEvictor, DeltaEvictor and TimeEvictor.\n\n### Watermark\n\n[Watermark](https://ci.apache.org/projects/flink/flink-docs-release-1.13/dev/event_timestamps_watermarks.html) is the mechanism to keep how the event time has progressed: with windowing operator, event time stamp is used, but windows are defined on elapse time, for example, 10 minutes, so watermark helps to track te point of time where no more delayed events will arrive. \nThe Flink API expects a WatermarkStrategy that contains both a TimestampAssigner and WatermarkGenerator. A TimestampAssigner is a simple function that extracts a field from an event. A number of common strategies are available out of the box as static methods on WatermarkStrategy, so reference to the documentation and examples.\n\nWatermark is crucial for out of order events, and when dealing with multi sources. Kafka topic partitions can be a challenge without watermark. With IoT device and network latency, it is possible to get an event with an earlier timestamp, while the operator has already processed such event timestamp from other sources.\n\nIt is possible to configure to accept late events, with the `allowed lateness` time by which element can be late before being dropped. Flink keeps a state of Window until the allowed lateness time expires.\n\n\n## Resources\n\n* [Product documentation](https://flink.apache.org/flink-architecture.html). \n* [Official training](https://ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/)\n* Base docker image is: [https://hub.docker.com/_/flink](https://hub.docker.com/_/flink)\n* [Flink docker setup](https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html) and the docker-compose files in this repo.\n* [FAQ](https://wints.github.io/flink-web/faq.html)\n* [Cloudera flink stateful tutorial](https://github.com/cloudera/flink-tutorials/tree/master/flink-stateful-tutorial): very good example for inventory transaction and queries on item considered as stream\n* [Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana](https://www.elastic.co/blog/building-real-time-dashboard-applications-with-apache-flink-elasticsearch-and-kibana)\n* Udemy Apache Flink a real time hands-on. (But a 2 stars enablement for me)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/flink/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}