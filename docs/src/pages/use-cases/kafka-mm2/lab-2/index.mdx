---
title: Mirror Maker 2 ES on RHOS to local cluster
description: Using Mirror Maker 2 from Event Streams on OpenShift to local cluster
---

<AnchorLinks>
  <AnchorLink>Overview</AnchorLink>
  <AnchorLink>Pre-requisites</AnchorLink>
  <AnchorLink>Start Mirror Maker 2</AnchorLink>
  <AnchorLink>Start Consumer from target cluster</AnchorLink>
  <AnchorLink>Start Producer to source cluster</AnchorLink>
  <AnchorLink>Clean up</AnchorLink>
</AnchorLinks>

## Overview

For this scenario the source cluster is an Event Streams on OpenShift, and the target cluster is another Kafka cluster (using strimzi) running locally. Mirror Maker 2 runs on local server in same data center as target cluster. (Your laptop is promoted as data center). This lab is similar to the lab 1, but it instead uses the Event Streams within Cloud Pak for Integration as illustrated in the figure below:

 ![1](../images/mm2-lab2.png)

 1. Mirror Maker 2 runs on local server.
 2. A producer in python to send records to `products` topic, will run locally or could be deployed on OpenShift as a **job**
 3. Local Kafka cluster with replicated topic and Kafka console consumer to see the replicated messages.

## Pre-requisites

You need to have one instance of Event Streams installed and configured using Cloud Pak for Integration. See the [product documentation for installation](https://ibm.github.io/event-streams/installing/installing/)

We need to define the `products` topic on the source cluster (Event Streams on CP4I) and get the authentication credentials and TLS certificates.

* Login to the OpenShift cluster using the console and get the API token

 ```shell
 oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443
 ```

* If not done from lab 1, clone the github to get access to the Mirror Maker 2 configuration we are using:

 ```shell
 git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools
 ```

* Verify the Event Streams on OpenShift service external end point URL. This URL will be used to configure Mirror Maker 2. 

 ```shell
 # Be sure to be in the project where eventstreams runs. The following command may be different.
 oc project eventstreams
 # Use the bootstrap internal URL: light-es is the name of the cluster
 oc get route | grep bootstrap

 # The URL is something like
 light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud 
 ```

* Rename the `.env-tmpl` file to `.env`. Then edit this `.env` file, with the bootstrap URL and the port 443 at the end: `light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443`.
* Get a user with write access to topic, group and transactions. You can use the Event Streams Console User Interface to define this user,  with the `Create SCRAM credential` on the internal URL panel. See [this section](/use-cases/overview/pre-requisites#getting-scram-authentication-from-event-streams-on-openshift) for detail.

 ```shell
  oc get kafkausers

 # NAME                                    CLUSTER       AUTHENTICATION   AUTHORIZATION
 # light-es-ibm-es-ac-reg                  light-es      scram-sha-512    
 # light-es-ibm-es-georep-source-user      light-es      scram-sha-512    simple
 # light-es-ibm-es-kafka-user              light-es      tls   
 # starter                                 light-es      scram-sha-512           
 ```

Modify the `.env` file to set the `ES_OCP_USER` variable with your selected user.

## Start the local Kafka cluster

In the `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` folder there is a [docker compose file](https://raw.githubusercontent.com/refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local/master/docker-compose.yml) to start a local three brokers cluster with one Zookeeper node.

1. In one Terminal window, start the local cluster using the command:

```shell
docker-compose up -d
```

The data are persisted on the local disk within the folder named `kafka-data`.

Your local environment is up and running.

## Start Mirror Maker 2

* If not done already, create a `products` topic (with one partition) in the EventStreams cluster using the management console. See [this note](/use-cases/overview/pre-requisites#creating-event-streams-topics) to see how to do this operation.
* As done in lab_1 the launch script will configure Mirror Maker 2 in standalone mode with the properties specific to the SCRAM authentication with TLS certificates.

    ```properties
    es-cp4i.ssl.protocol=TLSv1.2
    es-cp4i.ssl.truststore.password=<>
    es-cp4i.ssl.truststore.location=/home/es-cert.p12
    es-cp4i.ssl.truststore.type=PKCS12
    es-cp4i.security.protocol=SASL_SSL
    es-cp4i.ssl.endpoint.identification.algorithm=https
    es-cp4i.sasl.mechanism=SCRAM-SHA-512
    es-cp4i.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=starter password=<>;

    ```
 
* Start Mirror Maker2 using the launch script:

    ```shell
    # In the  es-ic-to-local folder
    ./launchMM2.sh
    ```


## Start consumer from target cluster

Use Apache Kafka tool like Console consumer to trace the message received on a topic

```shell
docker exec -ti kafka2 bash -c "/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-cp4i.products --from-beginning" 
```


## Start Producer to source cluster

As seen in lab 1, we will use the same python script to create products records. This time the script is producing products data to Event Streams on OpenShift. So we need the URL, pem, and user to access this cluster.


* If not done before download the pem file using the following CLI commands:

 ```shell
 cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation
 cloudctl es init
 # select the event stream cluster
 cloudctl es certificates --format pem
 ```

 save this `.pem` file under `mirror-maker2/es-cpri-to-local` folder.

 Now send five records:
 
 ```shell
 
 ./sendProductRecords.sh
 ```

 * Going to the Event Streams Console we can see the message in the `products` topic.

![](../images/es-products-topic.png)


## Clean up

You are done with the lab, to stop everything:

```shell
./cleanLab.sh
```
