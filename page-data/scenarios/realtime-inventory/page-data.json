{"componentChunkName":"component---src-pages-scenarios-realtime-inventory-index-mdx","path":"/scenarios/realtime-inventory/","result":{"pageContext":{"frontmatter":{"title":"Real-time Inventory, powered by Kafka","description":"An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect."},"relativePagePath":"/scenarios/realtime-inventory/index.mdx","titleType":"append","MdxNode":{"id":"645ba643-c2fb-5b45-9f6a-93fcb3400b19","children":[],"parent":"1397c2b6-79ae-57a1-adc3-d7c425fab022","internal":{"content":"---\ntitle: Real-time Inventory, powered by Kafka\ndescription: An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect.\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 06/09/2021</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>What you will learn</AnchorLink>\n  <AnchorLink>Solution anatomy</AnchorLink>\n  <AnchorLink>General pre-requisites</AnchorLink>\n  <AnchorLink>Lab 1: Kafka only solution to demonstrate real-time inventory</AnchorLink>\n  <AnchorLink>Lab 2: Deploy the MQ Source Connector</AnchorLink>\n  <AnchorLink>Lab 3: Deploy the MQ Sink Connector [B]</AnchorLink>\n  <AnchorLink>Lab 4: Deploy the JDBC Sink Connector</AnchorLink>\n </AnchorLinks>\n\n## Overview\n\nThis scenario implements a simple real time inventory management solution based on real life MVPs we developed in 2020. \nStores are sending their sale transactions to a central messaging platform, based on queues or topics, and with the adoption\nof loosely coupled microservices, real time analytics Kafka is part of the architecture. A\ndopting Kafka Connect helps to integrate with existing applications without any changes to their base code.\n\n![](./images/hl-view.png)\n\nThis scenario addresses multiple use cases that aim to build an end to end data pipeline solution from source to different potential sinks:\n\n* The store simulator injects directly sell events to Kafka to the `items` topic\n* The store simulator can also generate message to IBM MQ using JMS API or to RabbitMQ using AMQP protocol. So we can demonstrate Kafka connectors with RabbitMQ source or IBM MQ source.\n* When messages are sourced to Queues, then a Kafka Source Connector is used to propagate message to `items` topics.\n* The Inventory computation is done using Kafka Stream component which produces inventory items and keep, in-memory a stock per items. \nThis inventory is exposed by API and used the interactive query capability of Kafka Streams.\n* Different sink connectors can be plugged into this `inventory` topic to move data to DB2, Elastic Search, Cloud Object Storage...\n\nWe try to make the business scenario, easily demonstrable by running locally with docker or on an OpenShift Cluster.\n\nIn real life, an as-is solution will include backend applications to manage the warehouses inventory, connected to a fulfillment home build application,\ncombined with store application and servers, e-commerce suite, and a set of SOA services acting as modern backend system.\n\n ![](./images/as-is-soa.png)\n\nWe may have integration flows to do data mapping, most of those calls are synchronous and to get one item availability, a lot of SOAP calls are done, \nincreasing latency, and risk of failure.\n\n### Component view\n\nAt the high level Kafka Connect is used to integrate external systems into Kafka. For example external systems can inject item sale messages to queue, from which a first Kafka source connector \npublishes the messages to a Kafka topic, which then will be processed by a series of event-driven microservices down to a final topic, which will be used by Sink connectors\n to send records to other external systems.\n\n ![1](./images/kconnect-overview.png)\n\n\nAll the components of this scenario are ready to run on OpenShift, but we are also providing different docker compose files to run all of those components on your local computer. \nThe important body of knowledge of this scenario is related to the programming model we used, and the Kafka Connect configuration and code.\n\n## What you will learn\n\n* Use Quarkus, with reactive programming API like Mutiny, with Kafka API to produce message to Kafka\n* Same Quarkus app can generate messages to RabbitMQ using the AMQP API\n* Same Quarkus app can generate mesasges to IBM MQ using JMS\n* Use Kafka Connect to get source and sink cluster to get date from the RabbitMQ queue and to IBM MQ queue or DB2\n* Use Quarkus and Kafka streams to compute aggregates to build an inventory view from the stream of events\n* Use the RabbitMQ source connector from IBM Event messaging open source contribution\n* Use the IBM MQ sink connector from IBM Event messaging open source contribution\n* Use the JDBC sink connector from IBM Event messaging open source contribution\n\n## Solution anatomy\n\n1. The [store simulator application](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) is a Quarkus app, which generates item sales to different possible messaging middlewares ( RabbitMQ, MQ or directly to Kafka). \nThe code of this application is in this https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator. If you want to browse the code, the main readme of this project includes how to package and run this app with docker compose, \nand explains how the code works. \nThe docker image is [quay.io/ibmcase/eda-store-simulator/](https://quay.io/ibmcase/eda-store-simulator)\n1. The item inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-item-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory). \nConsider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on `item.inventory` topic. \n1. The store inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-store-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory) the output is in `store.inventory` topic. \n2. The mock up Inventory mainframe application is not implemented and we will use the MQ tools to view the message in the `item.inventory` MQ queue.\n\n\n## General pre-requisites\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* [git CLI](https://git-scm.com/downloads).\n* Get access to an OpenShift Cluster\n* [OpenShift CLI](https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html) on your local environment.\n* [jq](https://stedolan.github.io/jq/) on your local environment.\n* Clone the Inventory lab repository: \n\n```sh\noc clone  https://github.com/ibm-cloud-architecture/eda-lab-inventory.git\n```\n\n\n## Lab 1: Kafka only solution to demonstrate real-time inventory with Kafka Streams\n\nIn this lab we will deploy the solution using Strimzi on OpenShift. The diagram looks like the following:\n\n![](/images/inventory-components.png)\n\nThe Kafka version is Strimzi 2.7.\n\nThe deployment to OpenShift using Kafka Strimzi operator, and gitops approach is described in dedicated lab you can read the details in the [Kafka Stream use case section](/use-cases/kafka-streams/lab-3/).\n\nHere are the simplest steps to deploy the solution in minimum configuration:\n\n1. Login to the OpenShift cluster (4.6 version): `oc login --token=sha256~d... --server=https://....containers.cloud.ibm.com...`\n1. Work from the clone repository: `cd eda-lab-inventory`\n1. Define the environment variables for your deployment in the: `scripts/env-strimzi.sh`\n1. Start the deploy with one script `./scripts/deployInventorySolutionWithStrimzi.sh --skip-login`\n\n\n## Lab 2: Deploy the MQ Source Connector\n\nThis lab uses the store simulator to generate events to MQ queue. The component view may look like the following figure:\n\n![](./images/lab2-mq-kafka.png)\n\nThe major difference with previous lab, is the addition of the IBM MQ Source Kafka connector to move records from `Items` queue to `items` Kafka topic.\n\nThis lab is defined in the [MQ to Kafka repository](https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka) with instructions on how to run it locally or on OpenShift.\n\n## Lab 3: Deploy the MQ Sink Connector\n\nFollowing the [MQ Sink Connector](/use-cases/connect-mq/#lab-2:-mq-sink-from-kafka) use case, we can deploy the MNQ Sink connector to OpenShift, connecting your source Kafka environment and topic to the target MQ queue manager.\n\n ! This lab needs a refresh\n\n## Lab 4: Deploy the JDBC Sink Connector\n\nSame approach as abovee but now, we can follow the [JDBC Sink Connector](/use-cases/connect-jdbc/) use case to deploy the JDBC sink connector runtime, connecting your source Kafka environment and topic to the target DB2 database. \nThis project uses a simple Inventory App to present the content of the DB2 tables. The component view looks like in the following diagram:\n\n ![4](../../use-cases/connect-jdbc/images/comp-view.png)\n\n  ! This lab needs a refresh\n\n## Lab 5: RabbitMQ Source Connector to Kafka items topic\n\nFollow the [RabbitMQ Source Connector lab](/use-cases/connect-rabbitmq/) to deploy the Kafka connector runtime, and deploy the RabbitMQ source connector configuration so it can get messages from RabbitMQ `items` queue to the target Kafka `items` topic.\n\n ![2](../../use-cases/connect-rabbitmq/images/comp-view.png)\n\n\n\n","type":"Mdx","contentDigest":"4c2d1b60c9efb416ed6efe85a7a460bf","owner":"gatsby-plugin-mdx","counter":727},"frontmatter":{"title":"Real-time Inventory, powered by Kafka","description":"An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect."},"exports":{},"rawBody":"---\ntitle: Real-time Inventory, powered by Kafka\ndescription: An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect.\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 06/09/2021</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>What you will learn</AnchorLink>\n  <AnchorLink>Solution anatomy</AnchorLink>\n  <AnchorLink>General pre-requisites</AnchorLink>\n  <AnchorLink>Lab 1: Kafka only solution to demonstrate real-time inventory</AnchorLink>\n  <AnchorLink>Lab 2: Deploy the MQ Source Connector</AnchorLink>\n  <AnchorLink>Lab 3: Deploy the MQ Sink Connector [B]</AnchorLink>\n  <AnchorLink>Lab 4: Deploy the JDBC Sink Connector</AnchorLink>\n </AnchorLinks>\n\n## Overview\n\nThis scenario implements a simple real time inventory management solution based on real life MVPs we developed in 2020. \nStores are sending their sale transactions to a central messaging platform, based on queues or topics, and with the adoption\nof loosely coupled microservices, real time analytics Kafka is part of the architecture. A\ndopting Kafka Connect helps to integrate with existing applications without any changes to their base code.\n\n![](./images/hl-view.png)\n\nThis scenario addresses multiple use cases that aim to build an end to end data pipeline solution from source to different potential sinks:\n\n* The store simulator injects directly sell events to Kafka to the `items` topic\n* The store simulator can also generate message to IBM MQ using JMS API or to RabbitMQ using AMQP protocol. So we can demonstrate Kafka connectors with RabbitMQ source or IBM MQ source.\n* When messages are sourced to Queues, then a Kafka Source Connector is used to propagate message to `items` topics.\n* The Inventory computation is done using Kafka Stream component which produces inventory items and keep, in-memory a stock per items. \nThis inventory is exposed by API and used the interactive query capability of Kafka Streams.\n* Different sink connectors can be plugged into this `inventory` topic to move data to DB2, Elastic Search, Cloud Object Storage...\n\nWe try to make the business scenario, easily demonstrable by running locally with docker or on an OpenShift Cluster.\n\nIn real life, an as-is solution will include backend applications to manage the warehouses inventory, connected to a fulfillment home build application,\ncombined with store application and servers, e-commerce suite, and a set of SOA services acting as modern backend system.\n\n ![](./images/as-is-soa.png)\n\nWe may have integration flows to do data mapping, most of those calls are synchronous and to get one item availability, a lot of SOAP calls are done, \nincreasing latency, and risk of failure.\n\n### Component view\n\nAt the high level Kafka Connect is used to integrate external systems into Kafka. For example external systems can inject item sale messages to queue, from which a first Kafka source connector \npublishes the messages to a Kafka topic, which then will be processed by a series of event-driven microservices down to a final topic, which will be used by Sink connectors\n to send records to other external systems.\n\n ![1](./images/kconnect-overview.png)\n\n\nAll the components of this scenario are ready to run on OpenShift, but we are also providing different docker compose files to run all of those components on your local computer. \nThe important body of knowledge of this scenario is related to the programming model we used, and the Kafka Connect configuration and code.\n\n## What you will learn\n\n* Use Quarkus, with reactive programming API like Mutiny, with Kafka API to produce message to Kafka\n* Same Quarkus app can generate messages to RabbitMQ using the AMQP API\n* Same Quarkus app can generate mesasges to IBM MQ using JMS\n* Use Kafka Connect to get source and sink cluster to get date from the RabbitMQ queue and to IBM MQ queue or DB2\n* Use Quarkus and Kafka streams to compute aggregates to build an inventory view from the stream of events\n* Use the RabbitMQ source connector from IBM Event messaging open source contribution\n* Use the IBM MQ sink connector from IBM Event messaging open source contribution\n* Use the JDBC sink connector from IBM Event messaging open source contribution\n\n## Solution anatomy\n\n1. The [store simulator application](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) is a Quarkus app, which generates item sales to different possible messaging middlewares ( RabbitMQ, MQ or directly to Kafka). \nThe code of this application is in this https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator. If you want to browse the code, the main readme of this project includes how to package and run this app with docker compose, \nand explains how the code works. \nThe docker image is [quay.io/ibmcase/eda-store-simulator/](https://quay.io/ibmcase/eda-store-simulator)\n1. The item inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-item-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory). \nConsider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on `item.inventory` topic. \n1. The store inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in [the refarch-eda-store-inventory project](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory) the output is in `store.inventory` topic. \n2. The mock up Inventory mainframe application is not implemented and we will use the MQ tools to view the message in the `item.inventory` MQ queue.\n\n\n## General pre-requisites\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* [git CLI](https://git-scm.com/downloads).\n* Get access to an OpenShift Cluster\n* [OpenShift CLI](https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html) on your local environment.\n* [jq](https://stedolan.github.io/jq/) on your local environment.\n* Clone the Inventory lab repository: \n\n```sh\noc clone  https://github.com/ibm-cloud-architecture/eda-lab-inventory.git\n```\n\n\n## Lab 1: Kafka only solution to demonstrate real-time inventory with Kafka Streams\n\nIn this lab we will deploy the solution using Strimzi on OpenShift. The diagram looks like the following:\n\n![](/images/inventory-components.png)\n\nThe Kafka version is Strimzi 2.7.\n\nThe deployment to OpenShift using Kafka Strimzi operator, and gitops approach is described in dedicated lab you can read the details in the [Kafka Stream use case section](/use-cases/kafka-streams/lab-3/).\n\nHere are the simplest steps to deploy the solution in minimum configuration:\n\n1. Login to the OpenShift cluster (4.6 version): `oc login --token=sha256~d... --server=https://....containers.cloud.ibm.com...`\n1. Work from the clone repository: `cd eda-lab-inventory`\n1. Define the environment variables for your deployment in the: `scripts/env-strimzi.sh`\n1. Start the deploy with one script `./scripts/deployInventorySolutionWithStrimzi.sh --skip-login`\n\n\n## Lab 2: Deploy the MQ Source Connector\n\nThis lab uses the store simulator to generate events to MQ queue. The component view may look like the following figure:\n\n![](./images/lab2-mq-kafka.png)\n\nThe major difference with previous lab, is the addition of the IBM MQ Source Kafka connector to move records from `Items` queue to `items` Kafka topic.\n\nThis lab is defined in the [MQ to Kafka repository](https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka) with instructions on how to run it locally or on OpenShift.\n\n## Lab 3: Deploy the MQ Sink Connector\n\nFollowing the [MQ Sink Connector](/use-cases/connect-mq/#lab-2:-mq-sink-from-kafka) use case, we can deploy the MNQ Sink connector to OpenShift, connecting your source Kafka environment and topic to the target MQ queue manager.\n\n ! This lab needs a refresh\n\n## Lab 4: Deploy the JDBC Sink Connector\n\nSame approach as abovee but now, we can follow the [JDBC Sink Connector](/use-cases/connect-jdbc/) use case to deploy the JDBC sink connector runtime, connecting your source Kafka environment and topic to the target DB2 database. \nThis project uses a simple Inventory App to present the content of the DB2 tables. The component view looks like in the following diagram:\n\n ![4](../../use-cases/connect-jdbc/images/comp-view.png)\n\n  ! This lab needs a refresh\n\n## Lab 5: RabbitMQ Source Connector to Kafka items topic\n\nFollow the [RabbitMQ Source Connector lab](/use-cases/connect-rabbitmq/) to deploy the Kafka connector runtime, and deploy the RabbitMQ source connector configuration so it can get messages from RabbitMQ `items` queue to the target Kafka `items` topic.\n\n ![2](../../use-cases/connect-rabbitmq/images/comp-view.png)\n\n\n\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/scenarios/realtime-inventory/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}