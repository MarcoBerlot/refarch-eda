{"componentChunkName":"component---src-pages-use-cases-kafka-mm-2-lab-2-index-mdx","path":"/use-cases/kafka-mm2/lab-2/","result":{"pageContext":{"frontmatter":{"title":"Mirror Maker 2 ES on RHOS to ES on Cloud","description":"Using Mirror Maker 2 from Event Streams on OpenShift to Event Streams on cloud"},"relativePagePath":"/use-cases/kafka-mm2/lab-2/index.mdx","titleType":"append","MdxNode":{"id":"aa04f9fc-a36c-5149-a94c-dd7c4bd185a2","children":[],"parent":"60209e93-c715-5820-a67d-66fa0ff6e5d9","internal":{"content":"---\ntitle: Mirror Maker 2 ES on RHOS to ES on Cloud\ndescription: Using Mirror Maker 2 from Event Streams on OpenShift to Event Streams on cloud\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Pre-requisites</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on premise on OpenShift, and the target cluster is an Event Stream on Cloud. Mirror Maker 2 runs on OpenShift.\n\n ![1](../images/mm2-lab2.png)\n\n 1. Mirror Maker 2 runs on OpenShift as pod in the same namespace as Event Streams\n 2. A producer in python to send records to `products` topic, will run locally or could be deployed on OpenShift as a **job**\n 3. Event Streams on Cloud with replicated topic and Kafdrop to see the replicated messages.\n\n## Pre-requisites\n\n* We will use the Cloud shell tool, so connect and create the environment for this shell using the same instructions as in [the schema registry lab](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#ibm-cloud-shell).\n\n* Login to the OpenShift cluster using the console and get the API token\n\n ```shell\n oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443\n ```\n\n* If not done from lab 1, clone the github to get access to the Mirror Maker 2 manifests we are using:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency\n ```\n\n* As Event Streams on the Cloud does not authorize to create topic with the Kafka Admin API, we need to create the following topics with scripts or GUI. The naming convention respects the name of the source cluster as defined in the MirrorMaker2 manifest we will detail later.\n\n  * `es-1.products`   matches the number of partition allocated in the source topic (1 for example)\n  * `mirrormaker2-cluster-configs` one partition\n  * `mirrormaker2-cluster-offsets` 25 partitions with cleanup policy set to compact\n  * `mirrormaker2-cluster-status` 5 partitions.\n\nUse the default replication factor of 3.\n\n* Create a secret for the API KEY of the Event Streams on Cloud as target cluster:\n\n ```shell\n # Go to the project where Event Streams is installed\n oc project eventstreams\n oc create secret generic es-oc-api-secret --from-literal=password=<replace-with-event-streams-on-cloud-apikey>\n # Verify with \n oc describe secret es-oc-api-secret \n ```\n\n* Verify the Event Streams on OpenShift service end point URL. This URL will be used to configure Mirror Maker 2. \n\n ```shell\n # Use the bootstrap internal URL\n oc get svc light-es-kafka-bootstrap\n # The URL is something like\n light-es-kafka-bootstrap.integration.svc:9092\n ```\n\n* If the service was on 9093, this may mean eventstreams was configured with TLS security so, you need to verify the CA certificate secrets:\n\n ```shell\n  oc describe secret light-es-cluster-ca-cert\n ```\n\n* Get a user with write access to topic, group and transactions. You can use the User Interface to do so with the Create TLS credential on the internal URL panel. See [those instructions if needed](/use-cases/overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift).\n\n## Start Mirror Maker 2\n\nIn this lab, Mirror Maker 2 will run on the same cluster as Event Streams within the same namespace (e.g. eventstreams). \n\n* Define source and target cluster properties in a Mirror Maker 2 `es-ocp-to-es-oc.yml` descriptor file. There is one example of such file for the replication between Event Streams OCP to Event Streams on cloud [here: es-ocp-to-es-oc.yml](https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency/blob/master/mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml). We strongly recommend to study the schema definition of this [custom resource from this page](https://github.com/strimzi/strimzi-kafka-operator/blob/2d35bfcd99295bef8ee98de9d8b3c86cb33e5842/install/cluster-operator/048-Crd-kafkamirrormaker2.yaml#L648-L663). \n\n  Here are some important parameters you need to consider: The namespace needs to match the event streams project, and the annotations the product version and ID. The connectCluster needs to match the alias of the target cluster. The alias `es-ic` is defined later in this manifest:\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1alpha1\n  kind: KafkaMirrorMaker2\n  metadata:\n    name: mm2\n    namespace: eventstreams\n  spec:\n    template:\n      pod:\n        metadata:\n          annotations:\n            eventstreams.production.type: CloudPakForIntegrationNonProduction\n            productCloudpakRatio: \"2:1\"\n            productChargedContainers: mm2-mirrormaker2\n            productVersion: 10.1.0\n            productID: 2a79e49111f44ec3acd89608e56138f5\n            cloudpakName: IBM Cloud Pak for Integration\n            cloudpakId: c8b82d189e7545f0892db9ef2731b90d\n            productName: IBM Event Streams for Non Production\n            cloudpakVersion: 2020.3.1\n            productMetric: VIRTUAL_PROCESSOR_CORE\n    version: 2.6.0\n    replicas: 1\n    connectCluster: \"es-ic\"\n  ```\n\n  The version matches the Kafka version we use. The number of replicas can be set to 1 to start. The `eventstreams.production.type` is needed for Event Streams.\n\n  Then the yaml defines the connection configuration for each clusters: Event Streams on cloud, so you need to define the bootstrap servers (This could come from a config map too) and the API key coming from the previously defined secret. The user is always 'token'\n\n  ```yaml\n  clusters:\n    - alias: \"es-ic\"\n      bootstrapServers: broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n      config:\n        config.storage.replication.factor: 3\n        offset.storage.replication.factor: 3\n        status.storage.replication.factor: 3\n      tls: {}\n      authentication:\n        passwordSecret:\n            secretName: es-oc-api-secret  \n            password: password \n        username: token\n        type: plain\n  ```\n\n  For Event Streams on premise running within OpenShift, the connection uses TLS, certificates and Sram credentials. As we run in a separate namespace the URL is the 'external' one.\n\n  ```yaml\n  - alias: \"es-1\"\n      bootstrapServers: light-es-kafka-bootstrap.integration.svc:9093\n      config:\n        config.storage.replication.factor: 3\n        offset.storage.replication.factor: 3\n        status.storage.replication.factor: 3\n        ssl.endpoint.identification.algorithm: https\n      tls: \n        trustedCertificates:\n          - secretName: light-es-cluster-ca-cert\n            certificate: ca.crt\n      authentication:\n        type: tls\n        certificateAndKey:\n          certificate: user.crt\n          key: user.key\n          secretName: es-tls-user\n            \n  ```\n\n Finally the `connectCluster` attribute defines the cluster alias used by MirrorMaker2 to define its hidden topics, it must match the target cluster of the replication in the list at `spec.clusters`.\n    \n \n ```shell\n oc apply -f mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml\n ```\n\n* Verify the characteristics of the Mirror Maker 2 instance using the CLI\n\n ```shell\n oc describe kafkamirrormaker2 mm2\n ```\n* See the logs:\n\n  ```shell\n  oc get pods | grep mm2\n  oc logs mm2-mirrormaker2-...\n  ```\n\n## Start Consumer from target cluster\n\nAs Mirror Maker 2 is set to replicate topics to the `es-ic` cluster which is the event streams on cloud cluster we now need to connect a consumer to see the replicated records. For that we can use Kafdrop and we need tosSpecifying the target cluster as Event Streams on cloud:\n\nDefine environment variables to connect to Event Streams on cloud\n\n```\nES_IC_BROKERS=broker-0-...\nES_IC_USER=token\nES_IC_PASSWORD=n...\nES_IC_SASL_MECHANISM=PLAIN\nES_IC_LOGIN_MODULE=org.apache.kafka.common.security.plain.PlainLoginModule\n```\n\n```\n./scripts/startKafdrop.sh es-ic\n```\n\nGo to the es-1.products topic to see the replicated messages:\n\n ![9](../images/kafdrop-eso-esic.png)\n\nyou may have any now.\n\n## Start Producer to source cluster\n\nAs seen in lab 1, we will use the same python script to create products records. This time the script is producing products data to Event Streams on OpenShift. So we need the URL, pem, and user to access this cluster.\n\nThe credentials can be accessed and defined using the GUI. [This section in common use case prerequisite note](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#getting-scram-authentication-from-event-streams-on-openshift) goes into the details on how to get the external URL and Scram password. \n\n* If not done before download the pem file using the following CLI commands:\n\n ```shell\n cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n cloudctl es init\n cloudctl es certificates --format pem\n ```\n save this `.pem` file under `mirror-maker2` folder.\n\n* Get userid, password and information of one of thesram based authentication user defined previously. The namespace used below is the one where event streams is installed.\n\n ```shell\n oc get kafkausers \n ```\n* Set the following environment variables:\n\n ```shell\n ES_OCP_BROKERS=light-es-kafka-bootstrap-integration.gse-eda-2020-10-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-south.containers.appdomain.cloud:443\n ES_OCP_USER=es-scram-user\n ES_OCP_PASSWORD=$(oc get secret $ES_OCP_USER --namespace integration -o jsonpath='{.data.password}' | base64 --decode)\n ES_OCP_CERT=/home/es-cert.pem\n ES_OCP_SASL_MECHANISM=SCRAM-SHA-512\n \n ```\n Now send five records:\n ```shell\n cd mirror-maker-2\n ./sendProductRecords.sh es-ocp\n ```\n\n * Going to the Event Streams Console we can see the message in the `products` topic.\n\n ## Understanding MirrorMaker 2 trace \n\nA lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues.\n\nIf some messages happen with NO_LEADER for one of the topics, this means MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface.\n\nThen we can observe the following:\n * It creates a producer to the target cluster for the offsets topics: ` [Producer clientId=producer-1] Cluster ID: Bj7Ui3UPQaKtJx7HOkWxPw`\n * It creates consumer for the 25 offset topic partitions: `[Consumer clientId=consumer-mirrormaker2-cluster-1, groupId=mirrormaker2-cluster] Subscribed to partition(s): mirrormaker2-cluster-offsets-0,....`\n * One Kafka connect worker is started: `Worker started ... Starting KafkaBasedLog with topic mirrormaker2-cluster-status`\n * Create a producer and consumers for the ` mirrormaker2-cluster-status` topic for 5 partitions\n * Create another producer and consumer for the `mirrormaker2-cluster-config` topic\n\n * Create WorkerSourceTask{id=es-1->es-ic.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat.\n * WorkerSourceTask{id=es-1->es-ic.MirrorSourceConnector-0} for the topic to replicate","type":"Mdx","contentDigest":"28dd04b4f78694c61b96d5a9e61f2839","counter":695,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Mirror Maker 2 ES on RHOS to ES on Cloud","description":"Using Mirror Maker 2 from Event Streams on OpenShift to Event Streams on cloud"},"exports":{},"rawBody":"---\ntitle: Mirror Maker 2 ES on RHOS to ES on Cloud\ndescription: Using Mirror Maker 2 from Event Streams on OpenShift to Event Streams on cloud\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Pre-requisites</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on premise on OpenShift, and the target cluster is an Event Stream on Cloud. Mirror Maker 2 runs on OpenShift.\n\n ![1](../images/mm2-lab2.png)\n\n 1. Mirror Maker 2 runs on OpenShift as pod in the same namespace as Event Streams\n 2. A producer in python to send records to `products` topic, will run locally or could be deployed on OpenShift as a **job**\n 3. Event Streams on Cloud with replicated topic and Kafdrop to see the replicated messages.\n\n## Pre-requisites\n\n* We will use the Cloud shell tool, so connect and create the environment for this shell using the same instructions as in [the schema registry lab](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#ibm-cloud-shell).\n\n* Login to the OpenShift cluster using the console and get the API token\n\n ```shell\n oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443\n ```\n\n* If not done from lab 1, clone the github to get access to the Mirror Maker 2 manifests we are using:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency\n ```\n\n* As Event Streams on the Cloud does not authorize to create topic with the Kafka Admin API, we need to create the following topics with scripts or GUI. The naming convention respects the name of the source cluster as defined in the MirrorMaker2 manifest we will detail later.\n\n  * `es-1.products`   matches the number of partition allocated in the source topic (1 for example)\n  * `mirrormaker2-cluster-configs` one partition\n  * `mirrormaker2-cluster-offsets` 25 partitions with cleanup policy set to compact\n  * `mirrormaker2-cluster-status` 5 partitions.\n\nUse the default replication factor of 3.\n\n* Create a secret for the API KEY of the Event Streams on Cloud as target cluster:\n\n ```shell\n # Go to the project where Event Streams is installed\n oc project eventstreams\n oc create secret generic es-oc-api-secret --from-literal=password=<replace-with-event-streams-on-cloud-apikey>\n # Verify with \n oc describe secret es-oc-api-secret \n ```\n\n* Verify the Event Streams on OpenShift service end point URL. This URL will be used to configure Mirror Maker 2. \n\n ```shell\n # Use the bootstrap internal URL\n oc get svc light-es-kafka-bootstrap\n # The URL is something like\n light-es-kafka-bootstrap.integration.svc:9092\n ```\n\n* If the service was on 9093, this may mean eventstreams was configured with TLS security so, you need to verify the CA certificate secrets:\n\n ```shell\n  oc describe secret light-es-cluster-ca-cert\n ```\n\n* Get a user with write access to topic, group and transactions. You can use the User Interface to do so with the Create TLS credential on the internal URL panel. See [those instructions if needed](/use-cases/overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift).\n\n## Start Mirror Maker 2\n\nIn this lab, Mirror Maker 2 will run on the same cluster as Event Streams within the same namespace (e.g. eventstreams). \n\n* Define source and target cluster properties in a Mirror Maker 2 `es-ocp-to-es-oc.yml` descriptor file. There is one example of such file for the replication between Event Streams OCP to Event Streams on cloud [here: es-ocp-to-es-oc.yml](https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency/blob/master/mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml). We strongly recommend to study the schema definition of this [custom resource from this page](https://github.com/strimzi/strimzi-kafka-operator/blob/2d35bfcd99295bef8ee98de9d8b3c86cb33e5842/install/cluster-operator/048-Crd-kafkamirrormaker2.yaml#L648-L663). \n\n  Here are some important parameters you need to consider: The namespace needs to match the event streams project, and the annotations the product version and ID. The connectCluster needs to match the alias of the target cluster. The alias `es-ic` is defined later in this manifest:\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1alpha1\n  kind: KafkaMirrorMaker2\n  metadata:\n    name: mm2\n    namespace: eventstreams\n  spec:\n    template:\n      pod:\n        metadata:\n          annotations:\n            eventstreams.production.type: CloudPakForIntegrationNonProduction\n            productCloudpakRatio: \"2:1\"\n            productChargedContainers: mm2-mirrormaker2\n            productVersion: 10.1.0\n            productID: 2a79e49111f44ec3acd89608e56138f5\n            cloudpakName: IBM Cloud Pak for Integration\n            cloudpakId: c8b82d189e7545f0892db9ef2731b90d\n            productName: IBM Event Streams for Non Production\n            cloudpakVersion: 2020.3.1\n            productMetric: VIRTUAL_PROCESSOR_CORE\n    version: 2.6.0\n    replicas: 1\n    connectCluster: \"es-ic\"\n  ```\n\n  The version matches the Kafka version we use. The number of replicas can be set to 1 to start. The `eventstreams.production.type` is needed for Event Streams.\n\n  Then the yaml defines the connection configuration for each clusters: Event Streams on cloud, so you need to define the bootstrap servers (This could come from a config map too) and the API key coming from the previously defined secret. The user is always 'token'\n\n  ```yaml\n  clusters:\n    - alias: \"es-ic\"\n      bootstrapServers: broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n      config:\n        config.storage.replication.factor: 3\n        offset.storage.replication.factor: 3\n        status.storage.replication.factor: 3\n      tls: {}\n      authentication:\n        passwordSecret:\n            secretName: es-oc-api-secret  \n            password: password \n        username: token\n        type: plain\n  ```\n\n  For Event Streams on premise running within OpenShift, the connection uses TLS, certificates and Sram credentials. As we run in a separate namespace the URL is the 'external' one.\n\n  ```yaml\n  - alias: \"es-1\"\n      bootstrapServers: light-es-kafka-bootstrap.integration.svc:9093\n      config:\n        config.storage.replication.factor: 3\n        offset.storage.replication.factor: 3\n        status.storage.replication.factor: 3\n        ssl.endpoint.identification.algorithm: https\n      tls: \n        trustedCertificates:\n          - secretName: light-es-cluster-ca-cert\n            certificate: ca.crt\n      authentication:\n        type: tls\n        certificateAndKey:\n          certificate: user.crt\n          key: user.key\n          secretName: es-tls-user\n            \n  ```\n\n Finally the `connectCluster` attribute defines the cluster alias used by MirrorMaker2 to define its hidden topics, it must match the target cluster of the replication in the list at `spec.clusters`.\n    \n \n ```shell\n oc apply -f mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml\n ```\n\n* Verify the characteristics of the Mirror Maker 2 instance using the CLI\n\n ```shell\n oc describe kafkamirrormaker2 mm2\n ```\n* See the logs:\n\n  ```shell\n  oc get pods | grep mm2\n  oc logs mm2-mirrormaker2-...\n  ```\n\n## Start Consumer from target cluster\n\nAs Mirror Maker 2 is set to replicate topics to the `es-ic` cluster which is the event streams on cloud cluster we now need to connect a consumer to see the replicated records. For that we can use Kafdrop and we need tosSpecifying the target cluster as Event Streams on cloud:\n\nDefine environment variables to connect to Event Streams on cloud\n\n```\nES_IC_BROKERS=broker-0-...\nES_IC_USER=token\nES_IC_PASSWORD=n...\nES_IC_SASL_MECHANISM=PLAIN\nES_IC_LOGIN_MODULE=org.apache.kafka.common.security.plain.PlainLoginModule\n```\n\n```\n./scripts/startKafdrop.sh es-ic\n```\n\nGo to the es-1.products topic to see the replicated messages:\n\n ![9](../images/kafdrop-eso-esic.png)\n\nyou may have any now.\n\n## Start Producer to source cluster\n\nAs seen in lab 1, we will use the same python script to create products records. This time the script is producing products data to Event Streams on OpenShift. So we need the URL, pem, and user to access this cluster.\n\nThe credentials can be accessed and defined using the GUI. [This section in common use case prerequisite note](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#getting-scram-authentication-from-event-streams-on-openshift) goes into the details on how to get the external URL and Scram password. \n\n* If not done before download the pem file using the following CLI commands:\n\n ```shell\n cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n cloudctl es init\n cloudctl es certificates --format pem\n ```\n save this `.pem` file under `mirror-maker2` folder.\n\n* Get userid, password and information of one of thesram based authentication user defined previously. The namespace used below is the one where event streams is installed.\n\n ```shell\n oc get kafkausers \n ```\n* Set the following environment variables:\n\n ```shell\n ES_OCP_BROKERS=light-es-kafka-bootstrap-integration.gse-eda-2020-10-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-south.containers.appdomain.cloud:443\n ES_OCP_USER=es-scram-user\n ES_OCP_PASSWORD=$(oc get secret $ES_OCP_USER --namespace integration -o jsonpath='{.data.password}' | base64 --decode)\n ES_OCP_CERT=/home/es-cert.pem\n ES_OCP_SASL_MECHANISM=SCRAM-SHA-512\n \n ```\n Now send five records:\n ```shell\n cd mirror-maker-2\n ./sendProductRecords.sh es-ocp\n ```\n\n * Going to the Event Streams Console we can see the message in the `products` topic.\n\n ## Understanding MirrorMaker 2 trace \n\nA lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues.\n\nIf some messages happen with NO_LEADER for one of the topics, this means MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface.\n\nThen we can observe the following:\n * It creates a producer to the target cluster for the offsets topics: ` [Producer clientId=producer-1] Cluster ID: Bj7Ui3UPQaKtJx7HOkWxPw`\n * It creates consumer for the 25 offset topic partitions: `[Consumer clientId=consumer-mirrormaker2-cluster-1, groupId=mirrormaker2-cluster] Subscribed to partition(s): mirrormaker2-cluster-offsets-0,....`\n * One Kafka connect worker is started: `Worker started ... Starting KafkaBasedLog with topic mirrormaker2-cluster-status`\n * Create a producer and consumers for the ` mirrormaker2-cluster-status` topic for 5 partitions\n * Create another producer and consumer for the `mirrormaker2-cluster-config` topic\n\n * Create WorkerSourceTask{id=es-1->es-ic.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat.\n * WorkerSourceTask{id=es-1->es-ic.MirrorSourceConnector-0} for the topic to replicate","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-mm2/lab-2/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}