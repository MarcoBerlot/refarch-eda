{"componentChunkName":"component---src-pages-use-cases-overview-pre-requisites-mdx","path":"/use-cases/overview/pre-requisites/","result":{"pageContext":{"frontmatter":{"title":"Common pre-requisites","description":"Common pre-requisites for the different labs and use cases"},"relativePagePath":"/use-cases/overview/pre-requisites.mdx","titleType":"append","MdxNode":{"id":"0c32a2cc-672f-585a-bee3-10df722db658","children":[],"parent":"ba8759b6-67e9-59e6-8559-1263014dc089","internal":{"content":"---\ntitle: Common pre-requisites\ndescription: Common pre-requisites for the different labs and use cases\n---\n\n<AnchorLinks>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Creating Event Streams Topics</AnchorLink>\n  <AnchorLink>Getting TLS authentication from Event Streams on OpenShift</AnchorLink>\n</AnchorLinks>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Creating Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift using the User Interface. The example is to define a topic named INBOUND with 1 partition and a replica set to 3. Change to any other topic you want to create.\n\n- Get the route for event streams. We assume the namespace is `eventstreams`\n \n ```shell\n  oc get routes -n eventstreams | grep ui\n  ```\n\n- Navigate to the Event Streams Console using the exposed route URL.\n\n- Click the Topics option on the navigation bar on the left. Create the INBOUND topic.\n\n![Create Topic](./images/create-topic.png)\n\n![Topic Name](./images/inbound-topic-name.png)\n\n- Leave Partitions at 1.\n\n![Partition](./images/partitions.png)\n\n- Depending on how long you want messages to persist you can change this.\n\n![Message Retention](./images/message-retention.png)\n\n- You can leave Replication Factor at the default 3.\n\n![Replication](./images/replicas.png)\n\n- Click Create.\n\n### Create topic using CLI\n\nYou can do the samething using CLI, [see the product documentation](https://ibm.github.io/event-streams/getting-started/creating-topics/).\n\n ```shell\n cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n ```\n\n## Getting TLS authentication from Event Streams on OpenShift\n\n### Get TLS server public certificate\n\nThe cluster public certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. We can also get those certificate and password via CLI:\n\nSee [product documentation for detail](https://ibm.github.io/event-streams/getting-started/connecting/). A quick set of commands:\n\n ```shell\n oc get secret -n eventstreams | grep cluster-ca-cert\n # use the ca-cert for the instance you target \n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams -o jsonpath='{.data.ca\\.p12}' | base64 --decode > es-cert.p12\n # or the command:\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.p12 -n eventstreams\n # and password\n oc get secret minimal-prod-cluster-ca-cert -n eventstreams -o jsonpath='{.data.ca\\.password}' | base64 --decode\n # or the command\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.password -n eventstreams\n ```\n\n### Get Scram-sha-512 user \n\nSelect one of the Kafka users with Scram authentication defined or create a new one with the produce, consume messages and create topic and schemas authorizations, on all topics or topic with a specific prefix, on all consumer groups or again with a specific prefix, all transaction IDs.\n\n  ```shell\n  # if not logged yes to your openshift cluster where the docker private registry resides do:\n  oc login --token=... --server=https://c...\n  oc get kafkausers -n eventstreams\n\n  NAME                                     AUTHENTICATION   AUTHORIZATION\n  demo-app                                 scram-sha-512    simple\n  ```\n\n  We use a user with scram-sha-512 authentication named: `demo-app`\n\n* Copy user's secret to the current project where the application will run\n\n ```shell\n oc get secret  demo-app -n eventstreams --export -o yaml | oc apply -f -\n ```\n\n* Get the user password:\n\n ```shell\n oc get secret demo-app -n eventstreams -o jsonpath='{.data.password}' | base64 --decode\n ```\n\n* Set the KAFKA_USER and KAFKA_PASSWORD, and SECURE_PROTOCOL=SASL_SSL environment variables for your application.\n\n### Get TLS user \n\nIf you want to use mutual authentication with client certificate, you need a tls user. And use the same commands as in previous section to get user name and user's password. The differences are in the password access and that you need to download client certificate as keystore to be integrated in the Java app.\n\n ```shell\n # password is in another key\n oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.password}' | base64 --decode  \n ```\n\n* Get the user client certificate and password\n\n  ```shell\n  cd certs\n  oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.p12}' | base64 --decode > user.p12\n  ```\n\n  Modify USER_CERT_PWD and USER_CERT_PATH=${PWD}/certs/user.p12 in the `.env` file\n\n\nAlso set the env variable as SECURE_PROTOCOL=SSL","type":"Mdx","contentDigest":"a309b4dc34bd5295930b7b19467d88f3","counter":616,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Common pre-requisites\ndescription: Common pre-requisites for the different labs and use cases\n---\n\n<AnchorLinks>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Creating Event Streams Topics</AnchorLink>\n  <AnchorLink>Getting TLS authentication from Event Streams on OpenShift</AnchorLink>\n</AnchorLinks>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Creating Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift using the User Interface. The example is to define a topic named INBOUND with 1 partition and a replica set to 3. Change to any other topic you want to create.\n\n- Get the route for event streams. We assume the namespace is `eventstreams`\n \n ```shell\n  oc get routes -n eventstreams | grep ui\n  ```\n\n- Navigate to the Event Streams Console using the exposed route URL.\n\n- Click the Topics option on the navigation bar on the left. Create the INBOUND topic.\n\n![Create Topic](./images/create-topic.png)\n\n![Topic Name](./images/inbound-topic-name.png)\n\n- Leave Partitions at 1.\n\n![Partition](./images/partitions.png)\n\n- Depending on how long you want messages to persist you can change this.\n\n![Message Retention](./images/message-retention.png)\n\n- You can leave Replication Factor at the default 3.\n\n![Replication](./images/replicas.png)\n\n- Click Create.\n\n### Create topic using CLI\n\nYou can do the samething using CLI, [see the product documentation](https://ibm.github.io/event-streams/getting-started/creating-topics/).\n\n ```shell\n cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n ```\n\n## Getting TLS authentication from Event Streams on OpenShift\n\n### Get TLS server public certificate\n\nThe cluster public certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. We can also get those certificate and password via CLI:\n\nSee [product documentation for detail](https://ibm.github.io/event-streams/getting-started/connecting/). A quick set of commands:\n\n ```shell\n oc get secret -n eventstreams | grep cluster-ca-cert\n # use the ca-cert for the instance you target \n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams -o jsonpath='{.data.ca\\.p12}' | base64 --decode > es-cert.p12\n # or the command:\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.p12 -n eventstreams\n # and password\n oc get secret minimal-prod-cluster-ca-cert -n eventstreams -o jsonpath='{.data.ca\\.password}' | base64 --decode\n # or the command\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.password -n eventstreams\n ```\n\n### Get Scram-sha-512 user \n\nSelect one of the Kafka users with Scram authentication defined or create a new one with the produce, consume messages and create topic and schemas authorizations, on all topics or topic with a specific prefix, on all consumer groups or again with a specific prefix, all transaction IDs.\n\n  ```shell\n  # if not logged yes to your openshift cluster where the docker private registry resides do:\n  oc login --token=... --server=https://c...\n  oc get kafkausers -n eventstreams\n\n  NAME                                     AUTHENTICATION   AUTHORIZATION\n  demo-app                                 scram-sha-512    simple\n  ```\n\n  We use a user with scram-sha-512 authentication named: `demo-app`\n\n* Copy user's secret to the current project where the application will run\n\n ```shell\n oc get secret  demo-app -n eventstreams --export -o yaml | oc apply -f -\n ```\n\n* Get the user password:\n\n ```shell\n oc get secret demo-app -n eventstreams -o jsonpath='{.data.password}' | base64 --decode\n ```\n\n* Set the KAFKA_USER and KAFKA_PASSWORD, and SECURE_PROTOCOL=SASL_SSL environment variables for your application.\n\n### Get TLS user \n\nIf you want to use mutual authentication with client certificate, you need a tls user. And use the same commands as in previous section to get user name and user's password. The differences are in the password access and that you need to download client certificate as keystore to be integrated in the Java app.\n\n ```shell\n # password is in another key\n oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.password}' | base64 --decode  \n ```\n\n* Get the user client certificate and password\n\n  ```shell\n  cd certs\n  oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.p12}' | base64 --decode > user.p12\n  ```\n\n  Modify USER_CERT_PWD and USER_CERT_PATH=${PWD}/certs/user.p12 in the `.env` file\n\n\nAlso set the env variable as SECURE_PROTOCOL=SSL","frontmatter":{"title":"Common pre-requisites","description":"Common pre-requisites for the different labs and use cases"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/overview/pre-requisites.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}