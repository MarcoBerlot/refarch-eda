{"componentChunkName":"component---src-pages-technology-security-index-mdx","path":"/technology/security/","result":{"pageContext":{"frontmatter":{"title":"Kafka Security Overview","description":"Kafka Security Overview"},"relativePagePath":"/technology/security/index.mdx","titleType":"append","MdxNode":{"id":"69a6eeb5-a4a6-5b53-9590-d8db67b54135","children":[],"parent":"087b9bc6-13bf-570b-bb7c-4d3c8defba15","internal":{"content":"---\ntitle: Kafka Security Overview\ndescription: Kafka Security Overview\n---\n\nReview [this video for a refresh on SSL and TLS certificates](https://www.youtube.com/watch?v=T4Df5_cojAs) and keep in mind what the speaker quotes:\n\n> * Any message encrypted with Bob’s public key can only be decrypted with Bob’s private key\n> * Anyone with access to Alice’s public key can verify that a message could only have been created by someone with access to Alice’s private key.\n\n![](./images/tls-overview.png)\n\nFor a deeper dive into security administration [see confluent article](https://docs.confluent.io/platform/current/security/general-overview.html) and [product documentation](http://kafka.apache.org/documentation/#security)\nand Rick's blogs [Part 1](https://rosowski.medium.com/kafka-security-fundamentals-the-rosetta-stone-to-your-event-streaming-infrastructure-518f49640db4) and [Part 2](https://rosowski.medium.com/kafka-security-fundamentals-adding-tls-to-your-event-driven-utility-belt-432307f4ff62)\n\n\n## To connect to kafka using Kafka API\n\nThe important Kafka client application settings are:\n\n* the [security.protocol](http://kafka.apache.org/documentation/#adminclientconfigs_security.protocol)\nwhich should match the listeners configured in the Kafka cluster. The valid values are:\n\n```sh\nPLAINTEXT (using PLAINTEXT transport layer & no authentication - default value).\nSSL (using SSL transport layer & certificate-based authentication)\nSASL_PLAINTEXT (using PLAINTEXT transport layer & SASL-based authentication)\nSASL_SSL (using SSL transport layer & SASL-based authentication)\n```\n\nHere is an example from our [quickstarts]() of Quarkus properties for a Kafka Java API:\n\n```\n```\n\n## Understand the Kafka cluster listeners\n\nIn Event Streams the following yaml defines the Kafka listener to be used\nfor the different channel: \nOn port 9092 it will be a plain connection and no TLS encryption.\n\n```yaml\nlisteners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n        authentication:\n            type: tls\n      - name: external\n        type: route\n        port: 9094\n        tls: true \n        authentication:\n          type: scram-sha-512\n```\n`tls` boolean is for the traffic encryption, while `authentication.type` will define the matching security protocol.\n\n9093 is a mutual TLS authentication with TLS encrypted communication, while 9094 is using scram authentication and TLS encrypted communication\n\n* `ssl.truststore.location` and `ssl.truststore.password`: when doing TLS encryption we need to provide our Kafka clients\n with the location of a trusted Certificate Authority-based certificate. This file is often provided by the \n Kafka administrator and is generally unique to the specific Kafka cluster deployment. The certificate is in \n JKS or PKCS12 format for JVM languages and PEM/ P12 for nodejs or Python.\n\nImporting a certificate into one’s truststore also means trusting all certificates that are signed by that certificate.\n\nTo extract a PEM-based certificate from a JKS-based truststore, you can use the following command:\n\n```sh\nkeytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n```\n\nTo build a PKCS12 from a pem do\n\n```sh\nopenssl pkcs12 -export -in cert.pem -out cert.p12\n# if you want jks\nkeytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks\n```\n\n* [sasl.mechanism](http://kafka.apache.org/documentation/#adminclientconfigs_sasl.mechanism) for authentication protocol used. Possible values are:\n\n```\nPLAIN (cleartext passwords, although they will be encrypted across the wire per security.protocol settings above)\nSCRAM-SHA-512 (modern Salted Challenge Response Authentication Mechanism)\nGSSAPI (Kerberos-supported authentication and the default if not specified otherwise)\n```\n\n* for java based app, the `sasl.jaas.config` strings is one of the following depending of the sasl.mechanism:\n\n```\nsasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nsasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n```\n\nFor **external connection** to Strimzi cluster use the following, where USERNAME is a scram-user\n\n```sh\nbootstrap.servers={kafka-cluster-name}-kafka-bootstrap-{namespace}.{kubernetes-cluster-fully-qualified-domain-name}:443\nsecurity.protocol=SASL_SSL\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nssl.truststore.location={/provided/to/you/by/the/kafka/administrator}\nssl.truststore.password={__provided_to_you_by_the_kafka_administrator__}\n```\n\nTo get the user password get the user secret:\n\n```shell\noc get secret scram-user -o jsonpath='{.data.admin_password}' | base64 --decode && echo \"\"\n```\n\nTo get the Bootstrap URL use: \n\n```\nexport K_CLUSTER_NAME=mycluster\nexport BOOTSTRAP=\"$(oc get route ${K_CLUSTER_NAME}-kafka-bootstrap -o jsonpath='{.spec.host}'):443\"\n```\n\nThe `sasl.jaas.config` can come from an environment variable inside of a secret, but in fact it is already defined in the scram user in Strimzi:\n\n```sh\noc get secret my-user -o json | jq -r '.data[\"sasl.jaas.config\"]' | base64 -d -\n```\n\n* For internal communication, with PLAIN the setting is\n\n```sh\nbootstrap.servers={kafka-cluster-name}-kafka-bootstrap.{namespace}.svc.cluster.local:9093\nsecurity.protocol = SASL_PLAINTEXT (these clients do not require SSL-based encryption as they are local to the cluster)\nsasl.mechanism = PLAIN\nsasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n```\n\n* For internal authentication with mutual TLS the settings: The certificates are mounted into the pod:\n\n```\nbootstrap.servers={kafka-cluster-name}-kafka-bootstrap.{namespace}.svc.cluster.local:9093\nsecurity.protocol=SSL\nssl.truststore.location=/deployments/certs/server/ca.p12\nssl.truststore.password={__provided_to_you_by_kafka_administrator__}\nssl.keystore.location=/deployments/certs/user/user.p12\nssl.keystore.password={__extracted_from_generated_kafka_user_secret_with_key=user.password__}\n```\n\nRemember that if the application does not run in the same namespace as the kafka cluster then copy the secrets with something like\n\n```sh\nif [[ -z $(oc get secret ${TLS_USER} 2> /dev/null) ]]\nthen\n   # As the project is personal to the user, we can keep a generic name for the secret\n   oc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2> /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n```\n","type":"Mdx","contentDigest":"3a55f48ea4ee86d17b0cb3f3914f9788","owner":"gatsby-plugin-mdx","counter":806},"frontmatter":{"title":"Kafka Security Overview","description":"Kafka Security Overview"},"exports":{},"rawBody":"---\ntitle: Kafka Security Overview\ndescription: Kafka Security Overview\n---\n\nReview [this video for a refresh on SSL and TLS certificates](https://www.youtube.com/watch?v=T4Df5_cojAs) and keep in mind what the speaker quotes:\n\n> * Any message encrypted with Bob’s public key can only be decrypted with Bob’s private key\n> * Anyone with access to Alice’s public key can verify that a message could only have been created by someone with access to Alice’s private key.\n\n![](./images/tls-overview.png)\n\nFor a deeper dive into security administration [see confluent article](https://docs.confluent.io/platform/current/security/general-overview.html) and [product documentation](http://kafka.apache.org/documentation/#security)\nand Rick's blogs [Part 1](https://rosowski.medium.com/kafka-security-fundamentals-the-rosetta-stone-to-your-event-streaming-infrastructure-518f49640db4) and [Part 2](https://rosowski.medium.com/kafka-security-fundamentals-adding-tls-to-your-event-driven-utility-belt-432307f4ff62)\n\n\n## To connect to kafka using Kafka API\n\nThe important Kafka client application settings are:\n\n* the [security.protocol](http://kafka.apache.org/documentation/#adminclientconfigs_security.protocol)\nwhich should match the listeners configured in the Kafka cluster. The valid values are:\n\n```sh\nPLAINTEXT (using PLAINTEXT transport layer & no authentication - default value).\nSSL (using SSL transport layer & certificate-based authentication)\nSASL_PLAINTEXT (using PLAINTEXT transport layer & SASL-based authentication)\nSASL_SSL (using SSL transport layer & SASL-based authentication)\n```\n\nHere is an example from our [quickstarts]() of Quarkus properties for a Kafka Java API:\n\n```\n```\n\n## Understand the Kafka cluster listeners\n\nIn Event Streams the following yaml defines the Kafka listener to be used\nfor the different channel: \nOn port 9092 it will be a plain connection and no TLS encryption.\n\n```yaml\nlisteners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n        authentication:\n            type: tls\n      - name: external\n        type: route\n        port: 9094\n        tls: true \n        authentication:\n          type: scram-sha-512\n```\n`tls` boolean is for the traffic encryption, while `authentication.type` will define the matching security protocol.\n\n9093 is a mutual TLS authentication with TLS encrypted communication, while 9094 is using scram authentication and TLS encrypted communication\n\n* `ssl.truststore.location` and `ssl.truststore.password`: when doing TLS encryption we need to provide our Kafka clients\n with the location of a trusted Certificate Authority-based certificate. This file is often provided by the \n Kafka administrator and is generally unique to the specific Kafka cluster deployment. The certificate is in \n JKS or PKCS12 format for JVM languages and PEM/ P12 for nodejs or Python.\n\nImporting a certificate into one’s truststore also means trusting all certificates that are signed by that certificate.\n\nTo extract a PEM-based certificate from a JKS-based truststore, you can use the following command:\n\n```sh\nkeytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n```\n\nTo build a PKCS12 from a pem do\n\n```sh\nopenssl pkcs12 -export -in cert.pem -out cert.p12\n# if you want jks\nkeytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks\n```\n\n* [sasl.mechanism](http://kafka.apache.org/documentation/#adminclientconfigs_sasl.mechanism) for authentication protocol used. Possible values are:\n\n```\nPLAIN (cleartext passwords, although they will be encrypted across the wire per security.protocol settings above)\nSCRAM-SHA-512 (modern Salted Challenge Response Authentication Mechanism)\nGSSAPI (Kerberos-supported authentication and the default if not specified otherwise)\n```\n\n* for java based app, the `sasl.jaas.config` strings is one of the following depending of the sasl.mechanism:\n\n```\nsasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nsasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n```\n\nFor **external connection** to Strimzi cluster use the following, where USERNAME is a scram-user\n\n```sh\nbootstrap.servers={kafka-cluster-name}-kafka-bootstrap-{namespace}.{kubernetes-cluster-fully-qualified-domain-name}:443\nsecurity.protocol=SASL_SSL\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\nssl.truststore.location={/provided/to/you/by/the/kafka/administrator}\nssl.truststore.password={__provided_to_you_by_the_kafka_administrator__}\n```\n\nTo get the user password get the user secret:\n\n```shell\noc get secret scram-user -o jsonpath='{.data.admin_password}' | base64 --decode && echo \"\"\n```\n\nTo get the Bootstrap URL use: \n\n```\nexport K_CLUSTER_NAME=mycluster\nexport BOOTSTRAP=\"$(oc get route ${K_CLUSTER_NAME}-kafka-bootstrap -o jsonpath='{.spec.host}'):443\"\n```\n\nThe `sasl.jaas.config` can come from an environment variable inside of a secret, but in fact it is already defined in the scram user in Strimzi:\n\n```sh\noc get secret my-user -o json | jq -r '.data[\"sasl.jaas.config\"]' | base64 -d -\n```\n\n* For internal communication, with PLAIN the setting is\n\n```sh\nbootstrap.servers={kafka-cluster-name}-kafka-bootstrap.{namespace}.svc.cluster.local:9093\nsecurity.protocol = SASL_PLAINTEXT (these clients do not require SSL-based encryption as they are local to the cluster)\nsasl.mechanism = PLAIN\nsasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";\n```\n\n* For internal authentication with mutual TLS the settings: The certificates are mounted into the pod:\n\n```\nbootstrap.servers={kafka-cluster-name}-kafka-bootstrap.{namespace}.svc.cluster.local:9093\nsecurity.protocol=SSL\nssl.truststore.location=/deployments/certs/server/ca.p12\nssl.truststore.password={__provided_to_you_by_kafka_administrator__}\nssl.keystore.location=/deployments/certs/user/user.p12\nssl.keystore.password={__extracted_from_generated_kafka_user_secret_with_key=user.password__}\n```\n\nRemember that if the application does not run in the same namespace as the kafka cluster then copy the secrets with something like\n\n```sh\nif [[ -z $(oc get secret ${TLS_USER} 2> /dev/null) ]]\nthen\n   # As the project is personal to the user, we can keep a generic name for the secret\n   oc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2> /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n```\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/security/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}