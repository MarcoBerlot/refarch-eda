{"componentChunkName":"component---src-pages-technology-avro-schemas-index-mdx","path":"/technology/avro-schemas/","result":{"pageContext":{"frontmatter":{"title":"Apache Avro, Data Schemas and Schema Registry","description":"Apache Avro data serialization, data schemas for data definition and correctness and Schema Registry for data schema management"},"relativePagePath":"/technology/avro-schemas/index.mdx","titleType":"append","MdxNode":{"id":"29b08125-e614-53fd-a871-71f8e781e377","children":[],"parent":"7b81562c-0278-5f51-be9c-753af9a1eaeb","internal":{"content":"---\ntitle: Apache Avro, Data Schemas and Schema Registry\ndescription: Apache Avro data serialization, data schemas for data definition and correctness and Schema Registry for data schema management\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updates 1/24/2022</strong>\n</InlineNotification>\n\nThis chapter describes what and why Avro and Schema registry are important elements of any event-driven solutions.\n\n\n<AnchorLinks>\n    <AnchorLink>Why this is important</AnchorLink>\n    <AnchorLink>Schema Registry</AnchorLink>\n    <AnchorLink>Apache Avro</AnchorLink> \n    <AnchorLink>Data Schemas</AnchorLink> \n    <AnchorLink>Avro, Kafka and Schema Registry</AnchorLink>\n    <AnchorLink>More reading</AnchorLink>\n</AnchorLinks>\n\n## Why this is important\n\nLoosely coupling and asynchronous communication between applications does not mean there is no contract to enforce some constraint between producer and consumer. \nWhen we talk about contract we can first think about schema as we did with XSD. In the world of JSON, JSON schema and Avro schemas can be used to define\ndata structure of the message. As there is a need to get metadata around messaging, [cloudevents](https://cloudevents.io) is well accepted and adopted as a specification\nto describe event data. Also [AsyncAPI](/patterns/api-mgt/#support-for-async-api)\nestablishes standards for events and messaging in the asynchronous world with an API view, so combining message schema, channels and binding definitions so \nwe have most of the needed information for a consumer to access a data stream or queue. \n\nSo the contract is defined with a schema. From an EDA design point of view, the producer owns the definition of the schema as it owns the main business entity\nlife cycle, events are generated from. Producer will make sure the message complies with the schema at hand for serializing.\n\nOn top of those specifications, there are technologies to support the contract management in the form of schema registry and API manager. The following\nfigure gives us the foundations for integration between producer, schema registry and consumers.\n\n![schema registry management](images/schema-registry.png)\n\n\nProducers must have the schema definition at hand at serialization time. This can be done either by:\n\n1. Reading the schema from local file (or configmap/secret/variable/property).\n2. Retrieving it from the Schema Registry given a name/id.\n\nWhen the producer sends a message/event to a Kafka topic for the first time using the `io.apicurio.registry.serde.avro.AvroKafkaSerializer` class, it sends the schema for that message/event to the Schema Registry. \nThe Schema Registry registers this schema to the subject for the Kafka topic we want to send the message/event to, and returns the **schema id** to the producer. \nThe producer caches this mapping between the schema and schema id for subsequent message writes, so it only contacts Schema Registry on the first message/event writen \n(unless the schema has changed, that is evolved, when the schema registry will be contacted again for validation and storage of this new version of the schema). \nKafka messages are written along with the **schema id** rather than with the entire data schema.\n\nWhen a consumer reads this data, it sees the Avro schema id and sends a schema request to the Schema Registry. \nThe Schema Registry retrieves the schema associated to that schema id, and returns the schema to the consumer. The consumer caches this mapping between \nthe schema and schema id for subsequent message reads, so it only contacts Schema Registry on the first schema id read.\n\nNon Java Consumers usee a C library that requires the schema definition at hand for the deserializer too (see https://github.com/confluentinc/confluent-kafka-python/issues/834). \nThe strategy would be to have the schema loaded from the schema registry via API.\n\n## Schema Registry\n\nWith a pure open-source strategy, Event Streams within Cloud Pak for integration is using Apicu.io as schema registry. \nThe Event Streams [product documentation](https://ibm.github.io/event-streams/schemas/overview/) is doing an excellent job to \npresent the schema registry, we do not need to rewrite the story, just give you some summary from Apicur.io and links to code samples.\n\n### Apicurio\n\n[Apicur.io](https://www.apicur.io) includes a [schema registry](https://www.apicur.io/registry/docs/apicurio-registry/2.1.x/index.html) to store schema definitions. \nIt supports Avro, json, protobuf schemas, and an API registry to manage OpenApi and AsynchAPI.\n\nIt is a Cloud-native Quarkus Java runtime for low memory footprint and fast deployment times. It supports [different persistences](apicurio-registry/2.1.x/getting-started/assembly-intro-to-the-registry.html#registry-storage_registry)\nlike Kafka, Postgresql, Infinispan and supports different deployment models.\n\n#### Registry Characteristics\n\n* Apicurio Registry is a datastore for sharing standard event schemas and API designs across API and event-driven architectures.\nIn the messaging and event streaming world, data that are published to topics and queues often must be serialized or validated using a Schema.\n* The registry supports adding, removing, and updating the following types of artifacts: OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD).\n* Schema can be created via Web Console, core REST API or Maven plugin\n* It includes configurable rules to control the validity and compatibility.\n* Client applications can dynamically push or pull the latest schema updates to or from Apicurio Registry at runtime.\nApicurio is compatible with existing Confluent schema registry client applications.\n* It includes client serializers/deserializers (Serdes) to validate Kafka and other message types at runtime.\n* Operator-based installation of Apicurio Registry on OpenShift\n* Use the concept of artifact group to collect schema and APIs logically related.\n* Support search for artifacts by label, name, group, and description\n\nWhen using Kafka as persistence, special Kafka topic `<kafkastore.topic>` (default `_schemas`), with a single partition, is used as a highly available write ahead log. \nAll schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. \nA Schema Registry instance therefore both produces and consumes messages under the `_schemas` topic. \nIt produces messages to the log when, for example, new schemas are registered under a subject, or when updates to \ncompatibility settings are registered. Schema Registry consumes from the `_schemas` log in a background thread, and updates its local \ncaches on consumption of each new `_schemas` message to reflect the newly added schema or compatibility setting. \nUpdating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.\n\n\nThe way Event Streams / Apicur.io has to handle schema association to topics is by schema name. Given we have a topic called orders, \nthe schemas that will apply to it are avros-key (when using composite key) and orders-value (most likely based on cloudevents and then custom payload).\n\n## Apache Avro\n\nAvro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.\n\n### Why Apache Avro\n\nThere are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a [Confluent blog post](https://www.confluent.io/blog/avro-kafka-data/):\n\n- It has a direct mapping to and from JSON\n- It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.\n- It is very fast.\n- It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.\n- It has a rich, extensible schema language defined in pure JSON\n- It has the best notion of compatibility for evolving your data over time.\n\n## Data Schemas\n\nAvro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format.\n\n### How does a data schema look like?\n\nLet's see how a data schema to define a person's profile in a bank could look like:\n\n```json\n{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"profile\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a profile for a banking entity\",\n  \"fields \": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"surname\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"age\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"account\",\n      \"type\": \"banking.schemas.demo.account\"\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": {\n        \"type\": \"enum\",\n        \"name\": \"genderEnum\",\n        \"symbols\": [\n          \"male\",\n          \"female\"\n        ]\n      }\n    }\n  ]\n}\n```\n\nNotice:\n\n1. There are primitive data types like `string` and `int` but also complex types like `record` or `enum`.\n2. Complex type `record` requires a `name` attribute but it also can go along with a `namespace` attribute which is a JSON string that qualifies the name.\n3. Data schemas can be **nested** as you can see for the `account` data attribute. See below.\n\n```json\n{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"account\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a customer account with the credit cards associated to it\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"savings\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"cards\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"int\"\n      }\n    }\n  ]\n}\n```\n\nIn the picture below we see two messages, one complies with the above Apache Avro data schema and the other does not:\n\n![data examples](images/data_examples.png)\n\nYou might start realising by now the benefits of having the data flowing into your Apache Kafka event backbone validated against a schema. See next section for more.\n\nFor more information on the Apache Avro Data Schema specification see <https://avro.apache.org/docs/current/spec.html>\n\n### Benefits of using Data Schemas\n\n- **Clarity and Semantics**: They document the usage of the event and the meaning of each field in the \"doc\" fields.\n- **Robustness**: They protect downstream data consumers from malformed  data, as only valid data will be permitted in the topic. They let the producers or consumers of data streams know the right fields are need in an event and what type each field is (contract for microservices).\n- **Compatibility**: model and handle change in data format.\n\n## Avro, Kafka and Schema Registry\n\nIn this section we try to put all the pieces together for the common flow of sending and receiving messages through an event backbone \nsuch as kafka having those messages serialized using the Apache Avro data serialization system and complying with their respective messages \nthat are stored and managed by a schema registry.\n\nAvro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. An Avro schema defines \nthe structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. \nThe name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name.\n\nIn this case, the messages are serialized using Avro and sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the **schema id**. The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id.\n\n## More reading\n\n### Articles and product documentation\n\n* [IBM Event Streams-  Schemas overview](https://ibm.github.io/event-streams/schemas/overview/)\n* [Apicur.io schema registry documentation](https://www.apicur.io/registry/docs/apicurio-registry/2.1.x/index.html)\n* [Confluent schema registry overview](https://docs.confluent.io/platform/current/schema-registry/index.html)\n* [Producer code with reactive messaging and apicurio schema registry](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/quarkus-reactive-kafka-producer)\n* [Consumer code with reactive messaging and apicurio schema registry](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/quarkus-reactive-kafka-consumer)\n\n### Labs\n\nWe have developed two labs, one for the IBM Event Streams product that comes with the IBM CloudPak for Integration installed on a RedHat OpenShift cluster and the other for the IBM Event Streams on IBM Cloud offering, to get hands-on experience working with Apache Avro, data schemas and the IBM Event Streams Schema Registry:\n\n- [IBM Event Streams on IBM Cloud lab](/use-cases/schema-registry-on-cloud/)\n- [IBM Event Streams from IBM CloudPak for Integration lab](/use-cases/schema-registry-on-ocp/)\n","type":"Mdx","contentDigest":"8cb05e75c819113477965d11caf5f71a","owner":"gatsby-plugin-mdx","counter":823},"frontmatter":{"title":"Apache Avro, Data Schemas and Schema Registry","description":"Apache Avro data serialization, data schemas for data definition and correctness and Schema Registry for data schema management"},"exports":{},"rawBody":"---\ntitle: Apache Avro, Data Schemas and Schema Registry\ndescription: Apache Avro data serialization, data schemas for data definition and correctness and Schema Registry for data schema management\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updates 1/24/2022</strong>\n</InlineNotification>\n\nThis chapter describes what and why Avro and Schema registry are important elements of any event-driven solutions.\n\n\n<AnchorLinks>\n    <AnchorLink>Why this is important</AnchorLink>\n    <AnchorLink>Schema Registry</AnchorLink>\n    <AnchorLink>Apache Avro</AnchorLink> \n    <AnchorLink>Data Schemas</AnchorLink> \n    <AnchorLink>Avro, Kafka and Schema Registry</AnchorLink>\n    <AnchorLink>More reading</AnchorLink>\n</AnchorLinks>\n\n## Why this is important\n\nLoosely coupling and asynchronous communication between applications does not mean there is no contract to enforce some constraint between producer and consumer. \nWhen we talk about contract we can first think about schema as we did with XSD. In the world of JSON, JSON schema and Avro schemas can be used to define\ndata structure of the message. As there is a need to get metadata around messaging, [cloudevents](https://cloudevents.io) is well accepted and adopted as a specification\nto describe event data. Also [AsyncAPI](/patterns/api-mgt/#support-for-async-api)\nestablishes standards for events and messaging in the asynchronous world with an API view, so combining message schema, channels and binding definitions so \nwe have most of the needed information for a consumer to access a data stream or queue. \n\nSo the contract is defined with a schema. From an EDA design point of view, the producer owns the definition of the schema as it owns the main business entity\nlife cycle, events are generated from. Producer will make sure the message complies with the schema at hand for serializing.\n\nOn top of those specifications, there are technologies to support the contract management in the form of schema registry and API manager. The following\nfigure gives us the foundations for integration between producer, schema registry and consumers.\n\n![schema registry management](images/schema-registry.png)\n\n\nProducers must have the schema definition at hand at serialization time. This can be done either by:\n\n1. Reading the schema from local file (or configmap/secret/variable/property).\n2. Retrieving it from the Schema Registry given a name/id.\n\nWhen the producer sends a message/event to a Kafka topic for the first time using the `io.apicurio.registry.serde.avro.AvroKafkaSerializer` class, it sends the schema for that message/event to the Schema Registry. \nThe Schema Registry registers this schema to the subject for the Kafka topic we want to send the message/event to, and returns the **schema id** to the producer. \nThe producer caches this mapping between the schema and schema id for subsequent message writes, so it only contacts Schema Registry on the first message/event writen \n(unless the schema has changed, that is evolved, when the schema registry will be contacted again for validation and storage of this new version of the schema). \nKafka messages are written along with the **schema id** rather than with the entire data schema.\n\nWhen a consumer reads this data, it sees the Avro schema id and sends a schema request to the Schema Registry. \nThe Schema Registry retrieves the schema associated to that schema id, and returns the schema to the consumer. The consumer caches this mapping between \nthe schema and schema id for subsequent message reads, so it only contacts Schema Registry on the first schema id read.\n\nNon Java Consumers usee a C library that requires the schema definition at hand for the deserializer too (see https://github.com/confluentinc/confluent-kafka-python/issues/834). \nThe strategy would be to have the schema loaded from the schema registry via API.\n\n## Schema Registry\n\nWith a pure open-source strategy, Event Streams within Cloud Pak for integration is using Apicu.io as schema registry. \nThe Event Streams [product documentation](https://ibm.github.io/event-streams/schemas/overview/) is doing an excellent job to \npresent the schema registry, we do not need to rewrite the story, just give you some summary from Apicur.io and links to code samples.\n\n### Apicurio\n\n[Apicur.io](https://www.apicur.io) includes a [schema registry](https://www.apicur.io/registry/docs/apicurio-registry/2.1.x/index.html) to store schema definitions. \nIt supports Avro, json, protobuf schemas, and an API registry to manage OpenApi and AsynchAPI.\n\nIt is a Cloud-native Quarkus Java runtime for low memory footprint and fast deployment times. It supports [different persistences](apicurio-registry/2.1.x/getting-started/assembly-intro-to-the-registry.html#registry-storage_registry)\nlike Kafka, Postgresql, Infinispan and supports different deployment models.\n\n#### Registry Characteristics\n\n* Apicurio Registry is a datastore for sharing standard event schemas and API designs across API and event-driven architectures.\nIn the messaging and event streaming world, data that are published to topics and queues often must be serialized or validated using a Schema.\n* The registry supports adding, removing, and updating the following types of artifacts: OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD).\n* Schema can be created via Web Console, core REST API or Maven plugin\n* It includes configurable rules to control the validity and compatibility.\n* Client applications can dynamically push or pull the latest schema updates to or from Apicurio Registry at runtime.\nApicurio is compatible with existing Confluent schema registry client applications.\n* It includes client serializers/deserializers (Serdes) to validate Kafka and other message types at runtime.\n* Operator-based installation of Apicurio Registry on OpenShift\n* Use the concept of artifact group to collect schema and APIs logically related.\n* Support search for artifacts by label, name, group, and description\n\nWhen using Kafka as persistence, special Kafka topic `<kafkastore.topic>` (default `_schemas`), with a single partition, is used as a highly available write ahead log. \nAll schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. \nA Schema Registry instance therefore both produces and consumes messages under the `_schemas` topic. \nIt produces messages to the log when, for example, new schemas are registered under a subject, or when updates to \ncompatibility settings are registered. Schema Registry consumes from the `_schemas` log in a background thread, and updates its local \ncaches on consumption of each new `_schemas` message to reflect the newly added schema or compatibility setting. \nUpdating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.\n\n\nThe way Event Streams / Apicur.io has to handle schema association to topics is by schema name. Given we have a topic called orders, \nthe schemas that will apply to it are avros-key (when using composite key) and orders-value (most likely based on cloudevents and then custom payload).\n\n## Apache Avro\n\nAvro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.\n\n### Why Apache Avro\n\nThere are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a [Confluent blog post](https://www.confluent.io/blog/avro-kafka-data/):\n\n- It has a direct mapping to and from JSON\n- It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.\n- It is very fast.\n- It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.\n- It has a rich, extensible schema language defined in pure JSON\n- It has the best notion of compatibility for evolving your data over time.\n\n## Data Schemas\n\nAvro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format.\n\n### How does a data schema look like?\n\nLet's see how a data schema to define a person's profile in a bank could look like:\n\n```json\n{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"profile\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a profile for a banking entity\",\n  \"fields \": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"surname\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"age\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"account\",\n      \"type\": \"banking.schemas.demo.account\"\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": {\n        \"type\": \"enum\",\n        \"name\": \"genderEnum\",\n        \"symbols\": [\n          \"male\",\n          \"female\"\n        ]\n      }\n    }\n  ]\n}\n```\n\nNotice:\n\n1. There are primitive data types like `string` and `int` but also complex types like `record` or `enum`.\n2. Complex type `record` requires a `name` attribute but it also can go along with a `namespace` attribute which is a JSON string that qualifies the name.\n3. Data schemas can be **nested** as you can see for the `account` data attribute. See below.\n\n```json\n{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"account\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a customer account with the credit cards associated to it\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"savings\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"cards\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"int\"\n      }\n    }\n  ]\n}\n```\n\nIn the picture below we see two messages, one complies with the above Apache Avro data schema and the other does not:\n\n![data examples](images/data_examples.png)\n\nYou might start realising by now the benefits of having the data flowing into your Apache Kafka event backbone validated against a schema. See next section for more.\n\nFor more information on the Apache Avro Data Schema specification see <https://avro.apache.org/docs/current/spec.html>\n\n### Benefits of using Data Schemas\n\n- **Clarity and Semantics**: They document the usage of the event and the meaning of each field in the \"doc\" fields.\n- **Robustness**: They protect downstream data consumers from malformed  data, as only valid data will be permitted in the topic. They let the producers or consumers of data streams know the right fields are need in an event and what type each field is (contract for microservices).\n- **Compatibility**: model and handle change in data format.\n\n## Avro, Kafka and Schema Registry\n\nIn this section we try to put all the pieces together for the common flow of sending and receiving messages through an event backbone \nsuch as kafka having those messages serialized using the Apache Avro data serialization system and complying with their respective messages \nthat are stored and managed by a schema registry.\n\nAvro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. An Avro schema defines \nthe structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. \nThe name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name.\n\nIn this case, the messages are serialized using Avro and sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the **schema id**. The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id.\n\n## More reading\n\n### Articles and product documentation\n\n* [IBM Event Streams-  Schemas overview](https://ibm.github.io/event-streams/schemas/overview/)\n* [Apicur.io schema registry documentation](https://www.apicur.io/registry/docs/apicurio-registry/2.1.x/index.html)\n* [Confluent schema registry overview](https://docs.confluent.io/platform/current/schema-registry/index.html)\n* [Producer code with reactive messaging and apicurio schema registry](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/quarkus-reactive-kafka-producer)\n* [Consumer code with reactive messaging and apicurio schema registry](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/quarkus-reactive-kafka-consumer)\n\n### Labs\n\nWe have developed two labs, one for the IBM Event Streams product that comes with the IBM CloudPak for Integration installed on a RedHat OpenShift cluster and the other for the IBM Event Streams on IBM Cloud offering, to get hands-on experience working with Apache Avro, data schemas and the IBM Event Streams Schema Registry:\n\n- [IBM Event Streams on IBM Cloud lab](/use-cases/schema-registry-on-cloud/)\n- [IBM Event Streams from IBM CloudPak for Integration lab](/use-cases/schema-registry-on-ocp/)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/avro-schemas/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}